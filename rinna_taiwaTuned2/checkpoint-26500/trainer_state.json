{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.893960904226275,
  "eval_steps": 500,
  "global_step": 26500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0010920607185759528,
      "grad_norm": 4.754179000854492,
      "learning_rate": 4.999508572676641e-05,
      "loss": 14.9538,
      "step": 10
    },
    {
      "epoch": 0.0021841214371519056,
      "grad_norm": 5.060490131378174,
      "learning_rate": 4.9990171453532816e-05,
      "loss": 13.8237,
      "step": 20
    },
    {
      "epoch": 0.0032761821557278584,
      "grad_norm": 5.362720012664795,
      "learning_rate": 4.998471114993994e-05,
      "loss": 12.0431,
      "step": 30
    },
    {
      "epoch": 0.004368242874303811,
      "grad_norm": 5.764788627624512,
      "learning_rate": 4.997979687670635e-05,
      "loss": 10.2908,
      "step": 40
    },
    {
      "epoch": 0.005460303592879764,
      "grad_norm": 5.560097694396973,
      "learning_rate": 4.997433657311347e-05,
      "loss": 8.0375,
      "step": 50
    },
    {
      "epoch": 0.006552364311455717,
      "grad_norm": 3.320882558822632,
      "learning_rate": 4.996887626952059e-05,
      "loss": 5.8031,
      "step": 60
    },
    {
      "epoch": 0.00764442503003167,
      "grad_norm": 1.8858534097671509,
      "learning_rate": 4.9963961996286995e-05,
      "loss": 4.6535,
      "step": 70
    },
    {
      "epoch": 0.008736485748607623,
      "grad_norm": 0.582674503326416,
      "learning_rate": 4.9958501692694114e-05,
      "loss": 3.9802,
      "step": 80
    },
    {
      "epoch": 0.009828546467183575,
      "grad_norm": 0.7519481182098389,
      "learning_rate": 4.995304138910123e-05,
      "loss": 3.7486,
      "step": 90
    },
    {
      "epoch": 0.010920607185759528,
      "grad_norm": 0.32259875535964966,
      "learning_rate": 4.994758108550835e-05,
      "loss": 3.6386,
      "step": 100
    },
    {
      "epoch": 0.012012667904335481,
      "grad_norm": 0.40318766236305237,
      "learning_rate": 4.994212078191547e-05,
      "loss": 3.5274,
      "step": 110
    },
    {
      "epoch": 0.013104728622911434,
      "grad_norm": 0.47758153080940247,
      "learning_rate": 4.993666047832259e-05,
      "loss": 3.5608,
      "step": 120
    },
    {
      "epoch": 0.014196789341487387,
      "grad_norm": 0.40294358134269714,
      "learning_rate": 4.993120017472972e-05,
      "loss": 3.4986,
      "step": 130
    },
    {
      "epoch": 0.01528885006006334,
      "grad_norm": 0.7426188588142395,
      "learning_rate": 4.992573987113684e-05,
      "loss": 3.502,
      "step": 140
    },
    {
      "epoch": 0.016380910778639294,
      "grad_norm": 0.23428663611412048,
      "learning_rate": 4.992027956754396e-05,
      "loss": 3.4279,
      "step": 150
    },
    {
      "epoch": 0.017472971497215245,
      "grad_norm": 0.31310564279556274,
      "learning_rate": 4.9914819263951076e-05,
      "loss": 3.3648,
      "step": 160
    },
    {
      "epoch": 0.0185650322157912,
      "grad_norm": 0.5197393298149109,
      "learning_rate": 4.99093589603582e-05,
      "loss": 3.4195,
      "step": 170
    },
    {
      "epoch": 0.01965709293436715,
      "grad_norm": 0.44210827350616455,
      "learning_rate": 4.990389865676532e-05,
      "loss": 3.4001,
      "step": 180
    },
    {
      "epoch": 0.020749153652943105,
      "grad_norm": 0.25996872782707214,
      "learning_rate": 4.989843835317244e-05,
      "loss": 3.3195,
      "step": 190
    },
    {
      "epoch": 0.021841214371519056,
      "grad_norm": 0.2851574420928955,
      "learning_rate": 4.989297804957956e-05,
      "loss": 3.29,
      "step": 200
    },
    {
      "epoch": 0.02293327509009501,
      "grad_norm": 0.5907253623008728,
      "learning_rate": 4.988751774598668e-05,
      "loss": 3.3054,
      "step": 210
    },
    {
      "epoch": 0.024025335808670962,
      "grad_norm": 0.24961240589618683,
      "learning_rate": 4.98820574423938e-05,
      "loss": 3.284,
      "step": 220
    },
    {
      "epoch": 0.025117396527246916,
      "grad_norm": 0.2735273838043213,
      "learning_rate": 4.987659713880092e-05,
      "loss": 3.2769,
      "step": 230
    },
    {
      "epoch": 0.026209457245822868,
      "grad_norm": 0.49527740478515625,
      "learning_rate": 4.987113683520804e-05,
      "loss": 3.2571,
      "step": 240
    },
    {
      "epoch": 0.027301517964398822,
      "grad_norm": 0.4015358090400696,
      "learning_rate": 4.986567653161516e-05,
      "loss": 3.301,
      "step": 250
    },
    {
      "epoch": 0.028393578682974773,
      "grad_norm": 0.8532331585884094,
      "learning_rate": 4.9860216228022284e-05,
      "loss": 3.2611,
      "step": 260
    },
    {
      "epoch": 0.029485639401550728,
      "grad_norm": 0.18320761620998383,
      "learning_rate": 4.98547559244294e-05,
      "loss": 3.1968,
      "step": 270
    },
    {
      "epoch": 0.03057770012012668,
      "grad_norm": 0.2795444130897522,
      "learning_rate": 4.984929562083652e-05,
      "loss": 3.1695,
      "step": 280
    },
    {
      "epoch": 0.03166976083870263,
      "grad_norm": 0.2977141737937927,
      "learning_rate": 4.984383531724364e-05,
      "loss": 3.1847,
      "step": 290
    },
    {
      "epoch": 0.03276182155727859,
      "grad_norm": 0.253905326128006,
      "learning_rate": 4.983837501365076e-05,
      "loss": 3.162,
      "step": 300
    },
    {
      "epoch": 0.033853882275854535,
      "grad_norm": 0.3576558530330658,
      "learning_rate": 4.983291471005788e-05,
      "loss": 3.242,
      "step": 310
    },
    {
      "epoch": 0.03494594299443049,
      "grad_norm": 0.265037477016449,
      "learning_rate": 4.9827454406465e-05,
      "loss": 3.1776,
      "step": 320
    },
    {
      "epoch": 0.036038003713006445,
      "grad_norm": 0.24181362986564636,
      "learning_rate": 4.982199410287212e-05,
      "loss": 3.1662,
      "step": 330
    },
    {
      "epoch": 0.0371300644315824,
      "grad_norm": 0.16802984476089478,
      "learning_rate": 4.981653379927924e-05,
      "loss": 3.1195,
      "step": 340
    },
    {
      "epoch": 0.03822212515015835,
      "grad_norm": 0.7975373864173889,
      "learning_rate": 4.981107349568636e-05,
      "loss": 3.1331,
      "step": 350
    },
    {
      "epoch": 0.0393141858687343,
      "grad_norm": 0.1401757001876831,
      "learning_rate": 4.980561319209348e-05,
      "loss": 3.1653,
      "step": 360
    },
    {
      "epoch": 0.040406246587310256,
      "grad_norm": 0.44214072823524475,
      "learning_rate": 4.9800152888500604e-05,
      "loss": 3.1712,
      "step": 370
    },
    {
      "epoch": 0.04149830730588621,
      "grad_norm": 0.24024149775505066,
      "learning_rate": 4.979469258490772e-05,
      "loss": 3.1745,
      "step": 380
    },
    {
      "epoch": 0.04259036802446216,
      "grad_norm": 0.3400875926017761,
      "learning_rate": 4.978923228131484e-05,
      "loss": 3.0936,
      "step": 390
    },
    {
      "epoch": 0.04368242874303811,
      "grad_norm": 0.11841323226690292,
      "learning_rate": 4.978377197772197e-05,
      "loss": 3.11,
      "step": 400
    },
    {
      "epoch": 0.04477448946161407,
      "grad_norm": 0.12108517438173294,
      "learning_rate": 4.977831167412909e-05,
      "loss": 3.0886,
      "step": 410
    },
    {
      "epoch": 0.04586655018019002,
      "grad_norm": 0.4664306938648224,
      "learning_rate": 4.977285137053621e-05,
      "loss": 3.0696,
      "step": 420
    },
    {
      "epoch": 0.04695861089876597,
      "grad_norm": 0.35732024908065796,
      "learning_rate": 4.976739106694333e-05,
      "loss": 3.1056,
      "step": 430
    },
    {
      "epoch": 0.048050671617341924,
      "grad_norm": 0.2057914435863495,
      "learning_rate": 4.9761930763350447e-05,
      "loss": 3.0828,
      "step": 440
    },
    {
      "epoch": 0.04914273233591788,
      "grad_norm": 0.15009059011936188,
      "learning_rate": 4.9756470459757566e-05,
      "loss": 3.0855,
      "step": 450
    },
    {
      "epoch": 0.05023479305449383,
      "grad_norm": 0.14478079974651337,
      "learning_rate": 4.9751010156164685e-05,
      "loss": 3.0553,
      "step": 460
    },
    {
      "epoch": 0.05132685377306978,
      "grad_norm": 0.7357149124145508,
      "learning_rate": 4.9745549852571805e-05,
      "loss": 3.0897,
      "step": 470
    },
    {
      "epoch": 0.052418914491645735,
      "grad_norm": 0.17913348972797394,
      "learning_rate": 4.9740089548978924e-05,
      "loss": 3.0635,
      "step": 480
    },
    {
      "epoch": 0.05351097521022169,
      "grad_norm": 0.1879180520772934,
      "learning_rate": 4.9734629245386044e-05,
      "loss": 3.0403,
      "step": 490
    },
    {
      "epoch": 0.054603035928797644,
      "grad_norm": 0.2422257661819458,
      "learning_rate": 4.972916894179316e-05,
      "loss": 3.1317,
      "step": 500
    },
    {
      "epoch": 0.05569509664737359,
      "grad_norm": 0.5399864912033081,
      "learning_rate": 4.972370863820028e-05,
      "loss": 3.1099,
      "step": 510
    },
    {
      "epoch": 0.056787157365949546,
      "grad_norm": 0.4841179847717285,
      "learning_rate": 4.971824833460741e-05,
      "loss": 3.0674,
      "step": 520
    },
    {
      "epoch": 0.0578792180845255,
      "grad_norm": 0.15360023081302643,
      "learning_rate": 4.971278803101453e-05,
      "loss": 3.0574,
      "step": 530
    },
    {
      "epoch": 0.058971278803101455,
      "grad_norm": 0.32621416449546814,
      "learning_rate": 4.970732772742165e-05,
      "loss": 3.086,
      "step": 540
    },
    {
      "epoch": 0.0600633395216774,
      "grad_norm": 0.3033110201358795,
      "learning_rate": 4.970186742382877e-05,
      "loss": 3.0737,
      "step": 550
    },
    {
      "epoch": 0.06115540024025336,
      "grad_norm": 0.1206643208861351,
      "learning_rate": 4.9696407120235886e-05,
      "loss": 3.029,
      "step": 560
    },
    {
      "epoch": 0.06224746095882931,
      "grad_norm": 0.3758591115474701,
      "learning_rate": 4.9690946816643006e-05,
      "loss": 3.0294,
      "step": 570
    },
    {
      "epoch": 0.06333952167740527,
      "grad_norm": 0.46512648463249207,
      "learning_rate": 4.9685486513050125e-05,
      "loss": 3.0621,
      "step": 580
    },
    {
      "epoch": 0.06443158239598122,
      "grad_norm": 1.536746621131897,
      "learning_rate": 4.9680026209457245e-05,
      "loss": 3.1124,
      "step": 590
    },
    {
      "epoch": 0.06552364311455718,
      "grad_norm": 0.2737691104412079,
      "learning_rate": 4.967456590586437e-05,
      "loss": 3.0466,
      "step": 600
    },
    {
      "epoch": 0.06661570383313312,
      "grad_norm": 0.10010597109794617,
      "learning_rate": 4.966910560227149e-05,
      "loss": 3.0627,
      "step": 610
    },
    {
      "epoch": 0.06770776455170907,
      "grad_norm": 0.18747073411941528,
      "learning_rate": 4.966364529867861e-05,
      "loss": 3.0411,
      "step": 620
    },
    {
      "epoch": 0.06879982527028503,
      "grad_norm": 0.27971524000167847,
      "learning_rate": 4.965818499508573e-05,
      "loss": 3.0448,
      "step": 630
    },
    {
      "epoch": 0.06989188598886098,
      "grad_norm": 0.1396963894367218,
      "learning_rate": 4.965272469149285e-05,
      "loss": 3.0767,
      "step": 640
    },
    {
      "epoch": 0.07098394670743693,
      "grad_norm": 0.5628641247749329,
      "learning_rate": 4.9647264387899975e-05,
      "loss": 3.0277,
      "step": 650
    },
    {
      "epoch": 0.07207600742601289,
      "grad_norm": 0.3878334164619446,
      "learning_rate": 4.9641804084307094e-05,
      "loss": 3.0531,
      "step": 660
    },
    {
      "epoch": 0.07316806814458884,
      "grad_norm": 0.19237494468688965,
      "learning_rate": 4.963634378071421e-05,
      "loss": 3.0305,
      "step": 670
    },
    {
      "epoch": 0.0742601288631648,
      "grad_norm": 0.3006623089313507,
      "learning_rate": 4.963088347712133e-05,
      "loss": 3.0228,
      "step": 680
    },
    {
      "epoch": 0.07535218958174074,
      "grad_norm": 0.3796994388103485,
      "learning_rate": 4.962542317352845e-05,
      "loss": 3.0595,
      "step": 690
    },
    {
      "epoch": 0.0764442503003167,
      "grad_norm": 0.11862912029027939,
      "learning_rate": 4.961996286993557e-05,
      "loss": 3.034,
      "step": 700
    },
    {
      "epoch": 0.07753631101889265,
      "grad_norm": 0.14557842910289764,
      "learning_rate": 4.961450256634269e-05,
      "loss": 3.0462,
      "step": 710
    },
    {
      "epoch": 0.0786283717374686,
      "grad_norm": 0.2513599991798401,
      "learning_rate": 4.960904226274981e-05,
      "loss": 3.0823,
      "step": 720
    },
    {
      "epoch": 0.07972043245604456,
      "grad_norm": 0.21368694305419922,
      "learning_rate": 4.960358195915693e-05,
      "loss": 3.0258,
      "step": 730
    },
    {
      "epoch": 0.08081249317462051,
      "grad_norm": 0.33062419295310974,
      "learning_rate": 4.959812165556405e-05,
      "loss": 3.0389,
      "step": 740
    },
    {
      "epoch": 0.08190455389319647,
      "grad_norm": 0.0978386402130127,
      "learning_rate": 4.959266135197117e-05,
      "loss": 3.0592,
      "step": 750
    },
    {
      "epoch": 0.08299661461177242,
      "grad_norm": 0.33236730098724365,
      "learning_rate": 4.958720104837829e-05,
      "loss": 3.0992,
      "step": 760
    },
    {
      "epoch": 0.08408867533034836,
      "grad_norm": 0.6663192510604858,
      "learning_rate": 4.958174074478541e-05,
      "loss": 3.0517,
      "step": 770
    },
    {
      "epoch": 0.08518073604892432,
      "grad_norm": 0.2650691568851471,
      "learning_rate": 4.9576280441192534e-05,
      "loss": 3.0974,
      "step": 780
    },
    {
      "epoch": 0.08627279676750027,
      "grad_norm": 0.24680745601654053,
      "learning_rate": 4.957082013759965e-05,
      "loss": 3.0665,
      "step": 790
    },
    {
      "epoch": 0.08736485748607623,
      "grad_norm": 0.09422425925731659,
      "learning_rate": 4.956535983400677e-05,
      "loss": 3.0175,
      "step": 800
    },
    {
      "epoch": 0.08845691820465218,
      "grad_norm": 0.12826032936573029,
      "learning_rate": 4.955989953041389e-05,
      "loss": 3.0119,
      "step": 810
    },
    {
      "epoch": 0.08954897892322813,
      "grad_norm": 0.4773162007331848,
      "learning_rate": 4.955443922682101e-05,
      "loss": 3.0115,
      "step": 820
    },
    {
      "epoch": 0.09064103964180409,
      "grad_norm": 0.6988077163696289,
      "learning_rate": 4.954897892322814e-05,
      "loss": 3.0759,
      "step": 830
    },
    {
      "epoch": 0.09173310036038004,
      "grad_norm": 1.010422945022583,
      "learning_rate": 4.954351861963526e-05,
      "loss": 3.0445,
      "step": 840
    },
    {
      "epoch": 0.09282516107895598,
      "grad_norm": 0.3028877377510071,
      "learning_rate": 4.9538058316042376e-05,
      "loss": 3.0049,
      "step": 850
    },
    {
      "epoch": 0.09391722179753194,
      "grad_norm": 0.6542285084724426,
      "learning_rate": 4.9532598012449496e-05,
      "loss": 3.1203,
      "step": 860
    },
    {
      "epoch": 0.09500928251610789,
      "grad_norm": 0.18742786347866058,
      "learning_rate": 4.9527137708856615e-05,
      "loss": 3.023,
      "step": 870
    },
    {
      "epoch": 0.09610134323468385,
      "grad_norm": 0.21975848078727722,
      "learning_rate": 4.9521677405263735e-05,
      "loss": 2.9882,
      "step": 880
    },
    {
      "epoch": 0.0971934039532598,
      "grad_norm": 0.3898831307888031,
      "learning_rate": 4.9516217101670854e-05,
      "loss": 2.9889,
      "step": 890
    },
    {
      "epoch": 0.09828546467183576,
      "grad_norm": 0.12923415005207062,
      "learning_rate": 4.951075679807797e-05,
      "loss": 3.0022,
      "step": 900
    },
    {
      "epoch": 0.09937752539041171,
      "grad_norm": 0.1496366411447525,
      "learning_rate": 4.95052964944851e-05,
      "loss": 3.019,
      "step": 910
    },
    {
      "epoch": 0.10046958610898767,
      "grad_norm": 0.1856071501970291,
      "learning_rate": 4.949983619089222e-05,
      "loss": 2.9866,
      "step": 920
    },
    {
      "epoch": 0.1015616468275636,
      "grad_norm": 0.11975039541721344,
      "learning_rate": 4.949437588729934e-05,
      "loss": 2.9802,
      "step": 930
    },
    {
      "epoch": 0.10265370754613956,
      "grad_norm": 0.1363970935344696,
      "learning_rate": 4.948891558370646e-05,
      "loss": 2.9927,
      "step": 940
    },
    {
      "epoch": 0.10374576826471552,
      "grad_norm": 0.2719694972038269,
      "learning_rate": 4.948345528011358e-05,
      "loss": 3.0066,
      "step": 950
    },
    {
      "epoch": 0.10483782898329147,
      "grad_norm": 0.14449113607406616,
      "learning_rate": 4.9477994976520697e-05,
      "loss": 3.0043,
      "step": 960
    },
    {
      "epoch": 0.10592988970186742,
      "grad_norm": 0.36222004890441895,
      "learning_rate": 4.9472534672927816e-05,
      "loss": 3.022,
      "step": 970
    },
    {
      "epoch": 0.10702195042044338,
      "grad_norm": 0.5443436503410339,
      "learning_rate": 4.9467074369334935e-05,
      "loss": 2.9996,
      "step": 980
    },
    {
      "epoch": 0.10811401113901933,
      "grad_norm": 0.22301757335662842,
      "learning_rate": 4.9461614065742055e-05,
      "loss": 2.9917,
      "step": 990
    },
    {
      "epoch": 0.10920607185759529,
      "grad_norm": 0.2666536569595337,
      "learning_rate": 4.9456153762149174e-05,
      "loss": 3.0205,
      "step": 1000
    },
    {
      "epoch": 0.11029813257617123,
      "grad_norm": 0.10420911014080048,
      "learning_rate": 4.9450693458556294e-05,
      "loss": 2.9619,
      "step": 1010
    },
    {
      "epoch": 0.11139019329474718,
      "grad_norm": 0.24523629248142242,
      "learning_rate": 4.944523315496341e-05,
      "loss": 3.04,
      "step": 1020
    },
    {
      "epoch": 0.11248225401332314,
      "grad_norm": 0.2545890212059021,
      "learning_rate": 4.943977285137054e-05,
      "loss": 2.9819,
      "step": 1030
    },
    {
      "epoch": 0.11357431473189909,
      "grad_norm": 0.10015514492988586,
      "learning_rate": 4.943431254777766e-05,
      "loss": 2.9625,
      "step": 1040
    },
    {
      "epoch": 0.11466637545047505,
      "grad_norm": 0.2328045517206192,
      "learning_rate": 4.942885224418478e-05,
      "loss": 2.9963,
      "step": 1050
    },
    {
      "epoch": 0.115758436169051,
      "grad_norm": 0.3045975863933563,
      "learning_rate": 4.9423391940591904e-05,
      "loss": 2.969,
      "step": 1060
    },
    {
      "epoch": 0.11685049688762696,
      "grad_norm": 0.6456542015075684,
      "learning_rate": 4.9417931636999024e-05,
      "loss": 2.9785,
      "step": 1070
    },
    {
      "epoch": 0.11794255760620291,
      "grad_norm": 0.13046614825725555,
      "learning_rate": 4.941247133340614e-05,
      "loss": 2.9679,
      "step": 1080
    },
    {
      "epoch": 0.11903461832477885,
      "grad_norm": 0.12860442698001862,
      "learning_rate": 4.940701102981326e-05,
      "loss": 3.0121,
      "step": 1090
    },
    {
      "epoch": 0.1201266790433548,
      "grad_norm": 0.3883136212825775,
      "learning_rate": 4.940155072622038e-05,
      "loss": 3.0375,
      "step": 1100
    },
    {
      "epoch": 0.12121873976193076,
      "grad_norm": 0.1365566849708557,
      "learning_rate": 4.93960904226275e-05,
      "loss": 2.9407,
      "step": 1110
    },
    {
      "epoch": 0.12231080048050672,
      "grad_norm": 0.8121837377548218,
      "learning_rate": 4.939063011903462e-05,
      "loss": 3.0147,
      "step": 1120
    },
    {
      "epoch": 0.12340286119908267,
      "grad_norm": 0.21565212309360504,
      "learning_rate": 4.938516981544174e-05,
      "loss": 2.9868,
      "step": 1130
    },
    {
      "epoch": 0.12449492191765862,
      "grad_norm": 0.2365475744009018,
      "learning_rate": 4.937970951184886e-05,
      "loss": 2.9771,
      "step": 1140
    },
    {
      "epoch": 0.12558698263623458,
      "grad_norm": 0.21864183247089386,
      "learning_rate": 4.937424920825598e-05,
      "loss": 2.9708,
      "step": 1150
    },
    {
      "epoch": 0.12667904335481053,
      "grad_norm": 0.1294201910495758,
      "learning_rate": 4.93687889046631e-05,
      "loss": 2.963,
      "step": 1160
    },
    {
      "epoch": 0.1277711040733865,
      "grad_norm": 0.15154168009757996,
      "learning_rate": 4.9363328601070225e-05,
      "loss": 2.9754,
      "step": 1170
    },
    {
      "epoch": 0.12886316479196244,
      "grad_norm": 0.274677038192749,
      "learning_rate": 4.9357868297477344e-05,
      "loss": 3.0127,
      "step": 1180
    },
    {
      "epoch": 0.1299552255105384,
      "grad_norm": 0.8417817950248718,
      "learning_rate": 4.935240799388446e-05,
      "loss": 2.9856,
      "step": 1190
    },
    {
      "epoch": 0.13104728622911435,
      "grad_norm": 0.15351612865924835,
      "learning_rate": 4.934694769029158e-05,
      "loss": 2.9735,
      "step": 1200
    },
    {
      "epoch": 0.13213934694769028,
      "grad_norm": 0.1941174417734146,
      "learning_rate": 4.93414873866987e-05,
      "loss": 2.9978,
      "step": 1210
    },
    {
      "epoch": 0.13323140766626623,
      "grad_norm": 0.14045803248882294,
      "learning_rate": 4.933602708310582e-05,
      "loss": 2.986,
      "step": 1220
    },
    {
      "epoch": 0.1343234683848422,
      "grad_norm": 0.20861423015594482,
      "learning_rate": 4.933056677951294e-05,
      "loss": 2.9735,
      "step": 1230
    },
    {
      "epoch": 0.13541552910341814,
      "grad_norm": 0.39365682005882263,
      "learning_rate": 4.932510647592006e-05,
      "loss": 3.0784,
      "step": 1240
    },
    {
      "epoch": 0.1365075898219941,
      "grad_norm": 0.21240852773189545,
      "learning_rate": 4.931964617232718e-05,
      "loss": 2.9677,
      "step": 1250
    },
    {
      "epoch": 0.13759965054057005,
      "grad_norm": 0.297092080116272,
      "learning_rate": 4.9314185868734306e-05,
      "loss": 2.9511,
      "step": 1260
    },
    {
      "epoch": 0.138691711259146,
      "grad_norm": 0.11966480314731598,
      "learning_rate": 4.9308725565141425e-05,
      "loss": 2.9756,
      "step": 1270
    },
    {
      "epoch": 0.13978377197772196,
      "grad_norm": 0.35972487926483154,
      "learning_rate": 4.9303265261548545e-05,
      "loss": 3.0039,
      "step": 1280
    },
    {
      "epoch": 0.14087583269629791,
      "grad_norm": 0.180942565202713,
      "learning_rate": 4.9297804957955664e-05,
      "loss": 2.9575,
      "step": 1290
    },
    {
      "epoch": 0.14196789341487387,
      "grad_norm": 0.1931438446044922,
      "learning_rate": 4.929234465436279e-05,
      "loss": 2.9699,
      "step": 1300
    },
    {
      "epoch": 0.14305995413344982,
      "grad_norm": 0.15569119155406952,
      "learning_rate": 4.928688435076991e-05,
      "loss": 2.9451,
      "step": 1310
    },
    {
      "epoch": 0.14415201485202578,
      "grad_norm": 0.08627356588840485,
      "learning_rate": 4.928142404717703e-05,
      "loss": 2.9563,
      "step": 1320
    },
    {
      "epoch": 0.14524407557060173,
      "grad_norm": 0.18007811903953552,
      "learning_rate": 4.927596374358415e-05,
      "loss": 3.0413,
      "step": 1330
    },
    {
      "epoch": 0.1463361362891777,
      "grad_norm": 0.22552210092544556,
      "learning_rate": 4.927050343999127e-05,
      "loss": 2.9893,
      "step": 1340
    },
    {
      "epoch": 0.14742819700775364,
      "grad_norm": 0.18672046065330505,
      "learning_rate": 4.926504313639839e-05,
      "loss": 2.9896,
      "step": 1350
    },
    {
      "epoch": 0.1485202577263296,
      "grad_norm": 0.3773002326488495,
      "learning_rate": 4.925958283280551e-05,
      "loss": 3.0189,
      "step": 1360
    },
    {
      "epoch": 0.14961231844490552,
      "grad_norm": 0.12048670649528503,
      "learning_rate": 4.9254122529212626e-05,
      "loss": 2.9742,
      "step": 1370
    },
    {
      "epoch": 0.15070437916348148,
      "grad_norm": 0.1557401567697525,
      "learning_rate": 4.9248662225619746e-05,
      "loss": 2.9963,
      "step": 1380
    },
    {
      "epoch": 0.15179643988205743,
      "grad_norm": 0.18723343312740326,
      "learning_rate": 4.9243201922026865e-05,
      "loss": 2.9885,
      "step": 1390
    },
    {
      "epoch": 0.1528885006006334,
      "grad_norm": 0.14507035911083221,
      "learning_rate": 4.9237741618433984e-05,
      "loss": 2.9599,
      "step": 1400
    },
    {
      "epoch": 0.15398056131920934,
      "grad_norm": 0.22304365038871765,
      "learning_rate": 4.9232281314841104e-05,
      "loss": 2.9743,
      "step": 1410
    },
    {
      "epoch": 0.1550726220377853,
      "grad_norm": 0.3822101652622223,
      "learning_rate": 4.922682101124822e-05,
      "loss": 2.9203,
      "step": 1420
    },
    {
      "epoch": 0.15616468275636125,
      "grad_norm": 0.2565390467643738,
      "learning_rate": 4.922136070765535e-05,
      "loss": 2.9923,
      "step": 1430
    },
    {
      "epoch": 0.1572567434749372,
      "grad_norm": 0.46366608142852783,
      "learning_rate": 4.921590040406247e-05,
      "loss": 2.9424,
      "step": 1440
    },
    {
      "epoch": 0.15834880419351316,
      "grad_norm": 0.37247222661972046,
      "learning_rate": 4.921044010046959e-05,
      "loss": 2.9392,
      "step": 1450
    },
    {
      "epoch": 0.15944086491208911,
      "grad_norm": 0.09539580345153809,
      "learning_rate": 4.920497979687671e-05,
      "loss": 2.9685,
      "step": 1460
    },
    {
      "epoch": 0.16053292563066507,
      "grad_norm": 0.15838243067264557,
      "learning_rate": 4.919951949328383e-05,
      "loss": 2.999,
      "step": 1470
    },
    {
      "epoch": 0.16162498634924102,
      "grad_norm": 0.2829046845436096,
      "learning_rate": 4.9194059189690947e-05,
      "loss": 3.0016,
      "step": 1480
    },
    {
      "epoch": 0.16271704706781698,
      "grad_norm": 0.19589725136756897,
      "learning_rate": 4.918859888609807e-05,
      "loss": 2.9561,
      "step": 1490
    },
    {
      "epoch": 0.16380910778639293,
      "grad_norm": 0.8670446276664734,
      "learning_rate": 4.918313858250519e-05,
      "loss": 2.9717,
      "step": 1500
    },
    {
      "epoch": 0.1649011685049689,
      "grad_norm": 0.21322600543498993,
      "learning_rate": 4.917767827891231e-05,
      "loss": 2.9392,
      "step": 1510
    },
    {
      "epoch": 0.16599322922354484,
      "grad_norm": 0.36508503556251526,
      "learning_rate": 4.917221797531943e-05,
      "loss": 2.9296,
      "step": 1520
    },
    {
      "epoch": 0.16708528994212077,
      "grad_norm": 0.34896373748779297,
      "learning_rate": 4.916675767172655e-05,
      "loss": 2.9341,
      "step": 1530
    },
    {
      "epoch": 0.16817735066069672,
      "grad_norm": 0.34209269285202026,
      "learning_rate": 4.916129736813367e-05,
      "loss": 2.9669,
      "step": 1540
    },
    {
      "epoch": 0.16926941137927268,
      "grad_norm": 0.23769308626651764,
      "learning_rate": 4.915583706454079e-05,
      "loss": 2.9647,
      "step": 1550
    },
    {
      "epoch": 0.17036147209784863,
      "grad_norm": 0.39003679156303406,
      "learning_rate": 4.9150376760947915e-05,
      "loss": 2.9582,
      "step": 1560
    },
    {
      "epoch": 0.1714535328164246,
      "grad_norm": 0.1604020893573761,
      "learning_rate": 4.9144916457355035e-05,
      "loss": 2.9215,
      "step": 1570
    },
    {
      "epoch": 0.17254559353500054,
      "grad_norm": 0.1087222620844841,
      "learning_rate": 4.9139456153762154e-05,
      "loss": 2.9505,
      "step": 1580
    },
    {
      "epoch": 0.1736376542535765,
      "grad_norm": 0.2063933163881302,
      "learning_rate": 4.9133995850169274e-05,
      "loss": 2.9779,
      "step": 1590
    },
    {
      "epoch": 0.17472971497215245,
      "grad_norm": 0.19306665658950806,
      "learning_rate": 4.912853554657639e-05,
      "loss": 2.9707,
      "step": 1600
    },
    {
      "epoch": 0.1758217756907284,
      "grad_norm": 0.17432714998722076,
      "learning_rate": 4.912307524298351e-05,
      "loss": 2.9586,
      "step": 1610
    },
    {
      "epoch": 0.17691383640930436,
      "grad_norm": 0.4303067922592163,
      "learning_rate": 4.911761493939063e-05,
      "loss": 2.9384,
      "step": 1620
    },
    {
      "epoch": 0.1780058971278803,
      "grad_norm": 0.17555464804172516,
      "learning_rate": 4.911215463579775e-05,
      "loss": 2.9474,
      "step": 1630
    },
    {
      "epoch": 0.17909795784645627,
      "grad_norm": 0.19516989588737488,
      "learning_rate": 4.910669433220487e-05,
      "loss": 2.9378,
      "step": 1640
    },
    {
      "epoch": 0.18019001856503222,
      "grad_norm": 0.6573435664176941,
      "learning_rate": 4.910123402861199e-05,
      "loss": 2.9964,
      "step": 1650
    },
    {
      "epoch": 0.18128207928360818,
      "grad_norm": 0.20788581669330597,
      "learning_rate": 4.909577372501911e-05,
      "loss": 2.9602,
      "step": 1660
    },
    {
      "epoch": 0.18237414000218413,
      "grad_norm": 0.22091041505336761,
      "learning_rate": 4.909031342142623e-05,
      "loss": 2.9501,
      "step": 1670
    },
    {
      "epoch": 0.1834662007207601,
      "grad_norm": 0.2705259323120117,
      "learning_rate": 4.908485311783335e-05,
      "loss": 2.9544,
      "step": 1680
    },
    {
      "epoch": 0.184558261439336,
      "grad_norm": 0.17258495092391968,
      "learning_rate": 4.9079392814240474e-05,
      "loss": 2.9189,
      "step": 1690
    },
    {
      "epoch": 0.18565032215791197,
      "grad_norm": 0.2460288107395172,
      "learning_rate": 4.9073932510647594e-05,
      "loss": 2.959,
      "step": 1700
    },
    {
      "epoch": 0.18674238287648792,
      "grad_norm": 0.13036051392555237,
      "learning_rate": 4.906847220705471e-05,
      "loss": 2.9219,
      "step": 1710
    },
    {
      "epoch": 0.18783444359506388,
      "grad_norm": 0.4070861041545868,
      "learning_rate": 4.906301190346184e-05,
      "loss": 2.9237,
      "step": 1720
    },
    {
      "epoch": 0.18892650431363983,
      "grad_norm": 0.3946625888347626,
      "learning_rate": 4.905755159986896e-05,
      "loss": 2.9367,
      "step": 1730
    },
    {
      "epoch": 0.19001856503221579,
      "grad_norm": 0.25539693236351013,
      "learning_rate": 4.905209129627608e-05,
      "loss": 2.9404,
      "step": 1740
    },
    {
      "epoch": 0.19111062575079174,
      "grad_norm": 0.727344810962677,
      "learning_rate": 4.90466309926832e-05,
      "loss": 3.0119,
      "step": 1750
    },
    {
      "epoch": 0.1922026864693677,
      "grad_norm": 0.29172825813293457,
      "learning_rate": 4.904117068909032e-05,
      "loss": 2.9675,
      "step": 1760
    },
    {
      "epoch": 0.19329474718794365,
      "grad_norm": 0.27005019783973694,
      "learning_rate": 4.9035710385497437e-05,
      "loss": 2.9334,
      "step": 1770
    },
    {
      "epoch": 0.1943868079065196,
      "grad_norm": 0.942720353603363,
      "learning_rate": 4.9030250081904556e-05,
      "loss": 2.9505,
      "step": 1780
    },
    {
      "epoch": 0.19547886862509556,
      "grad_norm": 0.34097161889076233,
      "learning_rate": 4.9024789778311675e-05,
      "loss": 2.9278,
      "step": 1790
    },
    {
      "epoch": 0.1965709293436715,
      "grad_norm": 0.30042725801467896,
      "learning_rate": 4.9019329474718795e-05,
      "loss": 2.9174,
      "step": 1800
    },
    {
      "epoch": 0.19766299006224747,
      "grad_norm": 0.26545822620391846,
      "learning_rate": 4.9013869171125914e-05,
      "loss": 2.9632,
      "step": 1810
    },
    {
      "epoch": 0.19875505078082342,
      "grad_norm": 0.14210665225982666,
      "learning_rate": 4.900840886753304e-05,
      "loss": 2.9549,
      "step": 1820
    },
    {
      "epoch": 0.19984711149939938,
      "grad_norm": 0.16660544276237488,
      "learning_rate": 4.900294856394016e-05,
      "loss": 2.9345,
      "step": 1830
    },
    {
      "epoch": 0.20093917221797533,
      "grad_norm": 0.18804416060447693,
      "learning_rate": 4.899748826034728e-05,
      "loss": 2.9089,
      "step": 1840
    },
    {
      "epoch": 0.20203123293655129,
      "grad_norm": 0.18408320844173431,
      "learning_rate": 4.89920279567544e-05,
      "loss": 2.9367,
      "step": 1850
    },
    {
      "epoch": 0.2031232936551272,
      "grad_norm": 0.26522842049598694,
      "learning_rate": 4.898656765316152e-05,
      "loss": 2.9387,
      "step": 1860
    },
    {
      "epoch": 0.20421535437370317,
      "grad_norm": 0.6302298307418823,
      "learning_rate": 4.898110734956864e-05,
      "loss": 2.9696,
      "step": 1870
    },
    {
      "epoch": 0.20530741509227912,
      "grad_norm": 0.1955677568912506,
      "learning_rate": 4.897564704597576e-05,
      "loss": 2.9465,
      "step": 1880
    },
    {
      "epoch": 0.20639947581085508,
      "grad_norm": 0.36622998118400574,
      "learning_rate": 4.8970186742382876e-05,
      "loss": 2.9334,
      "step": 1890
    },
    {
      "epoch": 0.20749153652943103,
      "grad_norm": 0.7670555114746094,
      "learning_rate": 4.8964726438789996e-05,
      "loss": 2.9797,
      "step": 1900
    },
    {
      "epoch": 0.20858359724800699,
      "grad_norm": 0.2725318372249603,
      "learning_rate": 4.8959266135197115e-05,
      "loss": 2.9083,
      "step": 1910
    },
    {
      "epoch": 0.20967565796658294,
      "grad_norm": 0.1087900772690773,
      "learning_rate": 4.895380583160424e-05,
      "loss": 2.908,
      "step": 1920
    },
    {
      "epoch": 0.2107677186851589,
      "grad_norm": 0.12351549416780472,
      "learning_rate": 4.894834552801136e-05,
      "loss": 2.9042,
      "step": 1930
    },
    {
      "epoch": 0.21185977940373485,
      "grad_norm": 0.25071707367897034,
      "learning_rate": 4.894288522441848e-05,
      "loss": 2.9969,
      "step": 1940
    },
    {
      "epoch": 0.2129518401223108,
      "grad_norm": 0.40358057618141174,
      "learning_rate": 4.89374249208256e-05,
      "loss": 2.9174,
      "step": 1950
    },
    {
      "epoch": 0.21404390084088676,
      "grad_norm": 0.9131972789764404,
      "learning_rate": 4.8931964617232726e-05,
      "loss": 3.0195,
      "step": 1960
    },
    {
      "epoch": 0.2151359615594627,
      "grad_norm": 0.6347603797912598,
      "learning_rate": 4.8926504313639845e-05,
      "loss": 2.9295,
      "step": 1970
    },
    {
      "epoch": 0.21622802227803867,
      "grad_norm": 0.5563039183616638,
      "learning_rate": 4.8921044010046964e-05,
      "loss": 2.9387,
      "step": 1980
    },
    {
      "epoch": 0.21732008299661462,
      "grad_norm": 1.4621773958206177,
      "learning_rate": 4.8915583706454084e-05,
      "loss": 2.9607,
      "step": 1990
    },
    {
      "epoch": 0.21841214371519058,
      "grad_norm": 0.2660957872867584,
      "learning_rate": 4.89101234028612e-05,
      "loss": 2.951,
      "step": 2000
    },
    {
      "epoch": 0.21950420443376653,
      "grad_norm": 0.3033936321735382,
      "learning_rate": 4.890466309926832e-05,
      "loss": 2.9481,
      "step": 2010
    },
    {
      "epoch": 0.22059626515234246,
      "grad_norm": 0.21748268604278564,
      "learning_rate": 4.889920279567544e-05,
      "loss": 2.926,
      "step": 2020
    },
    {
      "epoch": 0.2216883258709184,
      "grad_norm": 0.1844208687543869,
      "learning_rate": 4.889374249208256e-05,
      "loss": 2.9474,
      "step": 2030
    },
    {
      "epoch": 0.22278038658949437,
      "grad_norm": 0.2142985761165619,
      "learning_rate": 4.888828218848968e-05,
      "loss": 2.9029,
      "step": 2040
    },
    {
      "epoch": 0.22387244730807032,
      "grad_norm": 0.22200337052345276,
      "learning_rate": 4.88828218848968e-05,
      "loss": 2.8968,
      "step": 2050
    },
    {
      "epoch": 0.22496450802664628,
      "grad_norm": 1.224103569984436,
      "learning_rate": 4.887736158130392e-05,
      "loss": 2.8925,
      "step": 2060
    },
    {
      "epoch": 0.22605656874522223,
      "grad_norm": 0.5809795260429382,
      "learning_rate": 4.887190127771104e-05,
      "loss": 2.9532,
      "step": 2070
    },
    {
      "epoch": 0.22714862946379819,
      "grad_norm": 0.17358742654323578,
      "learning_rate": 4.886644097411816e-05,
      "loss": 2.9257,
      "step": 2080
    },
    {
      "epoch": 0.22824069018237414,
      "grad_norm": 0.23174704611301422,
      "learning_rate": 4.8860980670525285e-05,
      "loss": 2.9082,
      "step": 2090
    },
    {
      "epoch": 0.2293327509009501,
      "grad_norm": 0.20760828256607056,
      "learning_rate": 4.8855520366932404e-05,
      "loss": 2.9046,
      "step": 2100
    },
    {
      "epoch": 0.23042481161952605,
      "grad_norm": 0.38274794816970825,
      "learning_rate": 4.8850060063339524e-05,
      "loss": 2.8946,
      "step": 2110
    },
    {
      "epoch": 0.231516872338102,
      "grad_norm": 0.16785460710525513,
      "learning_rate": 4.884459975974664e-05,
      "loss": 2.9222,
      "step": 2120
    },
    {
      "epoch": 0.23260893305667796,
      "grad_norm": 0.13475407660007477,
      "learning_rate": 4.883913945615376e-05,
      "loss": 2.9246,
      "step": 2130
    },
    {
      "epoch": 0.2337009937752539,
      "grad_norm": 0.5028625130653381,
      "learning_rate": 4.883367915256088e-05,
      "loss": 2.937,
      "step": 2140
    },
    {
      "epoch": 0.23479305449382987,
      "grad_norm": 0.8144591450691223,
      "learning_rate": 4.882821884896801e-05,
      "loss": 2.9006,
      "step": 2150
    },
    {
      "epoch": 0.23588511521240582,
      "grad_norm": 0.6190301179885864,
      "learning_rate": 4.882275854537513e-05,
      "loss": 2.9263,
      "step": 2160
    },
    {
      "epoch": 0.23697717593098178,
      "grad_norm": 0.7943978309631348,
      "learning_rate": 4.881729824178225e-05,
      "loss": 2.908,
      "step": 2170
    },
    {
      "epoch": 0.2380692366495577,
      "grad_norm": 0.46179065108299255,
      "learning_rate": 4.8811837938189366e-05,
      "loss": 2.9694,
      "step": 2180
    },
    {
      "epoch": 0.23916129736813366,
      "grad_norm": 0.1950836330652237,
      "learning_rate": 4.8806377634596486e-05,
      "loss": 2.9127,
      "step": 2190
    },
    {
      "epoch": 0.2402533580867096,
      "grad_norm": 0.16015757620334625,
      "learning_rate": 4.8800917331003605e-05,
      "loss": 2.945,
      "step": 2200
    },
    {
      "epoch": 0.24134541880528557,
      "grad_norm": 0.3373018205165863,
      "learning_rate": 4.8795457027410724e-05,
      "loss": 2.9635,
      "step": 2210
    },
    {
      "epoch": 0.24243747952386152,
      "grad_norm": 0.16827811300754547,
      "learning_rate": 4.878999672381785e-05,
      "loss": 2.9777,
      "step": 2220
    },
    {
      "epoch": 0.24352954024243748,
      "grad_norm": 0.1512952595949173,
      "learning_rate": 4.878453642022497e-05,
      "loss": 2.9168,
      "step": 2230
    },
    {
      "epoch": 0.24462160096101343,
      "grad_norm": 0.19481517374515533,
      "learning_rate": 4.877907611663209e-05,
      "loss": 2.9055,
      "step": 2240
    },
    {
      "epoch": 0.24571366167958938,
      "grad_norm": 0.1345648467540741,
      "learning_rate": 4.877361581303921e-05,
      "loss": 3.0067,
      "step": 2250
    },
    {
      "epoch": 0.24680572239816534,
      "grad_norm": 0.9799453020095825,
      "learning_rate": 4.876815550944633e-05,
      "loss": 2.9365,
      "step": 2260
    },
    {
      "epoch": 0.2478977831167413,
      "grad_norm": 0.2515411972999573,
      "learning_rate": 4.876269520585345e-05,
      "loss": 2.9053,
      "step": 2270
    },
    {
      "epoch": 0.24898984383531725,
      "grad_norm": 0.2606883943080902,
      "learning_rate": 4.875723490226057e-05,
      "loss": 2.9043,
      "step": 2280
    },
    {
      "epoch": 0.2500819045538932,
      "grad_norm": 1.9830280542373657,
      "learning_rate": 4.8751774598667686e-05,
      "loss": 2.9776,
      "step": 2290
    },
    {
      "epoch": 0.25117396527246916,
      "grad_norm": 0.38120028376579285,
      "learning_rate": 4.8746314295074806e-05,
      "loss": 2.944,
      "step": 2300
    },
    {
      "epoch": 0.2522660259910451,
      "grad_norm": 0.5657088160514832,
      "learning_rate": 4.8740853991481925e-05,
      "loss": 2.9019,
      "step": 2310
    },
    {
      "epoch": 0.25335808670962107,
      "grad_norm": 0.259487122297287,
      "learning_rate": 4.8735393687889045e-05,
      "loss": 2.9116,
      "step": 2320
    },
    {
      "epoch": 0.254450147428197,
      "grad_norm": 0.3641885817050934,
      "learning_rate": 4.8729933384296164e-05,
      "loss": 2.9427,
      "step": 2330
    },
    {
      "epoch": 0.255542208146773,
      "grad_norm": 0.3171207904815674,
      "learning_rate": 4.8724473080703284e-05,
      "loss": 2.9342,
      "step": 2340
    },
    {
      "epoch": 0.2566342688653489,
      "grad_norm": 0.17533652484416962,
      "learning_rate": 4.871901277711041e-05,
      "loss": 2.9058,
      "step": 2350
    },
    {
      "epoch": 0.2577263295839249,
      "grad_norm": 0.44601672887802124,
      "learning_rate": 4.871355247351753e-05,
      "loss": 2.8937,
      "step": 2360
    },
    {
      "epoch": 0.2588183903025008,
      "grad_norm": 0.2927924394607544,
      "learning_rate": 4.870809216992465e-05,
      "loss": 2.9101,
      "step": 2370
    },
    {
      "epoch": 0.2599104510210768,
      "grad_norm": 0.1346144825220108,
      "learning_rate": 4.8702631866331775e-05,
      "loss": 2.8951,
      "step": 2380
    },
    {
      "epoch": 0.2610025117396527,
      "grad_norm": 0.8994247913360596,
      "learning_rate": 4.8697171562738894e-05,
      "loss": 2.9066,
      "step": 2390
    },
    {
      "epoch": 0.2620945724582287,
      "grad_norm": 0.1948811560869217,
      "learning_rate": 4.8691711259146014e-05,
      "loss": 2.9086,
      "step": 2400
    },
    {
      "epoch": 0.26318663317680463,
      "grad_norm": 1.2004410028457642,
      "learning_rate": 4.868625095555313e-05,
      "loss": 2.934,
      "step": 2410
    },
    {
      "epoch": 0.26427869389538056,
      "grad_norm": 0.7191417813301086,
      "learning_rate": 4.868079065196025e-05,
      "loss": 2.9138,
      "step": 2420
    },
    {
      "epoch": 0.26537075461395654,
      "grad_norm": 0.16831889748573303,
      "learning_rate": 4.867533034836737e-05,
      "loss": 2.9151,
      "step": 2430
    },
    {
      "epoch": 0.26646281533253247,
      "grad_norm": 0.5088680386543274,
      "learning_rate": 4.866987004477449e-05,
      "loss": 2.9108,
      "step": 2440
    },
    {
      "epoch": 0.26755487605110845,
      "grad_norm": 0.2257450819015503,
      "learning_rate": 4.866440974118161e-05,
      "loss": 2.915,
      "step": 2450
    },
    {
      "epoch": 0.2686469367696844,
      "grad_norm": 0.3730233609676361,
      "learning_rate": 4.865894943758873e-05,
      "loss": 2.9212,
      "step": 2460
    },
    {
      "epoch": 0.26973899748826036,
      "grad_norm": 0.14563190937042236,
      "learning_rate": 4.865348913399585e-05,
      "loss": 2.8637,
      "step": 2470
    },
    {
      "epoch": 0.2708310582068363,
      "grad_norm": 0.1562906801700592,
      "learning_rate": 4.8648028830402976e-05,
      "loss": 2.8645,
      "step": 2480
    },
    {
      "epoch": 0.27192311892541227,
      "grad_norm": 0.255190908908844,
      "learning_rate": 4.8642568526810095e-05,
      "loss": 2.9153,
      "step": 2490
    },
    {
      "epoch": 0.2730151796439882,
      "grad_norm": 0.577784538269043,
      "learning_rate": 4.8637108223217214e-05,
      "loss": 2.8924,
      "step": 2500
    },
    {
      "epoch": 0.2741072403625642,
      "grad_norm": 0.4520682692527771,
      "learning_rate": 4.8631647919624334e-05,
      "loss": 2.9773,
      "step": 2510
    },
    {
      "epoch": 0.2751993010811401,
      "grad_norm": 0.38999924063682556,
      "learning_rate": 4.862618761603145e-05,
      "loss": 2.9417,
      "step": 2520
    },
    {
      "epoch": 0.2762913617997161,
      "grad_norm": 0.356044739484787,
      "learning_rate": 4.862072731243857e-05,
      "loss": 2.8839,
      "step": 2530
    },
    {
      "epoch": 0.277383422518292,
      "grad_norm": 0.36277204751968384,
      "learning_rate": 4.861526700884569e-05,
      "loss": 2.9139,
      "step": 2540
    },
    {
      "epoch": 0.278475483236868,
      "grad_norm": 0.2027060091495514,
      "learning_rate": 4.860980670525281e-05,
      "loss": 2.8884,
      "step": 2550
    },
    {
      "epoch": 0.2795675439554439,
      "grad_norm": 0.13813354074954987,
      "learning_rate": 4.860434640165993e-05,
      "loss": 2.9181,
      "step": 2560
    },
    {
      "epoch": 0.2806596046740199,
      "grad_norm": 0.2848769426345825,
      "learning_rate": 4.859888609806705e-05,
      "loss": 2.9228,
      "step": 2570
    },
    {
      "epoch": 0.28175166539259583,
      "grad_norm": 0.13965795934200287,
      "learning_rate": 4.8593425794474176e-05,
      "loss": 2.9464,
      "step": 2580
    },
    {
      "epoch": 0.28284372611117176,
      "grad_norm": 0.2936112880706787,
      "learning_rate": 4.8587965490881296e-05,
      "loss": 2.8965,
      "step": 2590
    },
    {
      "epoch": 0.28393578682974774,
      "grad_norm": 0.10222528874874115,
      "learning_rate": 4.8582505187288415e-05,
      "loss": 2.8626,
      "step": 2600
    },
    {
      "epoch": 0.28502784754832367,
      "grad_norm": 0.29997819662094116,
      "learning_rate": 4.857704488369554e-05,
      "loss": 2.9152,
      "step": 2610
    },
    {
      "epoch": 0.28611990826689965,
      "grad_norm": 0.18535593152046204,
      "learning_rate": 4.857158458010266e-05,
      "loss": 2.8891,
      "step": 2620
    },
    {
      "epoch": 0.2872119689854756,
      "grad_norm": 0.2204115092754364,
      "learning_rate": 4.856612427650978e-05,
      "loss": 2.8947,
      "step": 2630
    },
    {
      "epoch": 0.28830402970405156,
      "grad_norm": 0.4816991686820984,
      "learning_rate": 4.85606639729169e-05,
      "loss": 2.9021,
      "step": 2640
    },
    {
      "epoch": 0.2893960904226275,
      "grad_norm": 0.1519526243209839,
      "learning_rate": 4.855520366932402e-05,
      "loss": 2.916,
      "step": 2650
    },
    {
      "epoch": 0.29048815114120347,
      "grad_norm": 0.22199465334415436,
      "learning_rate": 4.854974336573114e-05,
      "loss": 2.8967,
      "step": 2660
    },
    {
      "epoch": 0.2915802118597794,
      "grad_norm": 0.5675742030143738,
      "learning_rate": 4.854428306213826e-05,
      "loss": 2.9054,
      "step": 2670
    },
    {
      "epoch": 0.2926722725783554,
      "grad_norm": 0.29147809743881226,
      "learning_rate": 4.853882275854538e-05,
      "loss": 2.8998,
      "step": 2680
    },
    {
      "epoch": 0.2937643332969313,
      "grad_norm": 0.1505354940891266,
      "learning_rate": 4.85333624549525e-05,
      "loss": 2.868,
      "step": 2690
    },
    {
      "epoch": 0.2948563940155073,
      "grad_norm": 0.35323721170425415,
      "learning_rate": 4.8527902151359616e-05,
      "loss": 2.8893,
      "step": 2700
    },
    {
      "epoch": 0.2959484547340832,
      "grad_norm": 0.3195950984954834,
      "learning_rate": 4.8522441847766736e-05,
      "loss": 2.9458,
      "step": 2710
    },
    {
      "epoch": 0.2970405154526592,
      "grad_norm": 0.40220531821250916,
      "learning_rate": 4.8516981544173855e-05,
      "loss": 2.9139,
      "step": 2720
    },
    {
      "epoch": 0.2981325761712351,
      "grad_norm": 0.40036365389823914,
      "learning_rate": 4.8511521240580974e-05,
      "loss": 2.8754,
      "step": 2730
    },
    {
      "epoch": 0.29922463688981105,
      "grad_norm": 0.5412570238113403,
      "learning_rate": 4.85060609369881e-05,
      "loss": 2.9056,
      "step": 2740
    },
    {
      "epoch": 0.30031669760838703,
      "grad_norm": 0.18247854709625244,
      "learning_rate": 4.850060063339522e-05,
      "loss": 2.8834,
      "step": 2750
    },
    {
      "epoch": 0.30140875832696296,
      "grad_norm": 0.11391414701938629,
      "learning_rate": 4.849514032980234e-05,
      "loss": 2.8696,
      "step": 2760
    },
    {
      "epoch": 0.30250081904553894,
      "grad_norm": 0.4551484286785126,
      "learning_rate": 4.848968002620946e-05,
      "loss": 2.9494,
      "step": 2770
    },
    {
      "epoch": 0.30359287976411486,
      "grad_norm": 0.37606510519981384,
      "learning_rate": 4.848421972261658e-05,
      "loss": 2.8813,
      "step": 2780
    },
    {
      "epoch": 0.30468494048269085,
      "grad_norm": 0.6022370457649231,
      "learning_rate": 4.84787594190237e-05,
      "loss": 2.9404,
      "step": 2790
    },
    {
      "epoch": 0.3057770012012668,
      "grad_norm": 0.4802386164665222,
      "learning_rate": 4.847329911543082e-05,
      "loss": 2.9153,
      "step": 2800
    },
    {
      "epoch": 0.30686906191984276,
      "grad_norm": 0.2742842733860016,
      "learning_rate": 4.846783881183794e-05,
      "loss": 2.9127,
      "step": 2810
    },
    {
      "epoch": 0.3079611226384187,
      "grad_norm": 0.33368709683418274,
      "learning_rate": 4.846237850824506e-05,
      "loss": 2.9561,
      "step": 2820
    },
    {
      "epoch": 0.30905318335699467,
      "grad_norm": 0.43931204080581665,
      "learning_rate": 4.845691820465218e-05,
      "loss": 2.882,
      "step": 2830
    },
    {
      "epoch": 0.3101452440755706,
      "grad_norm": 0.14203912019729614,
      "learning_rate": 4.84514579010593e-05,
      "loss": 2.8959,
      "step": 2840
    },
    {
      "epoch": 0.3112373047941466,
      "grad_norm": 0.13201682269573212,
      "learning_rate": 4.844599759746642e-05,
      "loss": 2.9143,
      "step": 2850
    },
    {
      "epoch": 0.3123293655127225,
      "grad_norm": 0.22393260896205902,
      "learning_rate": 4.844053729387354e-05,
      "loss": 2.8998,
      "step": 2860
    },
    {
      "epoch": 0.3134214262312985,
      "grad_norm": 0.101272813975811,
      "learning_rate": 4.8435076990280666e-05,
      "loss": 2.9205,
      "step": 2870
    },
    {
      "epoch": 0.3145134869498744,
      "grad_norm": 0.22014084458351135,
      "learning_rate": 4.8429616686687786e-05,
      "loss": 2.9092,
      "step": 2880
    },
    {
      "epoch": 0.3156055476684504,
      "grad_norm": 0.3929992914199829,
      "learning_rate": 4.8424156383094905e-05,
      "loss": 2.8848,
      "step": 2890
    },
    {
      "epoch": 0.3166976083870263,
      "grad_norm": 0.13756489753723145,
      "learning_rate": 4.8418696079502025e-05,
      "loss": 2.9443,
      "step": 2900
    },
    {
      "epoch": 0.31778966910560225,
      "grad_norm": 0.47740423679351807,
      "learning_rate": 4.8413235775909144e-05,
      "loss": 2.873,
      "step": 2910
    },
    {
      "epoch": 0.31888172982417823,
      "grad_norm": 0.25050392746925354,
      "learning_rate": 4.8407775472316264e-05,
      "loss": 2.925,
      "step": 2920
    },
    {
      "epoch": 0.31997379054275416,
      "grad_norm": 0.33282527327537537,
      "learning_rate": 4.840231516872338e-05,
      "loss": 2.9052,
      "step": 2930
    },
    {
      "epoch": 0.32106585126133014,
      "grad_norm": 0.14785771071910858,
      "learning_rate": 4.83968548651305e-05,
      "loss": 2.952,
      "step": 2940
    },
    {
      "epoch": 0.32215791197990606,
      "grad_norm": 0.1608937829732895,
      "learning_rate": 4.839139456153762e-05,
      "loss": 2.9121,
      "step": 2950
    },
    {
      "epoch": 0.32324997269848205,
      "grad_norm": 0.2914401590824127,
      "learning_rate": 4.838593425794474e-05,
      "loss": 2.917,
      "step": 2960
    },
    {
      "epoch": 0.324342033417058,
      "grad_norm": 0.887787938117981,
      "learning_rate": 4.838047395435186e-05,
      "loss": 2.9863,
      "step": 2970
    },
    {
      "epoch": 0.32543409413563396,
      "grad_norm": 0.2800025939941406,
      "learning_rate": 4.837501365075898e-05,
      "loss": 2.9,
      "step": 2980
    },
    {
      "epoch": 0.3265261548542099,
      "grad_norm": 0.16748149693012238,
      "learning_rate": 4.83695533471661e-05,
      "loss": 2.8863,
      "step": 2990
    },
    {
      "epoch": 0.32761821557278586,
      "grad_norm": 0.21588650345802307,
      "learning_rate": 4.8364093043573226e-05,
      "loss": 2.9086,
      "step": 3000
    },
    {
      "epoch": 0.3287102762913618,
      "grad_norm": 0.3386605679988861,
      "learning_rate": 4.8358632739980345e-05,
      "loss": 2.8664,
      "step": 3010
    },
    {
      "epoch": 0.3298023370099378,
      "grad_norm": 0.6915054321289062,
      "learning_rate": 4.8353172436387464e-05,
      "loss": 2.8992,
      "step": 3020
    },
    {
      "epoch": 0.3308943977285137,
      "grad_norm": 0.18876276910305023,
      "learning_rate": 4.8347712132794584e-05,
      "loss": 2.9081,
      "step": 3030
    },
    {
      "epoch": 0.3319864584470897,
      "grad_norm": 0.2330578714609146,
      "learning_rate": 4.834225182920171e-05,
      "loss": 2.9017,
      "step": 3040
    },
    {
      "epoch": 0.3330785191656656,
      "grad_norm": 0.41990527510643005,
      "learning_rate": 4.833679152560883e-05,
      "loss": 2.9359,
      "step": 3050
    },
    {
      "epoch": 0.33417057988424154,
      "grad_norm": 0.2211618572473526,
      "learning_rate": 4.833133122201595e-05,
      "loss": 2.9019,
      "step": 3060
    },
    {
      "epoch": 0.3352626406028175,
      "grad_norm": 0.49761566519737244,
      "learning_rate": 4.832587091842307e-05,
      "loss": 2.9021,
      "step": 3070
    },
    {
      "epoch": 0.33635470132139345,
      "grad_norm": 0.11679910868406296,
      "learning_rate": 4.832041061483019e-05,
      "loss": 2.9565,
      "step": 3080
    },
    {
      "epoch": 0.33744676203996943,
      "grad_norm": 0.15070748329162598,
      "learning_rate": 4.831495031123731e-05,
      "loss": 2.8977,
      "step": 3090
    },
    {
      "epoch": 0.33853882275854535,
      "grad_norm": 0.5633193254470825,
      "learning_rate": 4.8309490007644426e-05,
      "loss": 2.9173,
      "step": 3100
    },
    {
      "epoch": 0.33963088347712134,
      "grad_norm": 0.30738863348960876,
      "learning_rate": 4.8304029704051546e-05,
      "loss": 2.9245,
      "step": 3110
    },
    {
      "epoch": 0.34072294419569726,
      "grad_norm": 0.1465657353401184,
      "learning_rate": 4.8298569400458665e-05,
      "loss": 2.9044,
      "step": 3120
    },
    {
      "epoch": 0.34181500491427325,
      "grad_norm": 0.11495384573936462,
      "learning_rate": 4.829310909686579e-05,
      "loss": 2.8864,
      "step": 3130
    },
    {
      "epoch": 0.3429070656328492,
      "grad_norm": 0.17071515321731567,
      "learning_rate": 4.828764879327291e-05,
      "loss": 2.8899,
      "step": 3140
    },
    {
      "epoch": 0.34399912635142516,
      "grad_norm": 1.304741621017456,
      "learning_rate": 4.828218848968003e-05,
      "loss": 2.8839,
      "step": 3150
    },
    {
      "epoch": 0.3450911870700011,
      "grad_norm": 0.15810902416706085,
      "learning_rate": 4.827672818608715e-05,
      "loss": 2.8665,
      "step": 3160
    },
    {
      "epoch": 0.34618324778857706,
      "grad_norm": 0.09788883477449417,
      "learning_rate": 4.827126788249427e-05,
      "loss": 2.9015,
      "step": 3170
    },
    {
      "epoch": 0.347275308507153,
      "grad_norm": 0.19689181447029114,
      "learning_rate": 4.826580757890139e-05,
      "loss": 2.8903,
      "step": 3180
    },
    {
      "epoch": 0.348367369225729,
      "grad_norm": 0.20874309539794922,
      "learning_rate": 4.826034727530851e-05,
      "loss": 2.9252,
      "step": 3190
    },
    {
      "epoch": 0.3494594299443049,
      "grad_norm": 0.17090442776679993,
      "learning_rate": 4.825488697171563e-05,
      "loss": 2.9258,
      "step": 3200
    },
    {
      "epoch": 0.3505514906628809,
      "grad_norm": 0.4697932302951813,
      "learning_rate": 4.824942666812275e-05,
      "loss": 2.9007,
      "step": 3210
    },
    {
      "epoch": 0.3516435513814568,
      "grad_norm": 0.13707999885082245,
      "learning_rate": 4.8243966364529866e-05,
      "loss": 2.9037,
      "step": 3220
    },
    {
      "epoch": 0.35273561210003274,
      "grad_norm": 0.41324472427368164,
      "learning_rate": 4.8238506060936986e-05,
      "loss": 2.8986,
      "step": 3230
    },
    {
      "epoch": 0.3538276728186087,
      "grad_norm": 0.1734125316143036,
      "learning_rate": 4.823304575734411e-05,
      "loss": 2.8819,
      "step": 3240
    },
    {
      "epoch": 0.35491973353718465,
      "grad_norm": 0.3959606885910034,
      "learning_rate": 4.822758545375123e-05,
      "loss": 2.8818,
      "step": 3250
    },
    {
      "epoch": 0.3560117942557606,
      "grad_norm": 0.12635990977287292,
      "learning_rate": 4.822212515015835e-05,
      "loss": 2.9127,
      "step": 3260
    },
    {
      "epoch": 0.35710385497433655,
      "grad_norm": 1.1843936443328857,
      "learning_rate": 4.821666484656548e-05,
      "loss": 2.8919,
      "step": 3270
    },
    {
      "epoch": 0.35819591569291254,
      "grad_norm": 0.351077139377594,
      "learning_rate": 4.8211204542972596e-05,
      "loss": 2.8751,
      "step": 3280
    },
    {
      "epoch": 0.35928797641148846,
      "grad_norm": 0.19103915989398956,
      "learning_rate": 4.8205744239379716e-05,
      "loss": 2.8896,
      "step": 3290
    },
    {
      "epoch": 0.36038003713006445,
      "grad_norm": 0.9219859838485718,
      "learning_rate": 4.8200283935786835e-05,
      "loss": 2.8963,
      "step": 3300
    },
    {
      "epoch": 0.3614720978486404,
      "grad_norm": 0.27831870317459106,
      "learning_rate": 4.8194823632193954e-05,
      "loss": 2.8653,
      "step": 3310
    },
    {
      "epoch": 0.36256415856721635,
      "grad_norm": 1.4494452476501465,
      "learning_rate": 4.8189363328601074e-05,
      "loss": 2.9663,
      "step": 3320
    },
    {
      "epoch": 0.3636562192857923,
      "grad_norm": 0.2311297059059143,
      "learning_rate": 4.818390302500819e-05,
      "loss": 2.8974,
      "step": 3330
    },
    {
      "epoch": 0.36474828000436826,
      "grad_norm": 0.11412934958934784,
      "learning_rate": 4.817844272141531e-05,
      "loss": 2.8769,
      "step": 3340
    },
    {
      "epoch": 0.3658403407229442,
      "grad_norm": 0.16960258781909943,
      "learning_rate": 4.817298241782243e-05,
      "loss": 2.8792,
      "step": 3350
    },
    {
      "epoch": 0.3669324014415202,
      "grad_norm": 0.5966744422912598,
      "learning_rate": 4.816752211422955e-05,
      "loss": 2.9136,
      "step": 3360
    },
    {
      "epoch": 0.3680244621600961,
      "grad_norm": 0.22691306471824646,
      "learning_rate": 4.816206181063667e-05,
      "loss": 2.8792,
      "step": 3370
    },
    {
      "epoch": 0.369116522878672,
      "grad_norm": 0.1626768410205841,
      "learning_rate": 4.815660150704379e-05,
      "loss": 2.8683,
      "step": 3380
    },
    {
      "epoch": 0.370208583597248,
      "grad_norm": 0.521622359752655,
      "learning_rate": 4.8151141203450916e-05,
      "loss": 2.9014,
      "step": 3390
    },
    {
      "epoch": 0.37130064431582394,
      "grad_norm": 0.17117886245250702,
      "learning_rate": 4.8145680899858036e-05,
      "loss": 2.8523,
      "step": 3400
    },
    {
      "epoch": 0.3723927050343999,
      "grad_norm": 0.2601087689399719,
      "learning_rate": 4.8140220596265155e-05,
      "loss": 2.8765,
      "step": 3410
    },
    {
      "epoch": 0.37348476575297584,
      "grad_norm": 0.17942580580711365,
      "learning_rate": 4.8134760292672275e-05,
      "loss": 2.9012,
      "step": 3420
    },
    {
      "epoch": 0.3745768264715518,
      "grad_norm": 0.13069775700569153,
      "learning_rate": 4.8129299989079394e-05,
      "loss": 2.8707,
      "step": 3430
    },
    {
      "epoch": 0.37566888719012775,
      "grad_norm": 0.34677213430404663,
      "learning_rate": 4.8123839685486513e-05,
      "loss": 2.9266,
      "step": 3440
    },
    {
      "epoch": 0.37676094790870374,
      "grad_norm": 0.18746648728847504,
      "learning_rate": 4.811837938189363e-05,
      "loss": 2.8903,
      "step": 3450
    },
    {
      "epoch": 0.37785300862727966,
      "grad_norm": 0.6324611902236938,
      "learning_rate": 4.811291907830075e-05,
      "loss": 2.878,
      "step": 3460
    },
    {
      "epoch": 0.37894506934585565,
      "grad_norm": 0.10620764642953873,
      "learning_rate": 4.810745877470788e-05,
      "loss": 2.9087,
      "step": 3470
    },
    {
      "epoch": 0.38003713006443157,
      "grad_norm": 0.16709192097187042,
      "learning_rate": 4.8101998471115e-05,
      "loss": 2.9053,
      "step": 3480
    },
    {
      "epoch": 0.38112919078300755,
      "grad_norm": 0.1643228530883789,
      "learning_rate": 4.809653816752212e-05,
      "loss": 2.8784,
      "step": 3490
    },
    {
      "epoch": 0.3822212515015835,
      "grad_norm": 0.28658628463745117,
      "learning_rate": 4.809107786392924e-05,
      "loss": 2.8813,
      "step": 3500
    },
    {
      "epoch": 0.38331331222015946,
      "grad_norm": 0.11247728019952774,
      "learning_rate": 4.8085617560336356e-05,
      "loss": 2.8526,
      "step": 3510
    },
    {
      "epoch": 0.3844053729387354,
      "grad_norm": 0.10633261501789093,
      "learning_rate": 4.808015725674348e-05,
      "loss": 2.8816,
      "step": 3520
    },
    {
      "epoch": 0.3854974336573114,
      "grad_norm": 0.17377226054668427,
      "learning_rate": 4.80746969531506e-05,
      "loss": 2.8681,
      "step": 3530
    },
    {
      "epoch": 0.3865894943758873,
      "grad_norm": 0.7451058030128479,
      "learning_rate": 4.806923664955772e-05,
      "loss": 2.9122,
      "step": 3540
    },
    {
      "epoch": 0.3876815550944632,
      "grad_norm": 0.26748931407928467,
      "learning_rate": 4.806377634596484e-05,
      "loss": 2.9161,
      "step": 3550
    },
    {
      "epoch": 0.3887736158130392,
      "grad_norm": 0.4473212659358978,
      "learning_rate": 4.805831604237196e-05,
      "loss": 2.9528,
      "step": 3560
    },
    {
      "epoch": 0.38986567653161514,
      "grad_norm": 0.16916316747665405,
      "learning_rate": 4.805285573877908e-05,
      "loss": 2.9676,
      "step": 3570
    },
    {
      "epoch": 0.3909577372501911,
      "grad_norm": 0.17978593707084656,
      "learning_rate": 4.80473954351862e-05,
      "loss": 2.9314,
      "step": 3580
    },
    {
      "epoch": 0.39204979796876704,
      "grad_norm": 0.19689017534255981,
      "learning_rate": 4.804193513159332e-05,
      "loss": 2.8868,
      "step": 3590
    },
    {
      "epoch": 0.393141858687343,
      "grad_norm": 1.4586466550827026,
      "learning_rate": 4.803647482800044e-05,
      "loss": 2.9084,
      "step": 3600
    },
    {
      "epoch": 0.39423391940591895,
      "grad_norm": 0.32127615809440613,
      "learning_rate": 4.803101452440756e-05,
      "loss": 2.8885,
      "step": 3610
    },
    {
      "epoch": 0.39532598012449494,
      "grad_norm": 0.20530399680137634,
      "learning_rate": 4.8025554220814676e-05,
      "loss": 2.8678,
      "step": 3620
    },
    {
      "epoch": 0.39641804084307086,
      "grad_norm": 0.14254261553287506,
      "learning_rate": 4.8020093917221796e-05,
      "loss": 2.9117,
      "step": 3630
    },
    {
      "epoch": 0.39751010156164684,
      "grad_norm": 0.1588750183582306,
      "learning_rate": 4.8014633613628915e-05,
      "loss": 2.8672,
      "step": 3640
    },
    {
      "epoch": 0.39860216228022277,
      "grad_norm": 0.4920077919960022,
      "learning_rate": 4.800917331003604e-05,
      "loss": 2.9237,
      "step": 3650
    },
    {
      "epoch": 0.39969422299879875,
      "grad_norm": 0.3343278765678406,
      "learning_rate": 4.800371300644316e-05,
      "loss": 2.9652,
      "step": 3660
    },
    {
      "epoch": 0.4007862837173747,
      "grad_norm": 0.23369181156158447,
      "learning_rate": 4.799825270285028e-05,
      "loss": 2.8762,
      "step": 3670
    },
    {
      "epoch": 0.40187834443595066,
      "grad_norm": 0.11062227189540863,
      "learning_rate": 4.79927923992574e-05,
      "loss": 2.8957,
      "step": 3680
    },
    {
      "epoch": 0.4029704051545266,
      "grad_norm": 1.3800878524780273,
      "learning_rate": 4.798733209566452e-05,
      "loss": 2.9341,
      "step": 3690
    },
    {
      "epoch": 0.40406246587310257,
      "grad_norm": 0.40451785922050476,
      "learning_rate": 4.7981871792071645e-05,
      "loss": 2.9007,
      "step": 3700
    },
    {
      "epoch": 0.4051545265916785,
      "grad_norm": 0.2796151041984558,
      "learning_rate": 4.7976411488478765e-05,
      "loss": 2.8877,
      "step": 3710
    },
    {
      "epoch": 0.4062465873102544,
      "grad_norm": 0.15423762798309326,
      "learning_rate": 4.7970951184885884e-05,
      "loss": 2.8927,
      "step": 3720
    },
    {
      "epoch": 0.4073386480288304,
      "grad_norm": 0.6606218814849854,
      "learning_rate": 4.7965490881293003e-05,
      "loss": 2.9756,
      "step": 3730
    },
    {
      "epoch": 0.40843070874740633,
      "grad_norm": 0.27572575211524963,
      "learning_rate": 4.796003057770012e-05,
      "loss": 2.8967,
      "step": 3740
    },
    {
      "epoch": 0.4095227694659823,
      "grad_norm": 0.2629111409187317,
      "learning_rate": 4.795457027410724e-05,
      "loss": 2.8816,
      "step": 3750
    },
    {
      "epoch": 0.41061483018455824,
      "grad_norm": 0.2311195731163025,
      "learning_rate": 4.794910997051436e-05,
      "loss": 2.9478,
      "step": 3760
    },
    {
      "epoch": 0.4117068909031342,
      "grad_norm": 0.33298179507255554,
      "learning_rate": 4.794364966692148e-05,
      "loss": 2.8824,
      "step": 3770
    },
    {
      "epoch": 0.41279895162171015,
      "grad_norm": 0.2530178129673004,
      "learning_rate": 4.793818936332861e-05,
      "loss": 2.9077,
      "step": 3780
    },
    {
      "epoch": 0.41389101234028614,
      "grad_norm": 0.5152281522750854,
      "learning_rate": 4.793272905973573e-05,
      "loss": 3.0626,
      "step": 3790
    },
    {
      "epoch": 0.41498307305886206,
      "grad_norm": 0.32775089144706726,
      "learning_rate": 4.7927268756142846e-05,
      "loss": 2.8738,
      "step": 3800
    },
    {
      "epoch": 0.41607513377743804,
      "grad_norm": 0.3388046622276306,
      "learning_rate": 4.7921808452549966e-05,
      "loss": 2.9144,
      "step": 3810
    },
    {
      "epoch": 0.41716719449601397,
      "grad_norm": 0.38871923089027405,
      "learning_rate": 4.7916348148957085e-05,
      "loss": 2.9181,
      "step": 3820
    },
    {
      "epoch": 0.41825925521458995,
      "grad_norm": 0.14106884598731995,
      "learning_rate": 4.7910887845364204e-05,
      "loss": 2.8856,
      "step": 3830
    },
    {
      "epoch": 0.4193513159331659,
      "grad_norm": 0.1644401252269745,
      "learning_rate": 4.7905427541771324e-05,
      "loss": 2.8916,
      "step": 3840
    },
    {
      "epoch": 0.42044337665174186,
      "grad_norm": 0.6004515886306763,
      "learning_rate": 4.789996723817844e-05,
      "loss": 2.8709,
      "step": 3850
    },
    {
      "epoch": 0.4215354373703178,
      "grad_norm": 0.2490701824426651,
      "learning_rate": 4.789450693458556e-05,
      "loss": 2.8937,
      "step": 3860
    },
    {
      "epoch": 0.4226274980888937,
      "grad_norm": 0.15462006628513336,
      "learning_rate": 4.788904663099268e-05,
      "loss": 2.8743,
      "step": 3870
    },
    {
      "epoch": 0.4237195588074697,
      "grad_norm": 0.11352517455816269,
      "learning_rate": 4.78835863273998e-05,
      "loss": 2.8561,
      "step": 3880
    },
    {
      "epoch": 0.4248116195260456,
      "grad_norm": 0.2268199324607849,
      "learning_rate": 4.787812602380692e-05,
      "loss": 2.9012,
      "step": 3890
    },
    {
      "epoch": 0.4259036802446216,
      "grad_norm": 0.23080554604530334,
      "learning_rate": 4.787266572021405e-05,
      "loss": 2.8637,
      "step": 3900
    },
    {
      "epoch": 0.42699574096319753,
      "grad_norm": 0.728975236415863,
      "learning_rate": 4.7867205416621166e-05,
      "loss": 2.881,
      "step": 3910
    },
    {
      "epoch": 0.4280878016817735,
      "grad_norm": 0.09993771463632584,
      "learning_rate": 4.7861745113028286e-05,
      "loss": 2.9027,
      "step": 3920
    },
    {
      "epoch": 0.42917986240034944,
      "grad_norm": 0.16009634733200073,
      "learning_rate": 4.785628480943541e-05,
      "loss": 2.9222,
      "step": 3930
    },
    {
      "epoch": 0.4302719231189254,
      "grad_norm": 1.0807355642318726,
      "learning_rate": 4.785082450584253e-05,
      "loss": 2.8987,
      "step": 3940
    },
    {
      "epoch": 0.43136398383750135,
      "grad_norm": 0.3623906373977661,
      "learning_rate": 4.784536420224965e-05,
      "loss": 2.9038,
      "step": 3950
    },
    {
      "epoch": 0.43245604455607733,
      "grad_norm": 0.1212286651134491,
      "learning_rate": 4.783990389865677e-05,
      "loss": 2.8899,
      "step": 3960
    },
    {
      "epoch": 0.43354810527465326,
      "grad_norm": 0.2107001096010208,
      "learning_rate": 4.783444359506389e-05,
      "loss": 2.9541,
      "step": 3970
    },
    {
      "epoch": 0.43464016599322924,
      "grad_norm": 0.21049320697784424,
      "learning_rate": 4.782898329147101e-05,
      "loss": 2.891,
      "step": 3980
    },
    {
      "epoch": 0.43573222671180517,
      "grad_norm": 0.16851823031902313,
      "learning_rate": 4.782352298787813e-05,
      "loss": 2.9577,
      "step": 3990
    },
    {
      "epoch": 0.43682428743038115,
      "grad_norm": 0.1824929565191269,
      "learning_rate": 4.781806268428525e-05,
      "loss": 2.8963,
      "step": 4000
    },
    {
      "epoch": 0.4379163481489571,
      "grad_norm": 0.1692807525396347,
      "learning_rate": 4.781260238069237e-05,
      "loss": 2.8826,
      "step": 4010
    },
    {
      "epoch": 0.43900840886753306,
      "grad_norm": 0.11467009037733078,
      "learning_rate": 4.780714207709949e-05,
      "loss": 2.8928,
      "step": 4020
    },
    {
      "epoch": 0.440100469586109,
      "grad_norm": 0.24330008029937744,
      "learning_rate": 4.7801681773506606e-05,
      "loss": 2.9001,
      "step": 4030
    },
    {
      "epoch": 0.4411925303046849,
      "grad_norm": 0.4021991491317749,
      "learning_rate": 4.7796221469913726e-05,
      "loss": 2.9135,
      "step": 4040
    },
    {
      "epoch": 0.4422845910232609,
      "grad_norm": 0.1974569857120514,
      "learning_rate": 4.779076116632085e-05,
      "loss": 2.8879,
      "step": 4050
    },
    {
      "epoch": 0.4433766517418368,
      "grad_norm": 0.7483522891998291,
      "learning_rate": 4.778530086272797e-05,
      "loss": 2.891,
      "step": 4060
    },
    {
      "epoch": 0.4444687124604128,
      "grad_norm": 0.48694124817848206,
      "learning_rate": 4.777984055913509e-05,
      "loss": 2.8972,
      "step": 4070
    },
    {
      "epoch": 0.44556077317898873,
      "grad_norm": 0.10758813470602036,
      "learning_rate": 4.777438025554221e-05,
      "loss": 2.8899,
      "step": 4080
    },
    {
      "epoch": 0.4466528338975647,
      "grad_norm": 1.3143153190612793,
      "learning_rate": 4.776891995194933e-05,
      "loss": 2.9778,
      "step": 4090
    },
    {
      "epoch": 0.44774489461614064,
      "grad_norm": 0.24051204323768616,
      "learning_rate": 4.776345964835645e-05,
      "loss": 2.8654,
      "step": 4100
    },
    {
      "epoch": 0.4488369553347166,
      "grad_norm": 0.2751716375350952,
      "learning_rate": 4.775799934476357e-05,
      "loss": 2.8837,
      "step": 4110
    },
    {
      "epoch": 0.44992901605329255,
      "grad_norm": 0.19413091242313385,
      "learning_rate": 4.775253904117069e-05,
      "loss": 2.9211,
      "step": 4120
    },
    {
      "epoch": 0.45102107677186853,
      "grad_norm": 0.22465941309928894,
      "learning_rate": 4.7747078737577814e-05,
      "loss": 2.9181,
      "step": 4130
    },
    {
      "epoch": 0.45211313749044446,
      "grad_norm": 0.16895321011543274,
      "learning_rate": 4.774161843398493e-05,
      "loss": 2.8869,
      "step": 4140
    },
    {
      "epoch": 0.45320519820902044,
      "grad_norm": 0.10551019757986069,
      "learning_rate": 4.773615813039205e-05,
      "loss": 2.8939,
      "step": 4150
    },
    {
      "epoch": 0.45429725892759637,
      "grad_norm": 0.2047460824251175,
      "learning_rate": 4.773069782679917e-05,
      "loss": 2.8777,
      "step": 4160
    },
    {
      "epoch": 0.45538931964617235,
      "grad_norm": 0.30995678901672363,
      "learning_rate": 4.772523752320629e-05,
      "loss": 2.9,
      "step": 4170
    },
    {
      "epoch": 0.4564813803647483,
      "grad_norm": 0.2279433012008667,
      "learning_rate": 4.771977721961342e-05,
      "loss": 2.8998,
      "step": 4180
    },
    {
      "epoch": 0.4575734410833242,
      "grad_norm": 0.1350886970758438,
      "learning_rate": 4.771431691602054e-05,
      "loss": 2.8734,
      "step": 4190
    },
    {
      "epoch": 0.4586655018019002,
      "grad_norm": 0.30770906805992126,
      "learning_rate": 4.7708856612427656e-05,
      "loss": 2.8734,
      "step": 4200
    },
    {
      "epoch": 0.4597575625204761,
      "grad_norm": 0.4173000156879425,
      "learning_rate": 4.7703396308834776e-05,
      "loss": 2.8939,
      "step": 4210
    },
    {
      "epoch": 0.4608496232390521,
      "grad_norm": 0.24423444271087646,
      "learning_rate": 4.7697936005241895e-05,
      "loss": 2.9206,
      "step": 4220
    },
    {
      "epoch": 0.461941683957628,
      "grad_norm": 0.3369072377681732,
      "learning_rate": 4.7692475701649015e-05,
      "loss": 2.8945,
      "step": 4230
    },
    {
      "epoch": 0.463033744676204,
      "grad_norm": 0.09569679200649261,
      "learning_rate": 4.7687015398056134e-05,
      "loss": 2.8595,
      "step": 4240
    },
    {
      "epoch": 0.46412580539477993,
      "grad_norm": 0.16037516295909882,
      "learning_rate": 4.7681555094463253e-05,
      "loss": 2.8906,
      "step": 4250
    },
    {
      "epoch": 0.4652178661133559,
      "grad_norm": 0.18976549804210663,
      "learning_rate": 4.767609479087037e-05,
      "loss": 2.8628,
      "step": 4260
    },
    {
      "epoch": 0.46630992683193184,
      "grad_norm": 0.28207775950431824,
      "learning_rate": 4.767063448727749e-05,
      "loss": 2.9001,
      "step": 4270
    },
    {
      "epoch": 0.4674019875505078,
      "grad_norm": 0.16308392584323883,
      "learning_rate": 4.766517418368461e-05,
      "loss": 2.9244,
      "step": 4280
    },
    {
      "epoch": 0.46849404826908375,
      "grad_norm": 0.3358052968978882,
      "learning_rate": 4.765971388009173e-05,
      "loss": 2.8952,
      "step": 4290
    },
    {
      "epoch": 0.46958610898765973,
      "grad_norm": 0.39937976002693176,
      "learning_rate": 4.765425357649885e-05,
      "loss": 2.9365,
      "step": 4300
    },
    {
      "epoch": 0.47067816970623566,
      "grad_norm": 0.1816020905971527,
      "learning_rate": 4.764879327290598e-05,
      "loss": 2.9261,
      "step": 4310
    },
    {
      "epoch": 0.47177023042481164,
      "grad_norm": 0.3324086666107178,
      "learning_rate": 4.7643332969313096e-05,
      "loss": 2.9077,
      "step": 4320
    },
    {
      "epoch": 0.47286229114338757,
      "grad_norm": 0.2728114724159241,
      "learning_rate": 4.7637872665720215e-05,
      "loss": 2.8853,
      "step": 4330
    },
    {
      "epoch": 0.47395435186196355,
      "grad_norm": 0.19577330350875854,
      "learning_rate": 4.7632412362127335e-05,
      "loss": 2.8755,
      "step": 4340
    },
    {
      "epoch": 0.4750464125805395,
      "grad_norm": 0.8666092753410339,
      "learning_rate": 4.7626952058534454e-05,
      "loss": 2.894,
      "step": 4350
    },
    {
      "epoch": 0.4761384732991154,
      "grad_norm": 0.11834555864334106,
      "learning_rate": 4.762149175494158e-05,
      "loss": 2.8966,
      "step": 4360
    },
    {
      "epoch": 0.4772305340176914,
      "grad_norm": 0.3349895179271698,
      "learning_rate": 4.76160314513487e-05,
      "loss": 2.9142,
      "step": 4370
    },
    {
      "epoch": 0.4783225947362673,
      "grad_norm": 0.4616653621196747,
      "learning_rate": 4.761057114775582e-05,
      "loss": 2.9087,
      "step": 4380
    },
    {
      "epoch": 0.4794146554548433,
      "grad_norm": 0.10732448846101761,
      "learning_rate": 4.760511084416294e-05,
      "loss": 2.895,
      "step": 4390
    },
    {
      "epoch": 0.4805067161734192,
      "grad_norm": 0.19530825316905975,
      "learning_rate": 4.759965054057006e-05,
      "loss": 2.9166,
      "step": 4400
    },
    {
      "epoch": 0.4815987768919952,
      "grad_norm": 0.21237628161907196,
      "learning_rate": 4.759419023697718e-05,
      "loss": 2.8796,
      "step": 4410
    },
    {
      "epoch": 0.48269083761057113,
      "grad_norm": 0.38689616322517395,
      "learning_rate": 4.75887299333843e-05,
      "loss": 2.8824,
      "step": 4420
    },
    {
      "epoch": 0.4837828983291471,
      "grad_norm": 0.11188182234764099,
      "learning_rate": 4.7583269629791416e-05,
      "loss": 2.8615,
      "step": 4430
    },
    {
      "epoch": 0.48487495904772304,
      "grad_norm": 0.15232092142105103,
      "learning_rate": 4.757780932619854e-05,
      "loss": 2.869,
      "step": 4440
    },
    {
      "epoch": 0.485967019766299,
      "grad_norm": 0.16383905708789825,
      "learning_rate": 4.757234902260566e-05,
      "loss": 2.8615,
      "step": 4450
    },
    {
      "epoch": 0.48705908048487495,
      "grad_norm": 0.2445257157087326,
      "learning_rate": 4.756688871901278e-05,
      "loss": 2.8768,
      "step": 4460
    },
    {
      "epoch": 0.48815114120345093,
      "grad_norm": 0.11653807014226913,
      "learning_rate": 4.75614284154199e-05,
      "loss": 2.881,
      "step": 4470
    },
    {
      "epoch": 0.48924320192202686,
      "grad_norm": 0.27794405817985535,
      "learning_rate": 4.755596811182702e-05,
      "loss": 2.901,
      "step": 4480
    },
    {
      "epoch": 0.49033526264060284,
      "grad_norm": 0.7962833642959595,
      "learning_rate": 4.755050780823414e-05,
      "loss": 2.9295,
      "step": 4490
    },
    {
      "epoch": 0.49142732335917877,
      "grad_norm": 1.3956331014633179,
      "learning_rate": 4.754504750464126e-05,
      "loss": 2.9088,
      "step": 4500
    },
    {
      "epoch": 0.4925193840777547,
      "grad_norm": 0.15926852822303772,
      "learning_rate": 4.753958720104838e-05,
      "loss": 2.8791,
      "step": 4510
    },
    {
      "epoch": 0.4936114447963307,
      "grad_norm": 0.30125749111175537,
      "learning_rate": 4.75341268974555e-05,
      "loss": 2.8708,
      "step": 4520
    },
    {
      "epoch": 0.4947035055149066,
      "grad_norm": 0.5101651549339294,
      "learning_rate": 4.752866659386262e-05,
      "loss": 2.8618,
      "step": 4530
    },
    {
      "epoch": 0.4957955662334826,
      "grad_norm": 0.3257959485054016,
      "learning_rate": 4.752320629026974e-05,
      "loss": 2.9064,
      "step": 4540
    },
    {
      "epoch": 0.4968876269520585,
      "grad_norm": 0.22581246495246887,
      "learning_rate": 4.7517745986676856e-05,
      "loss": 2.9051,
      "step": 4550
    },
    {
      "epoch": 0.4979796876706345,
      "grad_norm": 0.1808708906173706,
      "learning_rate": 4.751228568308398e-05,
      "loss": 2.872,
      "step": 4560
    },
    {
      "epoch": 0.4990717483892104,
      "grad_norm": 0.30505144596099854,
      "learning_rate": 4.75068253794911e-05,
      "loss": 2.9261,
      "step": 4570
    },
    {
      "epoch": 0.5001638091077864,
      "grad_norm": 0.24786566197872162,
      "learning_rate": 4.750136507589822e-05,
      "loss": 2.8883,
      "step": 4580
    },
    {
      "epoch": 0.5012558698263624,
      "grad_norm": 0.16685861349105835,
      "learning_rate": 4.749590477230535e-05,
      "loss": 2.857,
      "step": 4590
    },
    {
      "epoch": 0.5023479305449383,
      "grad_norm": 0.15424880385398865,
      "learning_rate": 4.749044446871247e-05,
      "loss": 2.8929,
      "step": 4600
    },
    {
      "epoch": 0.5034399912635142,
      "grad_norm": 0.1375235915184021,
      "learning_rate": 4.7484984165119586e-05,
      "loss": 2.8683,
      "step": 4610
    },
    {
      "epoch": 0.5045320519820902,
      "grad_norm": 0.43770354986190796,
      "learning_rate": 4.7479523861526705e-05,
      "loss": 2.9865,
      "step": 4620
    },
    {
      "epoch": 0.5056241127006662,
      "grad_norm": 0.2991988956928253,
      "learning_rate": 4.7474063557933825e-05,
      "loss": 2.8985,
      "step": 4630
    },
    {
      "epoch": 0.5067161734192421,
      "grad_norm": 0.1299731284379959,
      "learning_rate": 4.7468603254340944e-05,
      "loss": 2.9163,
      "step": 4640
    },
    {
      "epoch": 0.5078082341378181,
      "grad_norm": 0.11794500052928925,
      "learning_rate": 4.7463142950748064e-05,
      "loss": 2.8817,
      "step": 4650
    },
    {
      "epoch": 0.508900294856394,
      "grad_norm": 0.27602776885032654,
      "learning_rate": 4.745768264715518e-05,
      "loss": 2.9229,
      "step": 4660
    },
    {
      "epoch": 0.5099923555749699,
      "grad_norm": 0.40254291892051697,
      "learning_rate": 4.74522223435623e-05,
      "loss": 2.9297,
      "step": 4670
    },
    {
      "epoch": 0.511084416293546,
      "grad_norm": 0.27360835671424866,
      "learning_rate": 4.744676203996942e-05,
      "loss": 2.8813,
      "step": 4680
    },
    {
      "epoch": 0.5121764770121219,
      "grad_norm": 0.4292996823787689,
      "learning_rate": 4.744130173637654e-05,
      "loss": 2.919,
      "step": 4690
    },
    {
      "epoch": 0.5132685377306978,
      "grad_norm": 0.2208862006664276,
      "learning_rate": 4.743584143278367e-05,
      "loss": 2.8713,
      "step": 4700
    },
    {
      "epoch": 0.5143605984492737,
      "grad_norm": 0.21761618554592133,
      "learning_rate": 4.743038112919079e-05,
      "loss": 2.9151,
      "step": 4710
    },
    {
      "epoch": 0.5154526591678498,
      "grad_norm": 0.21030694246292114,
      "learning_rate": 4.7424920825597906e-05,
      "loss": 2.8951,
      "step": 4720
    },
    {
      "epoch": 0.5165447198864257,
      "grad_norm": 0.4133654832839966,
      "learning_rate": 4.7419460522005026e-05,
      "loss": 2.9233,
      "step": 4730
    },
    {
      "epoch": 0.5176367806050016,
      "grad_norm": 0.150396928191185,
      "learning_rate": 4.7414000218412145e-05,
      "loss": 2.8867,
      "step": 4740
    },
    {
      "epoch": 0.5187288413235775,
      "grad_norm": 0.1976921111345291,
      "learning_rate": 4.7408539914819265e-05,
      "loss": 2.8697,
      "step": 4750
    },
    {
      "epoch": 0.5198209020421536,
      "grad_norm": 0.2699332535266876,
      "learning_rate": 4.7403079611226384e-05,
      "loss": 2.9053,
      "step": 4760
    },
    {
      "epoch": 0.5209129627607295,
      "grad_norm": 0.25323012471199036,
      "learning_rate": 4.7397619307633503e-05,
      "loss": 2.9123,
      "step": 4770
    },
    {
      "epoch": 0.5220050234793054,
      "grad_norm": 0.3560050427913666,
      "learning_rate": 4.739215900404062e-05,
      "loss": 2.8952,
      "step": 4780
    },
    {
      "epoch": 0.5230970841978814,
      "grad_norm": 0.354463130235672,
      "learning_rate": 4.738669870044775e-05,
      "loss": 2.8817,
      "step": 4790
    },
    {
      "epoch": 0.5241891449164574,
      "grad_norm": 0.5674113035202026,
      "learning_rate": 4.738123839685487e-05,
      "loss": 2.9171,
      "step": 4800
    },
    {
      "epoch": 0.5252812056350333,
      "grad_norm": 0.18430648744106293,
      "learning_rate": 4.737577809326199e-05,
      "loss": 2.9011,
      "step": 4810
    },
    {
      "epoch": 0.5263732663536093,
      "grad_norm": 0.18153148889541626,
      "learning_rate": 4.737031778966911e-05,
      "loss": 2.8965,
      "step": 4820
    },
    {
      "epoch": 0.5274653270721852,
      "grad_norm": 0.17066216468811035,
      "learning_rate": 4.7364857486076233e-05,
      "loss": 2.9043,
      "step": 4830
    },
    {
      "epoch": 0.5285573877907611,
      "grad_norm": 0.2731861472129822,
      "learning_rate": 4.735939718248335e-05,
      "loss": 2.9136,
      "step": 4840
    },
    {
      "epoch": 0.5296494485093372,
      "grad_norm": 0.12339148670434952,
      "learning_rate": 4.735393687889047e-05,
      "loss": 2.8807,
      "step": 4850
    },
    {
      "epoch": 0.5307415092279131,
      "grad_norm": 0.19076098501682281,
      "learning_rate": 4.734847657529759e-05,
      "loss": 2.8983,
      "step": 4860
    },
    {
      "epoch": 0.531833569946489,
      "grad_norm": 0.3051305413246155,
      "learning_rate": 4.734301627170471e-05,
      "loss": 2.8962,
      "step": 4870
    },
    {
      "epoch": 0.5329256306650649,
      "grad_norm": 0.15108540654182434,
      "learning_rate": 4.733755596811183e-05,
      "loss": 2.8888,
      "step": 4880
    },
    {
      "epoch": 0.534017691383641,
      "grad_norm": 0.3484257757663727,
      "learning_rate": 4.733209566451895e-05,
      "loss": 2.9183,
      "step": 4890
    },
    {
      "epoch": 0.5351097521022169,
      "grad_norm": 0.47622859477996826,
      "learning_rate": 4.732663536092607e-05,
      "loss": 2.9315,
      "step": 4900
    },
    {
      "epoch": 0.5362018128207928,
      "grad_norm": 0.363350510597229,
      "learning_rate": 4.732117505733319e-05,
      "loss": 2.8892,
      "step": 4910
    },
    {
      "epoch": 0.5372938735393687,
      "grad_norm": 0.20768970251083374,
      "learning_rate": 4.731571475374031e-05,
      "loss": 2.9085,
      "step": 4920
    },
    {
      "epoch": 0.5383859342579448,
      "grad_norm": 1.5081144571304321,
      "learning_rate": 4.731025445014743e-05,
      "loss": 2.9137,
      "step": 4930
    },
    {
      "epoch": 0.5394779949765207,
      "grad_norm": 0.24767087399959564,
      "learning_rate": 4.730479414655455e-05,
      "loss": 2.8677,
      "step": 4940
    },
    {
      "epoch": 0.5405700556950966,
      "grad_norm": 0.16256701946258545,
      "learning_rate": 4.7299333842961666e-05,
      "loss": 2.96,
      "step": 4950
    },
    {
      "epoch": 0.5416621164136726,
      "grad_norm": 0.11128143221139908,
      "learning_rate": 4.729387353936879e-05,
      "loss": 2.8795,
      "step": 4960
    },
    {
      "epoch": 0.5427541771322486,
      "grad_norm": 0.19766968488693237,
      "learning_rate": 4.728841323577591e-05,
      "loss": 2.8516,
      "step": 4970
    },
    {
      "epoch": 0.5438462378508245,
      "grad_norm": 0.39323678612709045,
      "learning_rate": 4.728295293218303e-05,
      "loss": 2.8878,
      "step": 4980
    },
    {
      "epoch": 0.5449382985694005,
      "grad_norm": 0.09096208214759827,
      "learning_rate": 4.727749262859015e-05,
      "loss": 2.8578,
      "step": 4990
    },
    {
      "epoch": 0.5460303592879764,
      "grad_norm": 0.16791391372680664,
      "learning_rate": 4.727203232499727e-05,
      "loss": 2.9535,
      "step": 5000
    },
    {
      "epoch": 0.5471224200065523,
      "grad_norm": 0.12635910511016846,
      "learning_rate": 4.726657202140439e-05,
      "loss": 2.8825,
      "step": 5010
    },
    {
      "epoch": 0.5482144807251284,
      "grad_norm": 0.12442783266305923,
      "learning_rate": 4.7261111717811516e-05,
      "loss": 2.8478,
      "step": 5020
    },
    {
      "epoch": 0.5493065414437043,
      "grad_norm": 0.10586634278297424,
      "learning_rate": 4.7255651414218635e-05,
      "loss": 2.8806,
      "step": 5030
    },
    {
      "epoch": 0.5503986021622802,
      "grad_norm": 0.2090948522090912,
      "learning_rate": 4.7250191110625755e-05,
      "loss": 2.9129,
      "step": 5040
    },
    {
      "epoch": 0.5514906628808561,
      "grad_norm": 0.14285218715667725,
      "learning_rate": 4.7244730807032874e-05,
      "loss": 2.8696,
      "step": 5050
    },
    {
      "epoch": 0.5525827235994322,
      "grad_norm": 0.23730318248271942,
      "learning_rate": 4.7239270503439993e-05,
      "loss": 2.8736,
      "step": 5060
    },
    {
      "epoch": 0.5536747843180081,
      "grad_norm": 0.41601255536079407,
      "learning_rate": 4.723381019984711e-05,
      "loss": 2.8846,
      "step": 5070
    },
    {
      "epoch": 0.554766845036584,
      "grad_norm": 0.6334588527679443,
      "learning_rate": 4.722834989625423e-05,
      "loss": 2.9175,
      "step": 5080
    },
    {
      "epoch": 0.55585890575516,
      "grad_norm": 0.20683753490447998,
      "learning_rate": 4.722288959266136e-05,
      "loss": 2.8857,
      "step": 5090
    },
    {
      "epoch": 0.556950966473736,
      "grad_norm": 0.10346873104572296,
      "learning_rate": 4.721742928906848e-05,
      "loss": 2.8704,
      "step": 5100
    },
    {
      "epoch": 0.5580430271923119,
      "grad_norm": 0.6940188407897949,
      "learning_rate": 4.72119689854756e-05,
      "loss": 2.9794,
      "step": 5110
    },
    {
      "epoch": 0.5591350879108878,
      "grad_norm": 0.21823886036872864,
      "learning_rate": 4.720650868188272e-05,
      "loss": 2.9139,
      "step": 5120
    },
    {
      "epoch": 0.5602271486294638,
      "grad_norm": 0.4252142608165741,
      "learning_rate": 4.7201048378289836e-05,
      "loss": 2.8744,
      "step": 5130
    },
    {
      "epoch": 0.5613192093480398,
      "grad_norm": 0.15142391622066498,
      "learning_rate": 4.7195588074696955e-05,
      "loss": 2.8804,
      "step": 5140
    },
    {
      "epoch": 0.5624112700666157,
      "grad_norm": 0.19545187056064606,
      "learning_rate": 4.7190127771104075e-05,
      "loss": 2.8866,
      "step": 5150
    },
    {
      "epoch": 0.5635033307851917,
      "grad_norm": 0.9557472467422485,
      "learning_rate": 4.7184667467511194e-05,
      "loss": 2.9146,
      "step": 5160
    },
    {
      "epoch": 0.5645953915037676,
      "grad_norm": 0.13140353560447693,
      "learning_rate": 4.7179207163918314e-05,
      "loss": 2.9062,
      "step": 5170
    },
    {
      "epoch": 0.5656874522223435,
      "grad_norm": 0.27822640538215637,
      "learning_rate": 4.717374686032543e-05,
      "loss": 2.9329,
      "step": 5180
    },
    {
      "epoch": 0.5667795129409195,
      "grad_norm": 0.13602270185947418,
      "learning_rate": 4.716828655673255e-05,
      "loss": 2.8605,
      "step": 5190
    },
    {
      "epoch": 0.5678715736594955,
      "grad_norm": 0.3228229284286499,
      "learning_rate": 4.716282625313967e-05,
      "loss": 2.8948,
      "step": 5200
    },
    {
      "epoch": 0.5689636343780714,
      "grad_norm": 0.2570655941963196,
      "learning_rate": 4.715736594954679e-05,
      "loss": 2.8783,
      "step": 5210
    },
    {
      "epoch": 0.5700556950966473,
      "grad_norm": 0.3271787166595459,
      "learning_rate": 4.715190564595392e-05,
      "loss": 2.9218,
      "step": 5220
    },
    {
      "epoch": 0.5711477558152234,
      "grad_norm": 0.26892775297164917,
      "learning_rate": 4.714644534236104e-05,
      "loss": 2.882,
      "step": 5230
    },
    {
      "epoch": 0.5722398165337993,
      "grad_norm": 0.20758463442325592,
      "learning_rate": 4.7140985038768156e-05,
      "loss": 2.9222,
      "step": 5240
    },
    {
      "epoch": 0.5733318772523752,
      "grad_norm": 0.21280215680599213,
      "learning_rate": 4.713552473517528e-05,
      "loss": 2.8753,
      "step": 5250
    },
    {
      "epoch": 0.5744239379709511,
      "grad_norm": 0.2614497244358063,
      "learning_rate": 4.71300644315824e-05,
      "loss": 2.9346,
      "step": 5260
    },
    {
      "epoch": 0.5755159986895272,
      "grad_norm": 0.1461663842201233,
      "learning_rate": 4.712460412798952e-05,
      "loss": 2.9029,
      "step": 5270
    },
    {
      "epoch": 0.5766080594081031,
      "grad_norm": 0.2034318447113037,
      "learning_rate": 4.711914382439664e-05,
      "loss": 2.8832,
      "step": 5280
    },
    {
      "epoch": 0.577700120126679,
      "grad_norm": 0.39449143409729004,
      "learning_rate": 4.711368352080376e-05,
      "loss": 2.9059,
      "step": 5290
    },
    {
      "epoch": 0.578792180845255,
      "grad_norm": 0.22159095108509064,
      "learning_rate": 4.710822321721088e-05,
      "loss": 2.8938,
      "step": 5300
    },
    {
      "epoch": 0.579884241563831,
      "grad_norm": 0.25994521379470825,
      "learning_rate": 4.7102762913618e-05,
      "loss": 2.9555,
      "step": 5310
    },
    {
      "epoch": 0.5809763022824069,
      "grad_norm": 0.21145674586296082,
      "learning_rate": 4.709730261002512e-05,
      "loss": 2.906,
      "step": 5320
    },
    {
      "epoch": 0.5820683630009829,
      "grad_norm": 0.12624654173851013,
      "learning_rate": 4.709184230643224e-05,
      "loss": 2.8854,
      "step": 5330
    },
    {
      "epoch": 0.5831604237195588,
      "grad_norm": 0.43433091044425964,
      "learning_rate": 4.708638200283936e-05,
      "loss": 2.8828,
      "step": 5340
    },
    {
      "epoch": 0.5842524844381347,
      "grad_norm": 0.14890584349632263,
      "learning_rate": 4.708092169924648e-05,
      "loss": 2.881,
      "step": 5350
    },
    {
      "epoch": 0.5853445451567107,
      "grad_norm": 0.12819045782089233,
      "learning_rate": 4.70754613956536e-05,
      "loss": 2.9116,
      "step": 5360
    },
    {
      "epoch": 0.5864366058752867,
      "grad_norm": 0.4840635359287262,
      "learning_rate": 4.707000109206072e-05,
      "loss": 2.9344,
      "step": 5370
    },
    {
      "epoch": 0.5875286665938626,
      "grad_norm": 0.5274866819381714,
      "learning_rate": 4.706454078846784e-05,
      "loss": 2.8574,
      "step": 5380
    },
    {
      "epoch": 0.5886207273124385,
      "grad_norm": 0.3613823652267456,
      "learning_rate": 4.705908048487496e-05,
      "loss": 2.8983,
      "step": 5390
    },
    {
      "epoch": 0.5897127880310146,
      "grad_norm": 0.19890615344047546,
      "learning_rate": 4.705362018128208e-05,
      "loss": 2.8749,
      "step": 5400
    },
    {
      "epoch": 0.5908048487495905,
      "grad_norm": 0.12396806478500366,
      "learning_rate": 4.70481598776892e-05,
      "loss": 2.9496,
      "step": 5410
    },
    {
      "epoch": 0.5918969094681664,
      "grad_norm": 0.41316479444503784,
      "learning_rate": 4.704269957409632e-05,
      "loss": 2.8625,
      "step": 5420
    },
    {
      "epoch": 0.5929889701867423,
      "grad_norm": 0.14244379103183746,
      "learning_rate": 4.703723927050344e-05,
      "loss": 2.9086,
      "step": 5430
    },
    {
      "epoch": 0.5940810309053184,
      "grad_norm": 0.6419324278831482,
      "learning_rate": 4.703177896691056e-05,
      "loss": 2.9004,
      "step": 5440
    },
    {
      "epoch": 0.5951730916238943,
      "grad_norm": 0.28605663776397705,
      "learning_rate": 4.7026318663317684e-05,
      "loss": 2.8792,
      "step": 5450
    },
    {
      "epoch": 0.5962651523424702,
      "grad_norm": 0.2536427080631256,
      "learning_rate": 4.7020858359724804e-05,
      "loss": 2.8644,
      "step": 5460
    },
    {
      "epoch": 0.5973572130610462,
      "grad_norm": 0.32662659883499146,
      "learning_rate": 4.701539805613192e-05,
      "loss": 3.0001,
      "step": 5470
    },
    {
      "epoch": 0.5984492737796221,
      "grad_norm": 0.2602006196975708,
      "learning_rate": 4.700993775253905e-05,
      "loss": 2.9114,
      "step": 5480
    },
    {
      "epoch": 0.5995413344981981,
      "grad_norm": 0.13570518791675568,
      "learning_rate": 4.700447744894617e-05,
      "loss": 2.9111,
      "step": 5490
    },
    {
      "epoch": 0.6006333952167741,
      "grad_norm": 0.11002914607524872,
      "learning_rate": 4.699901714535329e-05,
      "loss": 2.8595,
      "step": 5500
    },
    {
      "epoch": 0.60172545593535,
      "grad_norm": 0.26724347472190857,
      "learning_rate": 4.699355684176041e-05,
      "loss": 2.8664,
      "step": 5510
    },
    {
      "epoch": 0.6028175166539259,
      "grad_norm": 0.8987566828727722,
      "learning_rate": 4.698809653816753e-05,
      "loss": 2.9194,
      "step": 5520
    },
    {
      "epoch": 0.603909577372502,
      "grad_norm": 0.2578795850276947,
      "learning_rate": 4.6982636234574646e-05,
      "loss": 2.9111,
      "step": 5530
    },
    {
      "epoch": 0.6050016380910779,
      "grad_norm": 0.18456056714057922,
      "learning_rate": 4.6977175930981766e-05,
      "loss": 2.8659,
      "step": 5540
    },
    {
      "epoch": 0.6060936988096538,
      "grad_norm": 0.17914173007011414,
      "learning_rate": 4.6971715627388885e-05,
      "loss": 2.87,
      "step": 5550
    },
    {
      "epoch": 0.6071857595282297,
      "grad_norm": 0.34548360109329224,
      "learning_rate": 4.6966255323796005e-05,
      "loss": 2.8743,
      "step": 5560
    },
    {
      "epoch": 0.6082778202468058,
      "grad_norm": 0.11180058121681213,
      "learning_rate": 4.6960795020203124e-05,
      "loss": 2.8716,
      "step": 5570
    },
    {
      "epoch": 0.6093698809653817,
      "grad_norm": 0.2580375075340271,
      "learning_rate": 4.695533471661024e-05,
      "loss": 2.9005,
      "step": 5580
    },
    {
      "epoch": 0.6104619416839576,
      "grad_norm": 0.7339483499526978,
      "learning_rate": 4.694987441301736e-05,
      "loss": 2.9006,
      "step": 5590
    },
    {
      "epoch": 0.6115540024025335,
      "grad_norm": 0.30617809295654297,
      "learning_rate": 4.694441410942448e-05,
      "loss": 2.8712,
      "step": 5600
    },
    {
      "epoch": 0.6126460631211096,
      "grad_norm": 0.17083090543746948,
      "learning_rate": 4.693895380583161e-05,
      "loss": 2.8836,
      "step": 5610
    },
    {
      "epoch": 0.6137381238396855,
      "grad_norm": 0.19302231073379517,
      "learning_rate": 4.693349350223873e-05,
      "loss": 2.9353,
      "step": 5620
    },
    {
      "epoch": 0.6148301845582614,
      "grad_norm": 0.3397597074508667,
      "learning_rate": 4.692803319864585e-05,
      "loss": 2.8681,
      "step": 5630
    },
    {
      "epoch": 0.6159222452768374,
      "grad_norm": 0.2519616484642029,
      "learning_rate": 4.6922572895052967e-05,
      "loss": 2.9113,
      "step": 5640
    },
    {
      "epoch": 0.6170143059954133,
      "grad_norm": 0.1620813012123108,
      "learning_rate": 4.6917112591460086e-05,
      "loss": 2.8752,
      "step": 5650
    },
    {
      "epoch": 0.6181063667139893,
      "grad_norm": 0.1472918540239334,
      "learning_rate": 4.6911652287867205e-05,
      "loss": 2.9005,
      "step": 5660
    },
    {
      "epoch": 0.6191984274325653,
      "grad_norm": 0.13117308914661407,
      "learning_rate": 4.6906191984274325e-05,
      "loss": 2.8779,
      "step": 5670
    },
    {
      "epoch": 0.6202904881511412,
      "grad_norm": 0.17254087328910828,
      "learning_rate": 4.690073168068145e-05,
      "loss": 2.87,
      "step": 5680
    },
    {
      "epoch": 0.6213825488697171,
      "grad_norm": 0.4666428864002228,
      "learning_rate": 4.689527137708857e-05,
      "loss": 2.9312,
      "step": 5690
    },
    {
      "epoch": 0.6224746095882931,
      "grad_norm": 0.4020206928253174,
      "learning_rate": 4.688981107349569e-05,
      "loss": 2.933,
      "step": 5700
    },
    {
      "epoch": 0.6235666703068691,
      "grad_norm": 0.15589581429958344,
      "learning_rate": 4.688435076990281e-05,
      "loss": 2.8834,
      "step": 5710
    },
    {
      "epoch": 0.624658731025445,
      "grad_norm": 0.17500118911266327,
      "learning_rate": 4.687889046630993e-05,
      "loss": 2.854,
      "step": 5720
    },
    {
      "epoch": 0.6257507917440209,
      "grad_norm": 0.3170117735862732,
      "learning_rate": 4.687343016271705e-05,
      "loss": 2.9673,
      "step": 5730
    },
    {
      "epoch": 0.626842852462597,
      "grad_norm": 0.36342963576316833,
      "learning_rate": 4.6867969859124174e-05,
      "loss": 2.8796,
      "step": 5740
    },
    {
      "epoch": 0.6279349131811729,
      "grad_norm": 0.16191984713077545,
      "learning_rate": 4.6862509555531294e-05,
      "loss": 2.865,
      "step": 5750
    },
    {
      "epoch": 0.6290269738997488,
      "grad_norm": 0.21674685180187225,
      "learning_rate": 4.685704925193841e-05,
      "loss": 2.8586,
      "step": 5760
    },
    {
      "epoch": 0.6301190346183247,
      "grad_norm": 0.1435011327266693,
      "learning_rate": 4.685158894834553e-05,
      "loss": 2.8899,
      "step": 5770
    },
    {
      "epoch": 0.6312110953369008,
      "grad_norm": 0.8730763792991638,
      "learning_rate": 4.684612864475265e-05,
      "loss": 2.8971,
      "step": 5780
    },
    {
      "epoch": 0.6323031560554767,
      "grad_norm": 0.14078868925571442,
      "learning_rate": 4.684066834115977e-05,
      "loss": 2.8807,
      "step": 5790
    },
    {
      "epoch": 0.6333952167740526,
      "grad_norm": 0.15244139730930328,
      "learning_rate": 4.683520803756689e-05,
      "loss": 2.9226,
      "step": 5800
    },
    {
      "epoch": 0.6344872774926286,
      "grad_norm": 0.10958322882652283,
      "learning_rate": 4.682974773397401e-05,
      "loss": 2.872,
      "step": 5810
    },
    {
      "epoch": 0.6355793382112045,
      "grad_norm": 0.20410828292369843,
      "learning_rate": 4.682428743038113e-05,
      "loss": 2.8607,
      "step": 5820
    },
    {
      "epoch": 0.6366713989297805,
      "grad_norm": 0.2724675238132477,
      "learning_rate": 4.681882712678825e-05,
      "loss": 2.9565,
      "step": 5830
    },
    {
      "epoch": 0.6377634596483565,
      "grad_norm": 0.11276555806398392,
      "learning_rate": 4.681336682319537e-05,
      "loss": 2.8687,
      "step": 5840
    },
    {
      "epoch": 0.6388555203669324,
      "grad_norm": 0.18285922706127167,
      "learning_rate": 4.680790651960249e-05,
      "loss": 2.8664,
      "step": 5850
    },
    {
      "epoch": 0.6399475810855083,
      "grad_norm": 0.19218328595161438,
      "learning_rate": 4.680244621600961e-05,
      "loss": 2.9,
      "step": 5860
    },
    {
      "epoch": 0.6410396418040843,
      "grad_norm": 0.729701578617096,
      "learning_rate": 4.679698591241673e-05,
      "loss": 2.9513,
      "step": 5870
    },
    {
      "epoch": 0.6421317025226603,
      "grad_norm": 0.2203618735074997,
      "learning_rate": 4.679152560882385e-05,
      "loss": 2.9092,
      "step": 5880
    },
    {
      "epoch": 0.6432237632412362,
      "grad_norm": 0.15059272944927216,
      "learning_rate": 4.678606530523097e-05,
      "loss": 2.8568,
      "step": 5890
    },
    {
      "epoch": 0.6443158239598121,
      "grad_norm": 0.11015430837869644,
      "learning_rate": 4.678060500163809e-05,
      "loss": 2.8774,
      "step": 5900
    },
    {
      "epoch": 0.6454078846783882,
      "grad_norm": 0.21496208012104034,
      "learning_rate": 4.677514469804522e-05,
      "loss": 2.9204,
      "step": 5910
    },
    {
      "epoch": 0.6464999453969641,
      "grad_norm": 0.1529788076877594,
      "learning_rate": 4.676968439445234e-05,
      "loss": 2.8707,
      "step": 5920
    },
    {
      "epoch": 0.64759200611554,
      "grad_norm": 0.10105786472558975,
      "learning_rate": 4.6764224090859457e-05,
      "loss": 2.8835,
      "step": 5930
    },
    {
      "epoch": 0.648684066834116,
      "grad_norm": 0.226875901222229,
      "learning_rate": 4.6758763787266576e-05,
      "loss": 2.8904,
      "step": 5940
    },
    {
      "epoch": 0.649776127552692,
      "grad_norm": 0.2522377073764801,
      "learning_rate": 4.6753303483673695e-05,
      "loss": 2.8827,
      "step": 5950
    },
    {
      "epoch": 0.6508681882712679,
      "grad_norm": 0.20721742510795593,
      "learning_rate": 4.6747843180080815e-05,
      "loss": 2.8933,
      "step": 5960
    },
    {
      "epoch": 0.6519602489898438,
      "grad_norm": 0.1899692714214325,
      "learning_rate": 4.6742382876487934e-05,
      "loss": 2.8919,
      "step": 5970
    },
    {
      "epoch": 0.6530523097084198,
      "grad_norm": 0.15915946662425995,
      "learning_rate": 4.6736922572895054e-05,
      "loss": 2.8817,
      "step": 5980
    },
    {
      "epoch": 0.6541443704269957,
      "grad_norm": 0.11170276999473572,
      "learning_rate": 4.673146226930217e-05,
      "loss": 2.8703,
      "step": 5990
    },
    {
      "epoch": 0.6552364311455717,
      "grad_norm": 0.163194939494133,
      "learning_rate": 4.67260019657093e-05,
      "loss": 2.8564,
      "step": 6000
    },
    {
      "epoch": 0.6563284918641477,
      "grad_norm": 0.19558389484882355,
      "learning_rate": 4.672054166211642e-05,
      "loss": 2.898,
      "step": 6010
    },
    {
      "epoch": 0.6574205525827236,
      "grad_norm": 0.16866235435009003,
      "learning_rate": 4.671508135852354e-05,
      "loss": 2.8988,
      "step": 6020
    },
    {
      "epoch": 0.6585126133012995,
      "grad_norm": 0.3236064314842224,
      "learning_rate": 4.670962105493066e-05,
      "loss": 2.9296,
      "step": 6030
    },
    {
      "epoch": 0.6596046740198755,
      "grad_norm": 0.2278500348329544,
      "learning_rate": 4.670416075133778e-05,
      "loss": 2.9352,
      "step": 6040
    },
    {
      "epoch": 0.6606967347384515,
      "grad_norm": 0.31869298219680786,
      "learning_rate": 4.6698700447744896e-05,
      "loss": 2.8955,
      "step": 6050
    },
    {
      "epoch": 0.6617887954570274,
      "grad_norm": 0.18553900718688965,
      "learning_rate": 4.6693240144152016e-05,
      "loss": 2.9854,
      "step": 6060
    },
    {
      "epoch": 0.6628808561756033,
      "grad_norm": 0.1738298386335373,
      "learning_rate": 4.6687779840559135e-05,
      "loss": 2.8914,
      "step": 6070
    },
    {
      "epoch": 0.6639729168941794,
      "grad_norm": 0.4980132281780243,
      "learning_rate": 4.6682319536966255e-05,
      "loss": 2.8938,
      "step": 6080
    },
    {
      "epoch": 0.6650649776127553,
      "grad_norm": 0.4279356896877289,
      "learning_rate": 4.6676859233373374e-05,
      "loss": 2.8961,
      "step": 6090
    },
    {
      "epoch": 0.6661570383313312,
      "grad_norm": 0.1385810226202011,
      "learning_rate": 4.667139892978049e-05,
      "loss": 2.9275,
      "step": 6100
    },
    {
      "epoch": 0.6672490990499071,
      "grad_norm": 0.23129057884216309,
      "learning_rate": 4.666593862618762e-05,
      "loss": 2.9042,
      "step": 6110
    },
    {
      "epoch": 0.6683411597684831,
      "grad_norm": 0.13181227445602417,
      "learning_rate": 4.666047832259474e-05,
      "loss": 2.8981,
      "step": 6120
    },
    {
      "epoch": 0.6694332204870591,
      "grad_norm": 0.14928056299686432,
      "learning_rate": 4.665501801900186e-05,
      "loss": 2.8693,
      "step": 6130
    },
    {
      "epoch": 0.670525281205635,
      "grad_norm": 0.4045650362968445,
      "learning_rate": 4.6649557715408985e-05,
      "loss": 2.8648,
      "step": 6140
    },
    {
      "epoch": 0.671617341924211,
      "grad_norm": 0.13700799643993378,
      "learning_rate": 4.6644097411816104e-05,
      "loss": 2.8591,
      "step": 6150
    },
    {
      "epoch": 0.6727094026427869,
      "grad_norm": 0.3154970407485962,
      "learning_rate": 4.663863710822322e-05,
      "loss": 2.934,
      "step": 6160
    },
    {
      "epoch": 0.6738014633613629,
      "grad_norm": 0.30806881189346313,
      "learning_rate": 4.663317680463034e-05,
      "loss": 2.8939,
      "step": 6170
    },
    {
      "epoch": 0.6748935240799389,
      "grad_norm": 0.3379645049571991,
      "learning_rate": 4.662771650103746e-05,
      "loss": 2.8944,
      "step": 6180
    },
    {
      "epoch": 0.6759855847985148,
      "grad_norm": 0.15722626447677612,
      "learning_rate": 4.662225619744458e-05,
      "loss": 2.8533,
      "step": 6190
    },
    {
      "epoch": 0.6770776455170907,
      "grad_norm": 0.9667187333106995,
      "learning_rate": 4.66167958938517e-05,
      "loss": 2.8812,
      "step": 6200
    },
    {
      "epoch": 0.6781697062356667,
      "grad_norm": 0.38452762365341187,
      "learning_rate": 4.661133559025882e-05,
      "loss": 2.879,
      "step": 6210
    },
    {
      "epoch": 0.6792617669542427,
      "grad_norm": 0.1520158052444458,
      "learning_rate": 4.660587528666594e-05,
      "loss": 2.8636,
      "step": 6220
    },
    {
      "epoch": 0.6803538276728186,
      "grad_norm": 0.5219751596450806,
      "learning_rate": 4.660041498307306e-05,
      "loss": 2.9551,
      "step": 6230
    },
    {
      "epoch": 0.6814458883913945,
      "grad_norm": 0.26093822717666626,
      "learning_rate": 4.659495467948018e-05,
      "loss": 2.9274,
      "step": 6240
    },
    {
      "epoch": 0.6825379491099706,
      "grad_norm": 0.6641887426376343,
      "learning_rate": 4.65894943758873e-05,
      "loss": 2.9152,
      "step": 6250
    },
    {
      "epoch": 0.6836300098285465,
      "grad_norm": 0.21364964544773102,
      "learning_rate": 4.658403407229442e-05,
      "loss": 2.862,
      "step": 6260
    },
    {
      "epoch": 0.6847220705471224,
      "grad_norm": 0.37987789511680603,
      "learning_rate": 4.6578573768701544e-05,
      "loss": 2.8736,
      "step": 6270
    },
    {
      "epoch": 0.6858141312656983,
      "grad_norm": 0.16521143913269043,
      "learning_rate": 4.657311346510866e-05,
      "loss": 2.883,
      "step": 6280
    },
    {
      "epoch": 0.6869061919842743,
      "grad_norm": 0.1379879266023636,
      "learning_rate": 4.656765316151578e-05,
      "loss": 2.8911,
      "step": 6290
    },
    {
      "epoch": 0.6879982527028503,
      "grad_norm": 0.20142190158367157,
      "learning_rate": 4.65621928579229e-05,
      "loss": 2.877,
      "step": 6300
    },
    {
      "epoch": 0.6890903134214262,
      "grad_norm": 0.22836381196975708,
      "learning_rate": 4.655673255433002e-05,
      "loss": 2.9341,
      "step": 6310
    },
    {
      "epoch": 0.6901823741400022,
      "grad_norm": 0.13802367448806763,
      "learning_rate": 4.655127225073714e-05,
      "loss": 2.8677,
      "step": 6320
    },
    {
      "epoch": 0.6912744348585781,
      "grad_norm": 0.11647198349237442,
      "learning_rate": 4.654581194714426e-05,
      "loss": 2.8512,
      "step": 6330
    },
    {
      "epoch": 0.6923664955771541,
      "grad_norm": 0.10565950721502304,
      "learning_rate": 4.6540351643551386e-05,
      "loss": 2.8872,
      "step": 6340
    },
    {
      "epoch": 0.69345855629573,
      "grad_norm": 0.17397789657115936,
      "learning_rate": 4.6534891339958506e-05,
      "loss": 2.9094,
      "step": 6350
    },
    {
      "epoch": 0.694550617014306,
      "grad_norm": 0.15677613019943237,
      "learning_rate": 4.6529431036365625e-05,
      "loss": 2.8831,
      "step": 6360
    },
    {
      "epoch": 0.6956426777328819,
      "grad_norm": 0.5243321657180786,
      "learning_rate": 4.6523970732772744e-05,
      "loss": 2.8895,
      "step": 6370
    },
    {
      "epoch": 0.696734738451458,
      "grad_norm": 0.10832773149013519,
      "learning_rate": 4.6518510429179864e-05,
      "loss": 2.9038,
      "step": 6380
    },
    {
      "epoch": 0.6978267991700339,
      "grad_norm": 0.18537122011184692,
      "learning_rate": 4.651305012558698e-05,
      "loss": 2.8697,
      "step": 6390
    },
    {
      "epoch": 0.6989188598886098,
      "grad_norm": 1.2399793863296509,
      "learning_rate": 4.650758982199411e-05,
      "loss": 2.9197,
      "step": 6400
    },
    {
      "epoch": 0.7000109206071857,
      "grad_norm": 0.12942790985107422,
      "learning_rate": 4.650212951840123e-05,
      "loss": 2.8711,
      "step": 6410
    },
    {
      "epoch": 0.7011029813257618,
      "grad_norm": 0.4253769814968109,
      "learning_rate": 4.649666921480835e-05,
      "loss": 2.9112,
      "step": 6420
    },
    {
      "epoch": 0.7021950420443377,
      "grad_norm": 0.6233740448951721,
      "learning_rate": 4.649120891121547e-05,
      "loss": 2.8876,
      "step": 6430
    },
    {
      "epoch": 0.7032871027629136,
      "grad_norm": 0.1005149632692337,
      "learning_rate": 4.648574860762259e-05,
      "loss": 2.8724,
      "step": 6440
    },
    {
      "epoch": 0.7043791634814895,
      "grad_norm": 0.30387362837791443,
      "learning_rate": 4.6480288304029707e-05,
      "loss": 2.9067,
      "step": 6450
    },
    {
      "epoch": 0.7054712242000655,
      "grad_norm": 0.6021709442138672,
      "learning_rate": 4.6474828000436826e-05,
      "loss": 2.91,
      "step": 6460
    },
    {
      "epoch": 0.7065632849186415,
      "grad_norm": 0.3280709385871887,
      "learning_rate": 4.6469367696843945e-05,
      "loss": 2.8595,
      "step": 6470
    },
    {
      "epoch": 0.7076553456372174,
      "grad_norm": 0.1594933718442917,
      "learning_rate": 4.6463907393251065e-05,
      "loss": 2.854,
      "step": 6480
    },
    {
      "epoch": 0.7087474063557934,
      "grad_norm": 0.208318829536438,
      "learning_rate": 4.6458447089658184e-05,
      "loss": 2.8601,
      "step": 6490
    },
    {
      "epoch": 0.7098394670743693,
      "grad_norm": 0.19933457672595978,
      "learning_rate": 4.6452986786065304e-05,
      "loss": 2.8779,
      "step": 6500
    },
    {
      "epoch": 0.7109315277929453,
      "grad_norm": 0.21625861525535583,
      "learning_rate": 4.644752648247242e-05,
      "loss": 2.8942,
      "step": 6510
    },
    {
      "epoch": 0.7120235885115213,
      "grad_norm": 0.4627048075199127,
      "learning_rate": 4.644206617887954e-05,
      "loss": 2.8643,
      "step": 6520
    },
    {
      "epoch": 0.7131156492300972,
      "grad_norm": 0.14500191807746887,
      "learning_rate": 4.643660587528667e-05,
      "loss": 2.9337,
      "step": 6530
    },
    {
      "epoch": 0.7142077099486731,
      "grad_norm": 0.46910199522972107,
      "learning_rate": 4.643114557169379e-05,
      "loss": 2.8849,
      "step": 6540
    },
    {
      "epoch": 0.7152997706672491,
      "grad_norm": 0.11625290662050247,
      "learning_rate": 4.642568526810091e-05,
      "loss": 2.8925,
      "step": 6550
    },
    {
      "epoch": 0.7163918313858251,
      "grad_norm": 0.4329821467399597,
      "learning_rate": 4.642022496450803e-05,
      "loss": 2.9233,
      "step": 6560
    },
    {
      "epoch": 0.717483892104401,
      "grad_norm": 1.2498970031738281,
      "learning_rate": 4.641476466091515e-05,
      "loss": 2.894,
      "step": 6570
    },
    {
      "epoch": 0.7185759528229769,
      "grad_norm": 0.21500760316848755,
      "learning_rate": 4.640930435732227e-05,
      "loss": 2.8636,
      "step": 6580
    },
    {
      "epoch": 0.719668013541553,
      "grad_norm": 0.21754178404808044,
      "learning_rate": 4.640384405372939e-05,
      "loss": 2.8875,
      "step": 6590
    },
    {
      "epoch": 0.7207600742601289,
      "grad_norm": 0.39490991830825806,
      "learning_rate": 4.639838375013651e-05,
      "loss": 2.8925,
      "step": 6600
    },
    {
      "epoch": 0.7218521349787048,
      "grad_norm": 0.19111275672912598,
      "learning_rate": 4.639292344654363e-05,
      "loss": 2.8777,
      "step": 6610
    },
    {
      "epoch": 0.7229441956972807,
      "grad_norm": 0.5444620251655579,
      "learning_rate": 4.638746314295075e-05,
      "loss": 2.9429,
      "step": 6620
    },
    {
      "epoch": 0.7240362564158567,
      "grad_norm": 0.2247619777917862,
      "learning_rate": 4.638200283935787e-05,
      "loss": 2.9191,
      "step": 6630
    },
    {
      "epoch": 0.7251283171344327,
      "grad_norm": 0.1402537226676941,
      "learning_rate": 4.637654253576499e-05,
      "loss": 2.8929,
      "step": 6640
    },
    {
      "epoch": 0.7262203778530086,
      "grad_norm": 0.24699251353740692,
      "learning_rate": 4.637108223217211e-05,
      "loss": 2.9378,
      "step": 6650
    },
    {
      "epoch": 0.7273124385715846,
      "grad_norm": 0.15655526518821716,
      "learning_rate": 4.6365621928579234e-05,
      "loss": 2.8801,
      "step": 6660
    },
    {
      "epoch": 0.7284044992901605,
      "grad_norm": 0.16096115112304688,
      "learning_rate": 4.6360161624986354e-05,
      "loss": 2.8654,
      "step": 6670
    },
    {
      "epoch": 0.7294965600087365,
      "grad_norm": 0.09007943421602249,
      "learning_rate": 4.635470132139347e-05,
      "loss": 2.8878,
      "step": 6680
    },
    {
      "epoch": 0.7305886207273125,
      "grad_norm": 0.181983083486557,
      "learning_rate": 4.634924101780059e-05,
      "loss": 2.8663,
      "step": 6690
    },
    {
      "epoch": 0.7316806814458884,
      "grad_norm": 0.18521419167518616,
      "learning_rate": 4.634378071420771e-05,
      "loss": 2.8868,
      "step": 6700
    },
    {
      "epoch": 0.7327727421644643,
      "grad_norm": 0.24982239305973053,
      "learning_rate": 4.633832041061483e-05,
      "loss": 2.8651,
      "step": 6710
    },
    {
      "epoch": 0.7338648028830403,
      "grad_norm": 0.2312520146369934,
      "learning_rate": 4.633286010702195e-05,
      "loss": 2.8672,
      "step": 6720
    },
    {
      "epoch": 0.7349568636016163,
      "grad_norm": 0.14525747299194336,
      "learning_rate": 4.632739980342907e-05,
      "loss": 2.8958,
      "step": 6730
    },
    {
      "epoch": 0.7360489243201922,
      "grad_norm": 0.30630019307136536,
      "learning_rate": 4.632193949983619e-05,
      "loss": 2.8942,
      "step": 6740
    },
    {
      "epoch": 0.7371409850387681,
      "grad_norm": 0.09637890756130219,
      "learning_rate": 4.631647919624331e-05,
      "loss": 2.8856,
      "step": 6750
    },
    {
      "epoch": 0.738233045757344,
      "grad_norm": 0.14342127740383148,
      "learning_rate": 4.631101889265043e-05,
      "loss": 2.8821,
      "step": 6760
    },
    {
      "epoch": 0.7393251064759201,
      "grad_norm": 0.14155101776123047,
      "learning_rate": 4.6305558589057555e-05,
      "loss": 2.8639,
      "step": 6770
    },
    {
      "epoch": 0.740417167194496,
      "grad_norm": 0.20442655682563782,
      "learning_rate": 4.6300098285464674e-05,
      "loss": 2.879,
      "step": 6780
    },
    {
      "epoch": 0.7415092279130719,
      "grad_norm": 0.710991382598877,
      "learning_rate": 4.6294637981871794e-05,
      "loss": 2.868,
      "step": 6790
    },
    {
      "epoch": 0.7426012886316479,
      "grad_norm": 0.27936795353889465,
      "learning_rate": 4.628917767827892e-05,
      "loss": 2.9318,
      "step": 6800
    },
    {
      "epoch": 0.7436933493502239,
      "grad_norm": 0.19788023829460144,
      "learning_rate": 4.628371737468604e-05,
      "loss": 2.8662,
      "step": 6810
    },
    {
      "epoch": 0.7447854100687998,
      "grad_norm": 0.34612441062927246,
      "learning_rate": 4.627825707109316e-05,
      "loss": 2.8717,
      "step": 6820
    },
    {
      "epoch": 0.7458774707873758,
      "grad_norm": 0.06766906380653381,
      "learning_rate": 4.627279676750028e-05,
      "loss": 2.8464,
      "step": 6830
    },
    {
      "epoch": 0.7469695315059517,
      "grad_norm": 0.7711966633796692,
      "learning_rate": 4.62673364639074e-05,
      "loss": 2.8713,
      "step": 6840
    },
    {
      "epoch": 0.7480615922245277,
      "grad_norm": 0.15452823042869568,
      "learning_rate": 4.626187616031452e-05,
      "loss": 2.8703,
      "step": 6850
    },
    {
      "epoch": 0.7491536529431037,
      "grad_norm": 0.15900157392024994,
      "learning_rate": 4.6256415856721636e-05,
      "loss": 2.8733,
      "step": 6860
    },
    {
      "epoch": 0.7502457136616796,
      "grad_norm": 0.16714009642601013,
      "learning_rate": 4.6250955553128756e-05,
      "loss": 2.8843,
      "step": 6870
    },
    {
      "epoch": 0.7513377743802555,
      "grad_norm": 0.19600361585617065,
      "learning_rate": 4.6245495249535875e-05,
      "loss": 2.839,
      "step": 6880
    },
    {
      "epoch": 0.7524298350988315,
      "grad_norm": 0.3280240297317505,
      "learning_rate": 4.6240034945942994e-05,
      "loss": 2.865,
      "step": 6890
    },
    {
      "epoch": 0.7535218958174075,
      "grad_norm": 0.09240683168172836,
      "learning_rate": 4.6234574642350114e-05,
      "loss": 2.8936,
      "step": 6900
    },
    {
      "epoch": 0.7546139565359834,
      "grad_norm": 0.14752203226089478,
      "learning_rate": 4.622911433875723e-05,
      "loss": 2.8928,
      "step": 6910
    },
    {
      "epoch": 0.7557060172545593,
      "grad_norm": 0.2000894844532013,
      "learning_rate": 4.622365403516436e-05,
      "loss": 2.9027,
      "step": 6920
    },
    {
      "epoch": 0.7567980779731353,
      "grad_norm": 0.2424294650554657,
      "learning_rate": 4.621819373157148e-05,
      "loss": 2.9014,
      "step": 6930
    },
    {
      "epoch": 0.7578901386917113,
      "grad_norm": 1.082275152206421,
      "learning_rate": 4.62127334279786e-05,
      "loss": 2.9092,
      "step": 6940
    },
    {
      "epoch": 0.7589821994102872,
      "grad_norm": 0.138645738363266,
      "learning_rate": 4.620727312438572e-05,
      "loss": 2.8762,
      "step": 6950
    },
    {
      "epoch": 0.7600742601288631,
      "grad_norm": 0.10954362154006958,
      "learning_rate": 4.620181282079284e-05,
      "loss": 2.8758,
      "step": 6960
    },
    {
      "epoch": 0.7611663208474391,
      "grad_norm": 1.2629231214523315,
      "learning_rate": 4.6196352517199957e-05,
      "loss": 2.9614,
      "step": 6970
    },
    {
      "epoch": 0.7622583815660151,
      "grad_norm": 0.21519146859645844,
      "learning_rate": 4.6190892213607076e-05,
      "loss": 2.877,
      "step": 6980
    },
    {
      "epoch": 0.763350442284591,
      "grad_norm": 0.42766210436820984,
      "learning_rate": 4.6185431910014195e-05,
      "loss": 2.8681,
      "step": 6990
    },
    {
      "epoch": 0.764442503003167,
      "grad_norm": 0.45680347084999084,
      "learning_rate": 4.617997160642132e-05,
      "loss": 2.8692,
      "step": 7000
    },
    {
      "epoch": 0.7655345637217429,
      "grad_norm": 0.17411375045776367,
      "learning_rate": 4.617451130282844e-05,
      "loss": 2.8722,
      "step": 7010
    },
    {
      "epoch": 0.7666266244403189,
      "grad_norm": 0.20758482813835144,
      "learning_rate": 4.616905099923556e-05,
      "loss": 2.8497,
      "step": 7020
    },
    {
      "epoch": 0.7677186851588949,
      "grad_norm": 0.17189805209636688,
      "learning_rate": 4.616359069564268e-05,
      "loss": 2.8778,
      "step": 7030
    },
    {
      "epoch": 0.7688107458774708,
      "grad_norm": 0.32606634497642517,
      "learning_rate": 4.61581303920498e-05,
      "loss": 2.9572,
      "step": 7040
    },
    {
      "epoch": 0.7699028065960467,
      "grad_norm": 0.14716099202632904,
      "learning_rate": 4.6152670088456925e-05,
      "loss": 2.8659,
      "step": 7050
    },
    {
      "epoch": 0.7709948673146227,
      "grad_norm": 1.209848165512085,
      "learning_rate": 4.6147209784864045e-05,
      "loss": 2.968,
      "step": 7060
    },
    {
      "epoch": 0.7720869280331987,
      "grad_norm": 0.24369533360004425,
      "learning_rate": 4.6141749481271164e-05,
      "loss": 2.8692,
      "step": 7070
    },
    {
      "epoch": 0.7731789887517746,
      "grad_norm": 1.0236167907714844,
      "learning_rate": 4.6136289177678284e-05,
      "loss": 2.9746,
      "step": 7080
    },
    {
      "epoch": 0.7742710494703505,
      "grad_norm": 0.1772332340478897,
      "learning_rate": 4.61308288740854e-05,
      "loss": 2.9549,
      "step": 7090
    },
    {
      "epoch": 0.7753631101889265,
      "grad_norm": 0.15934525430202484,
      "learning_rate": 4.612536857049252e-05,
      "loss": 2.8554,
      "step": 7100
    },
    {
      "epoch": 0.7764551709075025,
      "grad_norm": 0.15742959082126617,
      "learning_rate": 4.611990826689964e-05,
      "loss": 2.8912,
      "step": 7110
    },
    {
      "epoch": 0.7775472316260784,
      "grad_norm": 0.12438376992940903,
      "learning_rate": 4.611444796330676e-05,
      "loss": 2.9069,
      "step": 7120
    },
    {
      "epoch": 0.7786392923446543,
      "grad_norm": 0.2161431610584259,
      "learning_rate": 4.610898765971388e-05,
      "loss": 2.8848,
      "step": 7130
    },
    {
      "epoch": 0.7797313530632303,
      "grad_norm": 0.17086251080036163,
      "learning_rate": 4.6103527356121e-05,
      "loss": 2.8549,
      "step": 7140
    },
    {
      "epoch": 0.7808234137818063,
      "grad_norm": 0.16011525690555573,
      "learning_rate": 4.609806705252812e-05,
      "loss": 2.8648,
      "step": 7150
    },
    {
      "epoch": 0.7819154745003822,
      "grad_norm": 0.28127217292785645,
      "learning_rate": 4.609260674893524e-05,
      "loss": 2.9218,
      "step": 7160
    },
    {
      "epoch": 0.7830075352189582,
      "grad_norm": 0.1526174545288086,
      "learning_rate": 4.608714644534236e-05,
      "loss": 2.8599,
      "step": 7170
    },
    {
      "epoch": 0.7840995959375341,
      "grad_norm": 0.38008737564086914,
      "learning_rate": 4.6081686141749484e-05,
      "loss": 2.8764,
      "step": 7180
    },
    {
      "epoch": 0.7851916566561101,
      "grad_norm": 0.133464515209198,
      "learning_rate": 4.6076225838156604e-05,
      "loss": 2.8547,
      "step": 7190
    },
    {
      "epoch": 0.786283717374686,
      "grad_norm": 0.17266450822353363,
      "learning_rate": 4.607076553456372e-05,
      "loss": 2.877,
      "step": 7200
    },
    {
      "epoch": 0.787375778093262,
      "grad_norm": 0.14427684247493744,
      "learning_rate": 4.606530523097084e-05,
      "loss": 2.8737,
      "step": 7210
    },
    {
      "epoch": 0.7884678388118379,
      "grad_norm": 0.3505189120769501,
      "learning_rate": 4.605984492737796e-05,
      "loss": 2.8562,
      "step": 7220
    },
    {
      "epoch": 0.7895598995304139,
      "grad_norm": 0.4685046970844269,
      "learning_rate": 4.605438462378509e-05,
      "loss": 2.8706,
      "step": 7230
    },
    {
      "epoch": 0.7906519602489899,
      "grad_norm": 0.3461189568042755,
      "learning_rate": 4.604892432019221e-05,
      "loss": 2.8706,
      "step": 7240
    },
    {
      "epoch": 0.7917440209675658,
      "grad_norm": 0.48899704217910767,
      "learning_rate": 4.604346401659933e-05,
      "loss": 2.9508,
      "step": 7250
    },
    {
      "epoch": 0.7928360816861417,
      "grad_norm": 0.19800609350204468,
      "learning_rate": 4.6038003713006447e-05,
      "loss": 2.8743,
      "step": 7260
    },
    {
      "epoch": 0.7939281424047177,
      "grad_norm": 0.2714170217514038,
      "learning_rate": 4.6032543409413566e-05,
      "loss": 2.9309,
      "step": 7270
    },
    {
      "epoch": 0.7950202031232937,
      "grad_norm": 0.24277061223983765,
      "learning_rate": 4.6027083105820685e-05,
      "loss": 2.9309,
      "step": 7280
    },
    {
      "epoch": 0.7961122638418696,
      "grad_norm": 0.2380189746618271,
      "learning_rate": 4.6021622802227805e-05,
      "loss": 2.8988,
      "step": 7290
    },
    {
      "epoch": 0.7972043245604455,
      "grad_norm": 0.1823546141386032,
      "learning_rate": 4.6016162498634924e-05,
      "loss": 2.8887,
      "step": 7300
    },
    {
      "epoch": 0.7982963852790215,
      "grad_norm": 0.46647340059280396,
      "learning_rate": 4.601070219504205e-05,
      "loss": 2.9362,
      "step": 7310
    },
    {
      "epoch": 0.7993884459975975,
      "grad_norm": 0.32990700006484985,
      "learning_rate": 4.600524189144917e-05,
      "loss": 2.8681,
      "step": 7320
    },
    {
      "epoch": 0.8004805067161734,
      "grad_norm": 0.20850194990634918,
      "learning_rate": 4.599978158785629e-05,
      "loss": 2.8723,
      "step": 7330
    },
    {
      "epoch": 0.8015725674347494,
      "grad_norm": 0.3037046194076538,
      "learning_rate": 4.599432128426341e-05,
      "loss": 2.847,
      "step": 7340
    },
    {
      "epoch": 0.8026646281533253,
      "grad_norm": 0.3145531117916107,
      "learning_rate": 4.598886098067053e-05,
      "loss": 2.8521,
      "step": 7350
    },
    {
      "epoch": 0.8037566888719013,
      "grad_norm": 0.21571387350559235,
      "learning_rate": 4.598340067707765e-05,
      "loss": 2.8744,
      "step": 7360
    },
    {
      "epoch": 0.8048487495904773,
      "grad_norm": 0.1322731077671051,
      "learning_rate": 4.597794037348477e-05,
      "loss": 2.9097,
      "step": 7370
    },
    {
      "epoch": 0.8059408103090532,
      "grad_norm": 0.34294092655181885,
      "learning_rate": 4.5972480069891886e-05,
      "loss": 2.8445,
      "step": 7380
    },
    {
      "epoch": 0.8070328710276291,
      "grad_norm": 0.16557157039642334,
      "learning_rate": 4.5967019766299006e-05,
      "loss": 2.8837,
      "step": 7390
    },
    {
      "epoch": 0.8081249317462051,
      "grad_norm": 0.16377118229866028,
      "learning_rate": 4.5961559462706125e-05,
      "loss": 2.9379,
      "step": 7400
    },
    {
      "epoch": 0.8092169924647811,
      "grad_norm": 0.20708033442497253,
      "learning_rate": 4.5956099159113244e-05,
      "loss": 3.0142,
      "step": 7410
    },
    {
      "epoch": 0.810309053183357,
      "grad_norm": 0.19384951889514923,
      "learning_rate": 4.5950638855520364e-05,
      "loss": 2.8839,
      "step": 7420
    },
    {
      "epoch": 0.8114011139019329,
      "grad_norm": 0.4856123626232147,
      "learning_rate": 4.594517855192749e-05,
      "loss": 2.8964,
      "step": 7430
    },
    {
      "epoch": 0.8124931746205089,
      "grad_norm": 0.18137407302856445,
      "learning_rate": 4.593971824833461e-05,
      "loss": 2.8807,
      "step": 7440
    },
    {
      "epoch": 0.8135852353390849,
      "grad_norm": 0.18984274566173553,
      "learning_rate": 4.593425794474173e-05,
      "loss": 2.876,
      "step": 7450
    },
    {
      "epoch": 0.8146772960576608,
      "grad_norm": 0.7376301884651184,
      "learning_rate": 4.5928797641148855e-05,
      "loss": 2.8703,
      "step": 7460
    },
    {
      "epoch": 0.8157693567762367,
      "grad_norm": 0.1493677794933319,
      "learning_rate": 4.5923337337555974e-05,
      "loss": 2.8951,
      "step": 7470
    },
    {
      "epoch": 0.8168614174948127,
      "grad_norm": 0.2378581166267395,
      "learning_rate": 4.5917877033963094e-05,
      "loss": 2.8906,
      "step": 7480
    },
    {
      "epoch": 0.8179534782133887,
      "grad_norm": 0.35209453105926514,
      "learning_rate": 4.591241673037021e-05,
      "loss": 2.9352,
      "step": 7490
    },
    {
      "epoch": 0.8190455389319646,
      "grad_norm": 0.15438804030418396,
      "learning_rate": 4.590695642677733e-05,
      "loss": 2.8979,
      "step": 7500
    },
    {
      "epoch": 0.8201375996505406,
      "grad_norm": 0.6350382566452026,
      "learning_rate": 4.590149612318445e-05,
      "loss": 2.8826,
      "step": 7510
    },
    {
      "epoch": 0.8212296603691165,
      "grad_norm": 0.1166548952460289,
      "learning_rate": 4.589603581959157e-05,
      "loss": 2.8526,
      "step": 7520
    },
    {
      "epoch": 0.8223217210876925,
      "grad_norm": 0.2833818197250366,
      "learning_rate": 4.589057551599869e-05,
      "loss": 2.8771,
      "step": 7530
    },
    {
      "epoch": 0.8234137818062685,
      "grad_norm": 0.1675335019826889,
      "learning_rate": 4.588511521240581e-05,
      "loss": 2.8668,
      "step": 7540
    },
    {
      "epoch": 0.8245058425248444,
      "grad_norm": 0.11422461271286011,
      "learning_rate": 4.587965490881293e-05,
      "loss": 2.8928,
      "step": 7550
    },
    {
      "epoch": 0.8255979032434203,
      "grad_norm": 0.16590315103530884,
      "learning_rate": 4.587419460522005e-05,
      "loss": 2.8678,
      "step": 7560
    },
    {
      "epoch": 0.8266899639619962,
      "grad_norm": 0.1601308286190033,
      "learning_rate": 4.5868734301627175e-05,
      "loss": 2.8899,
      "step": 7570
    },
    {
      "epoch": 0.8277820246805723,
      "grad_norm": 0.3472598195075989,
      "learning_rate": 4.5863273998034295e-05,
      "loss": 2.9368,
      "step": 7580
    },
    {
      "epoch": 0.8288740853991482,
      "grad_norm": 0.251625657081604,
      "learning_rate": 4.5857813694441414e-05,
      "loss": 2.9279,
      "step": 7590
    },
    {
      "epoch": 0.8299661461177241,
      "grad_norm": 0.28509971499443054,
      "learning_rate": 4.5852353390848534e-05,
      "loss": 2.8617,
      "step": 7600
    },
    {
      "epoch": 0.8310582068363,
      "grad_norm": 0.5333727598190308,
      "learning_rate": 4.584689308725565e-05,
      "loss": 2.8983,
      "step": 7610
    },
    {
      "epoch": 0.8321502675548761,
      "grad_norm": 0.16759860515594482,
      "learning_rate": 4.584143278366277e-05,
      "loss": 2.8827,
      "step": 7620
    },
    {
      "epoch": 0.833242328273452,
      "grad_norm": 0.3151234984397888,
      "learning_rate": 4.583597248006989e-05,
      "loss": 2.8534,
      "step": 7630
    },
    {
      "epoch": 0.8343343889920279,
      "grad_norm": 0.46504122018814087,
      "learning_rate": 4.583051217647701e-05,
      "loss": 2.8808,
      "step": 7640
    },
    {
      "epoch": 0.8354264497106039,
      "grad_norm": 0.23500116169452667,
      "learning_rate": 4.582505187288413e-05,
      "loss": 2.8836,
      "step": 7650
    },
    {
      "epoch": 0.8365185104291799,
      "grad_norm": 0.19508488476276398,
      "learning_rate": 4.581959156929126e-05,
      "loss": 2.9245,
      "step": 7660
    },
    {
      "epoch": 0.8376105711477558,
      "grad_norm": 0.5107021927833557,
      "learning_rate": 4.5814131265698376e-05,
      "loss": 2.9026,
      "step": 7670
    },
    {
      "epoch": 0.8387026318663318,
      "grad_norm": 0.3555387854576111,
      "learning_rate": 4.5808670962105496e-05,
      "loss": 2.8973,
      "step": 7680
    },
    {
      "epoch": 0.8397946925849077,
      "grad_norm": 0.12751404941082,
      "learning_rate": 4.5803210658512615e-05,
      "loss": 2.8943,
      "step": 7690
    },
    {
      "epoch": 0.8408867533034837,
      "grad_norm": 0.1954534351825714,
      "learning_rate": 4.579775035491974e-05,
      "loss": 2.8461,
      "step": 7700
    },
    {
      "epoch": 0.8419788140220597,
      "grad_norm": 0.3509501516819,
      "learning_rate": 4.579229005132686e-05,
      "loss": 2.8644,
      "step": 7710
    },
    {
      "epoch": 0.8430708747406356,
      "grad_norm": 0.6978354454040527,
      "learning_rate": 4.578682974773398e-05,
      "loss": 2.918,
      "step": 7720
    },
    {
      "epoch": 0.8441629354592115,
      "grad_norm": 0.19473478198051453,
      "learning_rate": 4.57813694441411e-05,
      "loss": 2.8597,
      "step": 7730
    },
    {
      "epoch": 0.8452549961777874,
      "grad_norm": 0.16805925965309143,
      "learning_rate": 4.577590914054822e-05,
      "loss": 2.8836,
      "step": 7740
    },
    {
      "epoch": 0.8463470568963635,
      "grad_norm": 0.19933275878429413,
      "learning_rate": 4.577044883695534e-05,
      "loss": 2.86,
      "step": 7750
    },
    {
      "epoch": 0.8474391176149394,
      "grad_norm": 0.6145083904266357,
      "learning_rate": 4.576498853336246e-05,
      "loss": 2.9237,
      "step": 7760
    },
    {
      "epoch": 0.8485311783335153,
      "grad_norm": 0.24809551239013672,
      "learning_rate": 4.575952822976958e-05,
      "loss": 2.8655,
      "step": 7770
    },
    {
      "epoch": 0.8496232390520913,
      "grad_norm": 0.126209557056427,
      "learning_rate": 4.5754067926176696e-05,
      "loss": 2.8711,
      "step": 7780
    },
    {
      "epoch": 0.8507152997706673,
      "grad_norm": 0.24738964438438416,
      "learning_rate": 4.5748607622583816e-05,
      "loss": 2.8472,
      "step": 7790
    },
    {
      "epoch": 0.8518073604892432,
      "grad_norm": 0.32890668511390686,
      "learning_rate": 4.5743147318990935e-05,
      "loss": 2.8703,
      "step": 7800
    },
    {
      "epoch": 0.8528994212078191,
      "grad_norm": 0.11696046590805054,
      "learning_rate": 4.5737687015398055e-05,
      "loss": 2.8733,
      "step": 7810
    },
    {
      "epoch": 0.8539914819263951,
      "grad_norm": 0.37981030344963074,
      "learning_rate": 4.5732226711805174e-05,
      "loss": 2.8653,
      "step": 7820
    },
    {
      "epoch": 0.8550835426449711,
      "grad_norm": 0.15625911951065063,
      "learning_rate": 4.57267664082123e-05,
      "loss": 2.8585,
      "step": 7830
    },
    {
      "epoch": 0.856175603363547,
      "grad_norm": 0.1918497234582901,
      "learning_rate": 4.572130610461942e-05,
      "loss": 2.8769,
      "step": 7840
    },
    {
      "epoch": 0.857267664082123,
      "grad_norm": 0.1430458277463913,
      "learning_rate": 4.571584580102654e-05,
      "loss": 3.0254,
      "step": 7850
    },
    {
      "epoch": 0.8583597248006989,
      "grad_norm": 0.4209378957748413,
      "learning_rate": 4.571038549743366e-05,
      "loss": 2.8911,
      "step": 7860
    },
    {
      "epoch": 0.8594517855192749,
      "grad_norm": 0.17282477021217346,
      "learning_rate": 4.570492519384078e-05,
      "loss": 2.9212,
      "step": 7870
    },
    {
      "epoch": 0.8605438462378509,
      "grad_norm": 0.30358263850212097,
      "learning_rate": 4.56994648902479e-05,
      "loss": 2.871,
      "step": 7880
    },
    {
      "epoch": 0.8616359069564268,
      "grad_norm": 0.5386655926704407,
      "learning_rate": 4.5694004586655024e-05,
      "loss": 2.9032,
      "step": 7890
    },
    {
      "epoch": 0.8627279676750027,
      "grad_norm": 0.14591453969478607,
      "learning_rate": 4.568854428306214e-05,
      "loss": 2.9049,
      "step": 7900
    },
    {
      "epoch": 0.8638200283935786,
      "grad_norm": 0.18746253848075867,
      "learning_rate": 4.568308397946926e-05,
      "loss": 2.8714,
      "step": 7910
    },
    {
      "epoch": 0.8649120891121547,
      "grad_norm": 0.30300402641296387,
      "learning_rate": 4.567762367587638e-05,
      "loss": 2.8907,
      "step": 7920
    },
    {
      "epoch": 0.8660041498307306,
      "grad_norm": 0.2859991490840912,
      "learning_rate": 4.56721633722835e-05,
      "loss": 2.8726,
      "step": 7930
    },
    {
      "epoch": 0.8670962105493065,
      "grad_norm": 0.626563310623169,
      "learning_rate": 4.566670306869062e-05,
      "loss": 2.8568,
      "step": 7940
    },
    {
      "epoch": 0.8681882712678824,
      "grad_norm": 0.138299360871315,
      "learning_rate": 4.566124276509774e-05,
      "loss": 2.846,
      "step": 7950
    },
    {
      "epoch": 0.8692803319864585,
      "grad_norm": 0.944065511226654,
      "learning_rate": 4.5655782461504866e-05,
      "loss": 2.8856,
      "step": 7960
    },
    {
      "epoch": 0.8703723927050344,
      "grad_norm": 0.4918147623538971,
      "learning_rate": 4.5650322157911986e-05,
      "loss": 2.884,
      "step": 7970
    },
    {
      "epoch": 0.8714644534236103,
      "grad_norm": 0.17488981783390045,
      "learning_rate": 4.5644861854319105e-05,
      "loss": 2.8796,
      "step": 7980
    },
    {
      "epoch": 0.8725565141421863,
      "grad_norm": 0.2022225260734558,
      "learning_rate": 4.5639401550726224e-05,
      "loss": 2.8594,
      "step": 7990
    },
    {
      "epoch": 0.8736485748607623,
      "grad_norm": 0.1510566771030426,
      "learning_rate": 4.5633941247133344e-05,
      "loss": 2.8816,
      "step": 8000
    },
    {
      "epoch": 0.8747406355793382,
      "grad_norm": 0.16996648907661438,
      "learning_rate": 4.562848094354046e-05,
      "loss": 2.883,
      "step": 8010
    },
    {
      "epoch": 0.8758326962979142,
      "grad_norm": 0.44673943519592285,
      "learning_rate": 4.562302063994758e-05,
      "loss": 2.8543,
      "step": 8020
    },
    {
      "epoch": 0.8769247570164901,
      "grad_norm": 0.30147382616996765,
      "learning_rate": 4.56175603363547e-05,
      "loss": 2.8732,
      "step": 8030
    },
    {
      "epoch": 0.8780168177350661,
      "grad_norm": 1.0102052688598633,
      "learning_rate": 4.561210003276182e-05,
      "loss": 2.9148,
      "step": 8040
    },
    {
      "epoch": 0.879108878453642,
      "grad_norm": 0.2796218991279602,
      "learning_rate": 4.560663972916894e-05,
      "loss": 2.8644,
      "step": 8050
    },
    {
      "epoch": 0.880200939172218,
      "grad_norm": 0.08986362814903259,
      "learning_rate": 4.560117942557606e-05,
      "loss": 2.8474,
      "step": 8060
    },
    {
      "epoch": 0.8812929998907939,
      "grad_norm": 0.20925253629684448,
      "learning_rate": 4.559571912198318e-05,
      "loss": 2.8804,
      "step": 8070
    },
    {
      "epoch": 0.8823850606093698,
      "grad_norm": 0.5023726224899292,
      "learning_rate": 4.55902588183903e-05,
      "loss": 2.9808,
      "step": 8080
    },
    {
      "epoch": 0.8834771213279459,
      "grad_norm": 0.5642533898353577,
      "learning_rate": 4.5584798514797425e-05,
      "loss": 2.8626,
      "step": 8090
    },
    {
      "epoch": 0.8845691820465218,
      "grad_norm": 0.15426917374134064,
      "learning_rate": 4.5579338211204545e-05,
      "loss": 2.8662,
      "step": 8100
    },
    {
      "epoch": 0.8856612427650977,
      "grad_norm": 0.2522447109222412,
      "learning_rate": 4.5573877907611664e-05,
      "loss": 2.9011,
      "step": 8110
    },
    {
      "epoch": 0.8867533034836736,
      "grad_norm": 0.103087417781353,
      "learning_rate": 4.556841760401879e-05,
      "loss": 2.8915,
      "step": 8120
    },
    {
      "epoch": 0.8878453642022497,
      "grad_norm": 0.20840367674827576,
      "learning_rate": 4.556295730042591e-05,
      "loss": 2.8949,
      "step": 8130
    },
    {
      "epoch": 0.8889374249208256,
      "grad_norm": 0.1720527708530426,
      "learning_rate": 4.555749699683303e-05,
      "loss": 2.8638,
      "step": 8140
    },
    {
      "epoch": 0.8900294856394015,
      "grad_norm": 0.1655399352312088,
      "learning_rate": 4.555203669324015e-05,
      "loss": 2.8267,
      "step": 8150
    },
    {
      "epoch": 0.8911215463579775,
      "grad_norm": 0.1649537980556488,
      "learning_rate": 4.554657638964727e-05,
      "loss": 2.8603,
      "step": 8160
    },
    {
      "epoch": 0.8922136070765535,
      "grad_norm": 0.326786607503891,
      "learning_rate": 4.554111608605439e-05,
      "loss": 2.9313,
      "step": 8170
    },
    {
      "epoch": 0.8933056677951294,
      "grad_norm": 0.18059022724628448,
      "learning_rate": 4.553565578246151e-05,
      "loss": 2.8709,
      "step": 8180
    },
    {
      "epoch": 0.8943977285137054,
      "grad_norm": 0.1803307980298996,
      "learning_rate": 4.5530195478868626e-05,
      "loss": 2.9213,
      "step": 8190
    },
    {
      "epoch": 0.8954897892322813,
      "grad_norm": 0.22844745218753815,
      "learning_rate": 4.5524735175275746e-05,
      "loss": 2.8881,
      "step": 8200
    },
    {
      "epoch": 0.8965818499508572,
      "grad_norm": 0.37925985455513,
      "learning_rate": 4.5519274871682865e-05,
      "loss": 2.9049,
      "step": 8210
    },
    {
      "epoch": 0.8976739106694333,
      "grad_norm": 0.21184834837913513,
      "learning_rate": 4.5513814568089984e-05,
      "loss": 2.8687,
      "step": 8220
    },
    {
      "epoch": 0.8987659713880092,
      "grad_norm": 0.1444709151983261,
      "learning_rate": 4.550835426449711e-05,
      "loss": 2.8496,
      "step": 8230
    },
    {
      "epoch": 0.8998580321065851,
      "grad_norm": 0.2430555820465088,
      "learning_rate": 4.550289396090423e-05,
      "loss": 2.9154,
      "step": 8240
    },
    {
      "epoch": 0.900950092825161,
      "grad_norm": 0.15721851587295532,
      "learning_rate": 4.549743365731135e-05,
      "loss": 2.8711,
      "step": 8250
    },
    {
      "epoch": 0.9020421535437371,
      "grad_norm": 0.2104209065437317,
      "learning_rate": 4.549197335371847e-05,
      "loss": 2.8751,
      "step": 8260
    },
    {
      "epoch": 0.903134214262313,
      "grad_norm": 0.9181521534919739,
      "learning_rate": 4.548651305012559e-05,
      "loss": 2.9298,
      "step": 8270
    },
    {
      "epoch": 0.9042262749808889,
      "grad_norm": 0.372384250164032,
      "learning_rate": 4.548105274653271e-05,
      "loss": 2.8709,
      "step": 8280
    },
    {
      "epoch": 0.9053183356994648,
      "grad_norm": 0.22595332562923431,
      "learning_rate": 4.547559244293983e-05,
      "loss": 2.8927,
      "step": 8290
    },
    {
      "epoch": 0.9064103964180409,
      "grad_norm": 0.33269432187080383,
      "learning_rate": 4.5470132139346946e-05,
      "loss": 2.8942,
      "step": 8300
    },
    {
      "epoch": 0.9075024571366168,
      "grad_norm": 0.18953263759613037,
      "learning_rate": 4.5464671835754066e-05,
      "loss": 2.9179,
      "step": 8310
    },
    {
      "epoch": 0.9085945178551927,
      "grad_norm": 0.5770497918128967,
      "learning_rate": 4.545921153216119e-05,
      "loss": 2.888,
      "step": 8320
    },
    {
      "epoch": 0.9096865785737687,
      "grad_norm": 0.19846464693546295,
      "learning_rate": 4.545375122856831e-05,
      "loss": 2.8536,
      "step": 8330
    },
    {
      "epoch": 0.9107786392923447,
      "grad_norm": 0.18421979248523712,
      "learning_rate": 4.544829092497543e-05,
      "loss": 2.8725,
      "step": 8340
    },
    {
      "epoch": 0.9118707000109206,
      "grad_norm": 0.218410924077034,
      "learning_rate": 4.544283062138255e-05,
      "loss": 2.8506,
      "step": 8350
    },
    {
      "epoch": 0.9129627607294966,
      "grad_norm": 0.1910502165555954,
      "learning_rate": 4.5437370317789676e-05,
      "loss": 2.931,
      "step": 8360
    },
    {
      "epoch": 0.9140548214480725,
      "grad_norm": 0.17263203859329224,
      "learning_rate": 4.5431910014196796e-05,
      "loss": 2.8485,
      "step": 8370
    },
    {
      "epoch": 0.9151468821666484,
      "grad_norm": 0.12266682833433151,
      "learning_rate": 4.5426449710603915e-05,
      "loss": 2.8633,
      "step": 8380
    },
    {
      "epoch": 0.9162389428852244,
      "grad_norm": 0.10681583732366562,
      "learning_rate": 4.5420989407011035e-05,
      "loss": 2.8896,
      "step": 8390
    },
    {
      "epoch": 0.9173310036038004,
      "grad_norm": 0.26571229100227356,
      "learning_rate": 4.5415529103418154e-05,
      "loss": 2.903,
      "step": 8400
    },
    {
      "epoch": 0.9184230643223763,
      "grad_norm": 0.12816724181175232,
      "learning_rate": 4.5410068799825273e-05,
      "loss": 2.8904,
      "step": 8410
    },
    {
      "epoch": 0.9195151250409522,
      "grad_norm": 0.10776367783546448,
      "learning_rate": 4.540460849623239e-05,
      "loss": 2.8916,
      "step": 8420
    },
    {
      "epoch": 0.9206071857595283,
      "grad_norm": 0.18272706866264343,
      "learning_rate": 4.539914819263951e-05,
      "loss": 2.8421,
      "step": 8430
    },
    {
      "epoch": 0.9216992464781042,
      "grad_norm": 0.2687116861343384,
      "learning_rate": 4.539368788904663e-05,
      "loss": 2.9085,
      "step": 8440
    },
    {
      "epoch": 0.9227913071966801,
      "grad_norm": 0.1343931406736374,
      "learning_rate": 4.538822758545375e-05,
      "loss": 2.8579,
      "step": 8450
    },
    {
      "epoch": 0.923883367915256,
      "grad_norm": 0.3399820327758789,
      "learning_rate": 4.538276728186087e-05,
      "loss": 2.8716,
      "step": 8460
    },
    {
      "epoch": 0.9249754286338321,
      "grad_norm": 0.32974308729171753,
      "learning_rate": 4.537730697826799e-05,
      "loss": 2.8804,
      "step": 8470
    },
    {
      "epoch": 0.926067489352408,
      "grad_norm": 0.15149268507957458,
      "learning_rate": 4.537184667467511e-05,
      "loss": 2.8659,
      "step": 8480
    },
    {
      "epoch": 0.9271595500709839,
      "grad_norm": 0.40875452756881714,
      "learning_rate": 4.5366386371082236e-05,
      "loss": 2.8908,
      "step": 8490
    },
    {
      "epoch": 0.9282516107895599,
      "grad_norm": 0.2679518163204193,
      "learning_rate": 4.5360926067489355e-05,
      "loss": 2.935,
      "step": 8500
    },
    {
      "epoch": 0.9293436715081359,
      "grad_norm": 0.16383284330368042,
      "learning_rate": 4.5355465763896474e-05,
      "loss": 2.889,
      "step": 8510
    },
    {
      "epoch": 0.9304357322267118,
      "grad_norm": 0.1310671716928482,
      "learning_rate": 4.5350005460303594e-05,
      "loss": 2.8839,
      "step": 8520
    },
    {
      "epoch": 0.9315277929452878,
      "grad_norm": 0.10808803141117096,
      "learning_rate": 4.534454515671071e-05,
      "loss": 2.8488,
      "step": 8530
    },
    {
      "epoch": 0.9326198536638637,
      "grad_norm": 0.2967012822628021,
      "learning_rate": 4.533908485311783e-05,
      "loss": 2.8676,
      "step": 8540
    },
    {
      "epoch": 0.9337119143824396,
      "grad_norm": 0.2594453990459442,
      "learning_rate": 4.533362454952496e-05,
      "loss": 2.8633,
      "step": 8550
    },
    {
      "epoch": 0.9348039751010156,
      "grad_norm": 0.15988454222679138,
      "learning_rate": 4.532816424593208e-05,
      "loss": 2.923,
      "step": 8560
    },
    {
      "epoch": 0.9358960358195916,
      "grad_norm": 0.3217194676399231,
      "learning_rate": 4.53227039423392e-05,
      "loss": 2.8702,
      "step": 8570
    },
    {
      "epoch": 0.9369880965381675,
      "grad_norm": 0.16378262639045715,
      "learning_rate": 4.531724363874632e-05,
      "loss": 2.8553,
      "step": 8580
    },
    {
      "epoch": 0.9380801572567434,
      "grad_norm": 0.09967589378356934,
      "learning_rate": 4.5311783335153436e-05,
      "loss": 2.8571,
      "step": 8590
    },
    {
      "epoch": 0.9391722179753195,
      "grad_norm": 0.12584693729877472,
      "learning_rate": 4.5306323031560556e-05,
      "loss": 2.8654,
      "step": 8600
    },
    {
      "epoch": 0.9402642786938954,
      "grad_norm": 0.12754245102405548,
      "learning_rate": 4.5300862727967675e-05,
      "loss": 2.8701,
      "step": 8610
    },
    {
      "epoch": 0.9413563394124713,
      "grad_norm": 0.13468889892101288,
      "learning_rate": 4.52954024243748e-05,
      "loss": 2.8892,
      "step": 8620
    },
    {
      "epoch": 0.9424484001310472,
      "grad_norm": 0.3447522521018982,
      "learning_rate": 4.528994212078192e-05,
      "loss": 2.9126,
      "step": 8630
    },
    {
      "epoch": 0.9435404608496233,
      "grad_norm": 0.5023335814476013,
      "learning_rate": 4.528448181718904e-05,
      "loss": 2.8836,
      "step": 8640
    },
    {
      "epoch": 0.9446325215681992,
      "grad_norm": 0.17402608692646027,
      "learning_rate": 4.527902151359616e-05,
      "loss": 2.8647,
      "step": 8650
    },
    {
      "epoch": 0.9457245822867751,
      "grad_norm": 0.12210176885128021,
      "learning_rate": 4.527356121000328e-05,
      "loss": 2.8589,
      "step": 8660
    },
    {
      "epoch": 0.9468166430053511,
      "grad_norm": 0.8928805589675903,
      "learning_rate": 4.52681009064104e-05,
      "loss": 2.9368,
      "step": 8670
    },
    {
      "epoch": 0.9479087037239271,
      "grad_norm": 0.9808562397956848,
      "learning_rate": 4.526264060281752e-05,
      "loss": 2.8908,
      "step": 8680
    },
    {
      "epoch": 0.949000764442503,
      "grad_norm": 0.38967642188072205,
      "learning_rate": 4.525718029922464e-05,
      "loss": 2.8817,
      "step": 8690
    },
    {
      "epoch": 0.950092825161079,
      "grad_norm": 0.13093061745166779,
      "learning_rate": 4.525171999563176e-05,
      "loss": 2.8912,
      "step": 8700
    },
    {
      "epoch": 0.9511848858796549,
      "grad_norm": 0.33962950110435486,
      "learning_rate": 4.5246259692038876e-05,
      "loss": 2.9342,
      "step": 8710
    },
    {
      "epoch": 0.9522769465982308,
      "grad_norm": 0.2518679201602936,
      "learning_rate": 4.5240799388445996e-05,
      "loss": 2.8906,
      "step": 8720
    },
    {
      "epoch": 0.9533690073168068,
      "grad_norm": 0.14418579638004303,
      "learning_rate": 4.5235339084853115e-05,
      "loss": 2.8796,
      "step": 8730
    },
    {
      "epoch": 0.9544610680353828,
      "grad_norm": 0.1872376650571823,
      "learning_rate": 4.5229878781260234e-05,
      "loss": 2.8962,
      "step": 8740
    },
    {
      "epoch": 0.9555531287539587,
      "grad_norm": 0.2661217451095581,
      "learning_rate": 4.522441847766736e-05,
      "loss": 2.8791,
      "step": 8750
    },
    {
      "epoch": 0.9566451894725346,
      "grad_norm": 0.24702617526054382,
      "learning_rate": 4.521895817407448e-05,
      "loss": 2.8878,
      "step": 8760
    },
    {
      "epoch": 0.9577372501911107,
      "grad_norm": 0.09034670144319534,
      "learning_rate": 4.52134978704816e-05,
      "loss": 2.8667,
      "step": 8770
    },
    {
      "epoch": 0.9588293109096866,
      "grad_norm": 1.11221444606781,
      "learning_rate": 4.5208037566888726e-05,
      "loss": 2.8989,
      "step": 8780
    },
    {
      "epoch": 0.9599213716282625,
      "grad_norm": 0.3586934506893158,
      "learning_rate": 4.5202577263295845e-05,
      "loss": 2.8972,
      "step": 8790
    },
    {
      "epoch": 0.9610134323468384,
      "grad_norm": 0.22569432854652405,
      "learning_rate": 4.5197116959702964e-05,
      "loss": 2.8648,
      "step": 8800
    },
    {
      "epoch": 0.9621054930654145,
      "grad_norm": 0.21645715832710266,
      "learning_rate": 4.5191656656110084e-05,
      "loss": 2.8566,
      "step": 8810
    },
    {
      "epoch": 0.9631975537839904,
      "grad_norm": 0.1659054160118103,
      "learning_rate": 4.51861963525172e-05,
      "loss": 2.8491,
      "step": 8820
    },
    {
      "epoch": 0.9642896145025663,
      "grad_norm": 0.39270517230033875,
      "learning_rate": 4.518073604892432e-05,
      "loss": 2.8758,
      "step": 8830
    },
    {
      "epoch": 0.9653816752211423,
      "grad_norm": 0.18190832436084747,
      "learning_rate": 4.517527574533144e-05,
      "loss": 2.8572,
      "step": 8840
    },
    {
      "epoch": 0.9664737359397183,
      "grad_norm": 0.11265325546264648,
      "learning_rate": 4.516981544173856e-05,
      "loss": 2.8441,
      "step": 8850
    },
    {
      "epoch": 0.9675657966582942,
      "grad_norm": 0.7571744918823242,
      "learning_rate": 4.516435513814568e-05,
      "loss": 2.9063,
      "step": 8860
    },
    {
      "epoch": 0.9686578573768702,
      "grad_norm": 0.5819035768508911,
      "learning_rate": 4.51588948345528e-05,
      "loss": 2.8925,
      "step": 8870
    },
    {
      "epoch": 0.9697499180954461,
      "grad_norm": 0.1873948723077774,
      "learning_rate": 4.5153434530959926e-05,
      "loss": 2.8618,
      "step": 8880
    },
    {
      "epoch": 0.970841978814022,
      "grad_norm": 0.08295705169439316,
      "learning_rate": 4.5147974227367046e-05,
      "loss": 2.8592,
      "step": 8890
    },
    {
      "epoch": 0.971934039532598,
      "grad_norm": 0.21264569461345673,
      "learning_rate": 4.5142513923774165e-05,
      "loss": 3.0044,
      "step": 8900
    },
    {
      "epoch": 0.973026100251174,
      "grad_norm": 0.10405688732862473,
      "learning_rate": 4.5137053620181285e-05,
      "loss": 2.867,
      "step": 8910
    },
    {
      "epoch": 0.9741181609697499,
      "grad_norm": 0.1889263391494751,
      "learning_rate": 4.5131593316588404e-05,
      "loss": 2.8535,
      "step": 8920
    },
    {
      "epoch": 0.9752102216883258,
      "grad_norm": 0.38601240515708923,
      "learning_rate": 4.5126133012995523e-05,
      "loss": 2.8911,
      "step": 8930
    },
    {
      "epoch": 0.9763022824069019,
      "grad_norm": 0.11795299500226974,
      "learning_rate": 4.512067270940264e-05,
      "loss": 2.8955,
      "step": 8940
    },
    {
      "epoch": 0.9773943431254778,
      "grad_norm": 0.1707402914762497,
      "learning_rate": 4.511521240580976e-05,
      "loss": 2.8517,
      "step": 8950
    },
    {
      "epoch": 0.9784864038440537,
      "grad_norm": 0.09658535569906235,
      "learning_rate": 4.510975210221688e-05,
      "loss": 2.8707,
      "step": 8960
    },
    {
      "epoch": 0.9795784645626296,
      "grad_norm": 0.22554369270801544,
      "learning_rate": 4.5104291798624e-05,
      "loss": 2.8903,
      "step": 8970
    },
    {
      "epoch": 0.9806705252812057,
      "grad_norm": 0.3625219166278839,
      "learning_rate": 4.509883149503113e-05,
      "loss": 2.8558,
      "step": 8980
    },
    {
      "epoch": 0.9817625859997816,
      "grad_norm": 0.21793603897094727,
      "learning_rate": 4.509337119143825e-05,
      "loss": 2.8579,
      "step": 8990
    },
    {
      "epoch": 0.9828546467183575,
      "grad_norm": 0.11249877512454987,
      "learning_rate": 4.5087910887845366e-05,
      "loss": 2.9033,
      "step": 9000
    },
    {
      "epoch": 0.9839467074369335,
      "grad_norm": 0.1767159402370453,
      "learning_rate": 4.508245058425249e-05,
      "loss": 2.8489,
      "step": 9010
    },
    {
      "epoch": 0.9850387681555094,
      "grad_norm": 0.1633744090795517,
      "learning_rate": 4.507699028065961e-05,
      "loss": 2.8626,
      "step": 9020
    },
    {
      "epoch": 0.9861308288740854,
      "grad_norm": 0.42756348848342896,
      "learning_rate": 4.507152997706673e-05,
      "loss": 2.9064,
      "step": 9030
    },
    {
      "epoch": 0.9872228895926614,
      "grad_norm": 0.14707355201244354,
      "learning_rate": 4.506606967347385e-05,
      "loss": 2.9203,
      "step": 9040
    },
    {
      "epoch": 0.9883149503112373,
      "grad_norm": 0.17372147738933563,
      "learning_rate": 4.506060936988097e-05,
      "loss": 2.8445,
      "step": 9050
    },
    {
      "epoch": 0.9894070110298132,
      "grad_norm": 0.2927263379096985,
      "learning_rate": 4.505514906628809e-05,
      "loss": 2.8922,
      "step": 9060
    },
    {
      "epoch": 0.9904990717483892,
      "grad_norm": 0.20215483009815216,
      "learning_rate": 4.504968876269521e-05,
      "loss": 2.9401,
      "step": 9070
    },
    {
      "epoch": 0.9915911324669652,
      "grad_norm": 0.1585003286600113,
      "learning_rate": 4.504422845910233e-05,
      "loss": 2.8744,
      "step": 9080
    },
    {
      "epoch": 0.9926831931855411,
      "grad_norm": 0.11770275980234146,
      "learning_rate": 4.503876815550945e-05,
      "loss": 2.8778,
      "step": 9090
    },
    {
      "epoch": 0.993775253904117,
      "grad_norm": 0.1505434513092041,
      "learning_rate": 4.503330785191657e-05,
      "loss": 2.8683,
      "step": 9100
    },
    {
      "epoch": 0.9948673146226931,
      "grad_norm": 0.13814900815486908,
      "learning_rate": 4.5027847548323686e-05,
      "loss": 2.8645,
      "step": 9110
    },
    {
      "epoch": 0.995959375341269,
      "grad_norm": 0.1179598718881607,
      "learning_rate": 4.5022387244730806e-05,
      "loss": 2.8552,
      "step": 9120
    },
    {
      "epoch": 0.9970514360598449,
      "grad_norm": 0.3039071261882782,
      "learning_rate": 4.5016926941137925e-05,
      "loss": 2.8607,
      "step": 9130
    },
    {
      "epoch": 0.9981434967784208,
      "grad_norm": 0.2900407910346985,
      "learning_rate": 4.501146663754505e-05,
      "loss": 2.8657,
      "step": 9140
    },
    {
      "epoch": 0.9992355574969969,
      "grad_norm": 0.1448604315519333,
      "learning_rate": 4.500600633395217e-05,
      "loss": 2.8523,
      "step": 9150
    },
    {
      "epoch": 1.0003276182155727,
      "grad_norm": 0.2111513316631317,
      "learning_rate": 4.500054603035929e-05,
      "loss": 2.8541,
      "step": 9160
    },
    {
      "epoch": 1.0014196789341487,
      "grad_norm": 0.1495814025402069,
      "learning_rate": 4.499508572676641e-05,
      "loss": 2.8689,
      "step": 9170
    },
    {
      "epoch": 1.0025117396527248,
      "grad_norm": 0.1673811674118042,
      "learning_rate": 4.498962542317353e-05,
      "loss": 2.8684,
      "step": 9180
    },
    {
      "epoch": 1.0036038003713006,
      "grad_norm": 0.11184374988079071,
      "learning_rate": 4.498416511958065e-05,
      "loss": 2.8966,
      "step": 9190
    },
    {
      "epoch": 1.0046958610898766,
      "grad_norm": 0.30651554465293884,
      "learning_rate": 4.497870481598777e-05,
      "loss": 2.8676,
      "step": 9200
    },
    {
      "epoch": 1.0057879218084524,
      "grad_norm": 0.13199421763420105,
      "learning_rate": 4.4973244512394894e-05,
      "loss": 2.9,
      "step": 9210
    },
    {
      "epoch": 1.0068799825270285,
      "grad_norm": 0.4105694890022278,
      "learning_rate": 4.4967784208802013e-05,
      "loss": 2.8929,
      "step": 9220
    },
    {
      "epoch": 1.0079720432456045,
      "grad_norm": 0.16782329976558685,
      "learning_rate": 4.496232390520913e-05,
      "loss": 2.9053,
      "step": 9230
    },
    {
      "epoch": 1.0090641039641803,
      "grad_norm": 0.1820969581604004,
      "learning_rate": 4.495686360161625e-05,
      "loss": 2.9085,
      "step": 9240
    },
    {
      "epoch": 1.0101561646827564,
      "grad_norm": 0.13307620584964752,
      "learning_rate": 4.495140329802337e-05,
      "loss": 2.8535,
      "step": 9250
    },
    {
      "epoch": 1.0112482254013324,
      "grad_norm": 0.1768629550933838,
      "learning_rate": 4.494594299443049e-05,
      "loss": 2.88,
      "step": 9260
    },
    {
      "epoch": 1.0123402861199082,
      "grad_norm": 0.33954915404319763,
      "learning_rate": 4.494048269083762e-05,
      "loss": 2.8888,
      "step": 9270
    },
    {
      "epoch": 1.0134323468384843,
      "grad_norm": 0.16305653750896454,
      "learning_rate": 4.493502238724474e-05,
      "loss": 2.8492,
      "step": 9280
    },
    {
      "epoch": 1.01452440755706,
      "grad_norm": 0.14236068725585938,
      "learning_rate": 4.4929562083651856e-05,
      "loss": 2.8586,
      "step": 9290
    },
    {
      "epoch": 1.0156164682756361,
      "grad_norm": 0.15932203829288483,
      "learning_rate": 4.4924101780058976e-05,
      "loss": 2.8475,
      "step": 9300
    },
    {
      "epoch": 1.0167085289942122,
      "grad_norm": 0.16392864286899567,
      "learning_rate": 4.4918641476466095e-05,
      "loss": 2.8636,
      "step": 9310
    },
    {
      "epoch": 1.017800589712788,
      "grad_norm": 0.12356557697057724,
      "learning_rate": 4.4913181172873214e-05,
      "loss": 2.8332,
      "step": 9320
    },
    {
      "epoch": 1.018892650431364,
      "grad_norm": 0.10844803601503372,
      "learning_rate": 4.4907720869280334e-05,
      "loss": 2.8728,
      "step": 9330
    },
    {
      "epoch": 1.0199847111499398,
      "grad_norm": 0.5331308841705322,
      "learning_rate": 4.490226056568745e-05,
      "loss": 2.8943,
      "step": 9340
    },
    {
      "epoch": 1.0210767718685159,
      "grad_norm": 0.1725131869316101,
      "learning_rate": 4.489680026209457e-05,
      "loss": 2.8601,
      "step": 9350
    },
    {
      "epoch": 1.022168832587092,
      "grad_norm": 0.1141042485833168,
      "learning_rate": 4.489133995850169e-05,
      "loss": 2.8701,
      "step": 9360
    },
    {
      "epoch": 1.0232608933056677,
      "grad_norm": 0.08743453770875931,
      "learning_rate": 4.488587965490881e-05,
      "loss": 2.8531,
      "step": 9370
    },
    {
      "epoch": 1.0243529540242438,
      "grad_norm": 0.1955462396144867,
      "learning_rate": 4.488041935131593e-05,
      "loss": 2.8316,
      "step": 9380
    },
    {
      "epoch": 1.0254450147428198,
      "grad_norm": 0.10020673274993896,
      "learning_rate": 4.487495904772305e-05,
      "loss": 2.8788,
      "step": 9390
    },
    {
      "epoch": 1.0265370754613956,
      "grad_norm": 0.5505827069282532,
      "learning_rate": 4.4869498744130176e-05,
      "loss": 2.8748,
      "step": 9400
    },
    {
      "epoch": 1.0276291361799716,
      "grad_norm": 2.927521228790283,
      "learning_rate": 4.4864038440537296e-05,
      "loss": 2.9927,
      "step": 9410
    },
    {
      "epoch": 1.0287211968985475,
      "grad_norm": 0.1855514645576477,
      "learning_rate": 4.4858578136944415e-05,
      "loss": 2.9156,
      "step": 9420
    },
    {
      "epoch": 1.0298132576171235,
      "grad_norm": 0.27460676431655884,
      "learning_rate": 4.4853117833351535e-05,
      "loss": 2.9053,
      "step": 9430
    },
    {
      "epoch": 1.0309053183356995,
      "grad_norm": 0.12148751318454742,
      "learning_rate": 4.484765752975866e-05,
      "loss": 2.8677,
      "step": 9440
    },
    {
      "epoch": 1.0319973790542754,
      "grad_norm": 0.16257460415363312,
      "learning_rate": 4.484219722616578e-05,
      "loss": 2.9031,
      "step": 9450
    },
    {
      "epoch": 1.0330894397728514,
      "grad_norm": 0.20607154071331024,
      "learning_rate": 4.48367369225729e-05,
      "loss": 2.8826,
      "step": 9460
    },
    {
      "epoch": 1.0341815004914274,
      "grad_norm": 0.25945910811424255,
      "learning_rate": 4.483127661898002e-05,
      "loss": 2.8844,
      "step": 9470
    },
    {
      "epoch": 1.0352735612100032,
      "grad_norm": 0.31629249453544617,
      "learning_rate": 4.482581631538714e-05,
      "loss": 2.8418,
      "step": 9480
    },
    {
      "epoch": 1.0363656219285793,
      "grad_norm": 0.30768898129463196,
      "learning_rate": 4.482035601179426e-05,
      "loss": 2.8967,
      "step": 9490
    },
    {
      "epoch": 1.037457682647155,
      "grad_norm": 0.1794349104166031,
      "learning_rate": 4.481489570820138e-05,
      "loss": 2.858,
      "step": 9500
    },
    {
      "epoch": 1.0385497433657311,
      "grad_norm": 0.2873963713645935,
      "learning_rate": 4.48094354046085e-05,
      "loss": 2.851,
      "step": 9510
    },
    {
      "epoch": 1.0396418040843072,
      "grad_norm": 0.145773783326149,
      "learning_rate": 4.4803975101015616e-05,
      "loss": 2.9012,
      "step": 9520
    },
    {
      "epoch": 1.040733864802883,
      "grad_norm": 0.18231414258480072,
      "learning_rate": 4.479851479742274e-05,
      "loss": 2.8599,
      "step": 9530
    },
    {
      "epoch": 1.041825925521459,
      "grad_norm": 0.14625324308872223,
      "learning_rate": 4.479305449382986e-05,
      "loss": 2.8498,
      "step": 9540
    },
    {
      "epoch": 1.0429179862400348,
      "grad_norm": 0.18947987258434296,
      "learning_rate": 4.478759419023698e-05,
      "loss": 2.8692,
      "step": 9550
    },
    {
      "epoch": 1.0440100469586109,
      "grad_norm": 0.774933934211731,
      "learning_rate": 4.47821338866441e-05,
      "loss": 2.8981,
      "step": 9560
    },
    {
      "epoch": 1.045102107677187,
      "grad_norm": 0.11057677865028381,
      "learning_rate": 4.477667358305122e-05,
      "loss": 2.8632,
      "step": 9570
    },
    {
      "epoch": 1.0461941683957627,
      "grad_norm": 0.27672064304351807,
      "learning_rate": 4.477121327945834e-05,
      "loss": 2.8668,
      "step": 9580
    },
    {
      "epoch": 1.0472862291143388,
      "grad_norm": 0.11621365696191788,
      "learning_rate": 4.476575297586546e-05,
      "loss": 2.8587,
      "step": 9590
    },
    {
      "epoch": 1.0483782898329148,
      "grad_norm": 0.21165798604488373,
      "learning_rate": 4.476029267227258e-05,
      "loss": 2.8893,
      "step": 9600
    },
    {
      "epoch": 1.0494703505514906,
      "grad_norm": 0.8999245762825012,
      "learning_rate": 4.47548323686797e-05,
      "loss": 2.911,
      "step": 9610
    },
    {
      "epoch": 1.0505624112700667,
      "grad_norm": 0.10780136287212372,
      "learning_rate": 4.474937206508682e-05,
      "loss": 2.8562,
      "step": 9620
    },
    {
      "epoch": 1.0516544719886425,
      "grad_norm": 0.11260148137807846,
      "learning_rate": 4.4743911761493936e-05,
      "loss": 2.8729,
      "step": 9630
    },
    {
      "epoch": 1.0527465327072185,
      "grad_norm": 0.12980981171131134,
      "learning_rate": 4.473845145790106e-05,
      "loss": 2.8546,
      "step": 9640
    },
    {
      "epoch": 1.0538385934257946,
      "grad_norm": 0.1469738930463791,
      "learning_rate": 4.473299115430818e-05,
      "loss": 2.882,
      "step": 9650
    },
    {
      "epoch": 1.0549306541443704,
      "grad_norm": 0.15866748988628387,
      "learning_rate": 4.47275308507153e-05,
      "loss": 2.8462,
      "step": 9660
    },
    {
      "epoch": 1.0560227148629464,
      "grad_norm": 0.10800832509994507,
      "learning_rate": 4.472207054712243e-05,
      "loss": 2.858,
      "step": 9670
    },
    {
      "epoch": 1.0571147755815222,
      "grad_norm": 0.09047269076108932,
      "learning_rate": 4.471661024352955e-05,
      "loss": 2.8829,
      "step": 9680
    },
    {
      "epoch": 1.0582068363000983,
      "grad_norm": 0.13794751465320587,
      "learning_rate": 4.4711149939936666e-05,
      "loss": 2.8689,
      "step": 9690
    },
    {
      "epoch": 1.0592988970186743,
      "grad_norm": 0.26925739645957947,
      "learning_rate": 4.4705689636343786e-05,
      "loss": 2.9262,
      "step": 9700
    },
    {
      "epoch": 1.0603909577372501,
      "grad_norm": 0.20053517818450928,
      "learning_rate": 4.4700229332750905e-05,
      "loss": 2.9113,
      "step": 9710
    },
    {
      "epoch": 1.0614830184558262,
      "grad_norm": 0.13897787034511566,
      "learning_rate": 4.4694769029158025e-05,
      "loss": 2.8808,
      "step": 9720
    },
    {
      "epoch": 1.0625750791744022,
      "grad_norm": 0.1660907119512558,
      "learning_rate": 4.4689308725565144e-05,
      "loss": 2.8767,
      "step": 9730
    },
    {
      "epoch": 1.063667139892978,
      "grad_norm": 0.15063542127609253,
      "learning_rate": 4.4683848421972263e-05,
      "loss": 2.8679,
      "step": 9740
    },
    {
      "epoch": 1.064759200611554,
      "grad_norm": 0.3042473793029785,
      "learning_rate": 4.467838811837938e-05,
      "loss": 2.8631,
      "step": 9750
    },
    {
      "epoch": 1.0658512613301299,
      "grad_norm": 0.12352876365184784,
      "learning_rate": 4.46729278147865e-05,
      "loss": 2.8762,
      "step": 9760
    },
    {
      "epoch": 1.066943322048706,
      "grad_norm": 0.21888995170593262,
      "learning_rate": 4.466746751119362e-05,
      "loss": 2.8709,
      "step": 9770
    },
    {
      "epoch": 1.068035382767282,
      "grad_norm": 0.2800211012363434,
      "learning_rate": 4.466200720760074e-05,
      "loss": 2.9071,
      "step": 9780
    },
    {
      "epoch": 1.0691274434858578,
      "grad_norm": 0.20856378972530365,
      "learning_rate": 4.465654690400787e-05,
      "loss": 2.9539,
      "step": 9790
    },
    {
      "epoch": 1.0702195042044338,
      "grad_norm": 0.33213376998901367,
      "learning_rate": 4.465108660041499e-05,
      "loss": 2.9079,
      "step": 9800
    },
    {
      "epoch": 1.0713115649230098,
      "grad_norm": 0.21886956691741943,
      "learning_rate": 4.4645626296822106e-05,
      "loss": 2.8793,
      "step": 9810
    },
    {
      "epoch": 1.0724036256415856,
      "grad_norm": 0.35651007294654846,
      "learning_rate": 4.4640165993229225e-05,
      "loss": 2.8583,
      "step": 9820
    },
    {
      "epoch": 1.0734956863601617,
      "grad_norm": 0.13481232523918152,
      "learning_rate": 4.4634705689636345e-05,
      "loss": 2.8545,
      "step": 9830
    },
    {
      "epoch": 1.0745877470787375,
      "grad_norm": 0.19365762174129486,
      "learning_rate": 4.4629245386043464e-05,
      "loss": 2.8528,
      "step": 9840
    },
    {
      "epoch": 1.0756798077973135,
      "grad_norm": 0.11404673010110855,
      "learning_rate": 4.4623785082450584e-05,
      "loss": 2.8482,
      "step": 9850
    },
    {
      "epoch": 1.0767718685158896,
      "grad_norm": 0.27331098914146423,
      "learning_rate": 4.46183247788577e-05,
      "loss": 2.8896,
      "step": 9860
    },
    {
      "epoch": 1.0778639292344654,
      "grad_norm": 0.19337323307991028,
      "learning_rate": 4.461286447526483e-05,
      "loss": 2.874,
      "step": 9870
    },
    {
      "epoch": 1.0789559899530414,
      "grad_norm": 0.11590950936079025,
      "learning_rate": 4.460740417167195e-05,
      "loss": 2.8427,
      "step": 9880
    },
    {
      "epoch": 1.0800480506716172,
      "grad_norm": 0.1296950727701187,
      "learning_rate": 4.460194386807907e-05,
      "loss": 2.8848,
      "step": 9890
    },
    {
      "epoch": 1.0811401113901933,
      "grad_norm": 0.27705079317092896,
      "learning_rate": 4.459648356448619e-05,
      "loss": 2.8807,
      "step": 9900
    },
    {
      "epoch": 1.0822321721087693,
      "grad_norm": 1.3492182493209839,
      "learning_rate": 4.459102326089331e-05,
      "loss": 2.9212,
      "step": 9910
    },
    {
      "epoch": 1.0833242328273451,
      "grad_norm": 0.26979494094848633,
      "learning_rate": 4.458556295730043e-05,
      "loss": 2.8689,
      "step": 9920
    },
    {
      "epoch": 1.0844162935459212,
      "grad_norm": 0.3139997720718384,
      "learning_rate": 4.458010265370755e-05,
      "loss": 2.8646,
      "step": 9930
    },
    {
      "epoch": 1.0855083542644972,
      "grad_norm": 0.21191999316215515,
      "learning_rate": 4.457464235011467e-05,
      "loss": 2.8416,
      "step": 9940
    },
    {
      "epoch": 1.086600414983073,
      "grad_norm": 0.23759615421295166,
      "learning_rate": 4.456918204652179e-05,
      "loss": 2.8962,
      "step": 9950
    },
    {
      "epoch": 1.087692475701649,
      "grad_norm": 0.231721892952919,
      "learning_rate": 4.456372174292891e-05,
      "loss": 2.8752,
      "step": 9960
    },
    {
      "epoch": 1.0887845364202249,
      "grad_norm": 0.21115945279598236,
      "learning_rate": 4.455826143933603e-05,
      "loss": 2.8616,
      "step": 9970
    },
    {
      "epoch": 1.089876597138801,
      "grad_norm": 0.13905847072601318,
      "learning_rate": 4.455280113574315e-05,
      "loss": 2.881,
      "step": 9980
    },
    {
      "epoch": 1.090968657857377,
      "grad_norm": 0.15387795865535736,
      "learning_rate": 4.454734083215027e-05,
      "loss": 2.8479,
      "step": 9990
    },
    {
      "epoch": 1.0920607185759528,
      "grad_norm": 0.30674850940704346,
      "learning_rate": 4.454188052855739e-05,
      "loss": 2.9441,
      "step": 10000
    },
    {
      "epoch": 1.0931527792945288,
      "grad_norm": 0.5632759928703308,
      "learning_rate": 4.453642022496451e-05,
      "loss": 2.973,
      "step": 10010
    },
    {
      "epoch": 1.0942448400131046,
      "grad_norm": 0.21575379371643066,
      "learning_rate": 4.453095992137163e-05,
      "loss": 2.8804,
      "step": 10020
    },
    {
      "epoch": 1.0953369007316807,
      "grad_norm": 0.4778563380241394,
      "learning_rate": 4.452549961777875e-05,
      "loss": 2.8836,
      "step": 10030
    },
    {
      "epoch": 1.0964289614502567,
      "grad_norm": 0.18842101097106934,
      "learning_rate": 4.4520039314185866e-05,
      "loss": 2.9053,
      "step": 10040
    },
    {
      "epoch": 1.0975210221688325,
      "grad_norm": 0.44165483117103577,
      "learning_rate": 4.451457901059299e-05,
      "loss": 2.895,
      "step": 10050
    },
    {
      "epoch": 1.0986130828874086,
      "grad_norm": 0.6246609687805176,
      "learning_rate": 4.450911870700011e-05,
      "loss": 2.9316,
      "step": 10060
    },
    {
      "epoch": 1.0997051436059846,
      "grad_norm": 0.36214640736579895,
      "learning_rate": 4.450365840340723e-05,
      "loss": 2.8867,
      "step": 10070
    },
    {
      "epoch": 1.1007972043245604,
      "grad_norm": 0.2318934202194214,
      "learning_rate": 4.449819809981435e-05,
      "loss": 2.9734,
      "step": 10080
    },
    {
      "epoch": 1.1018892650431364,
      "grad_norm": 0.19936111569404602,
      "learning_rate": 4.449273779622147e-05,
      "loss": 2.9411,
      "step": 10090
    },
    {
      "epoch": 1.1029813257617123,
      "grad_norm": 0.25889909267425537,
      "learning_rate": 4.4487277492628596e-05,
      "loss": 2.8687,
      "step": 10100
    },
    {
      "epoch": 1.1040733864802883,
      "grad_norm": 0.3904361426830292,
      "learning_rate": 4.4481817189035715e-05,
      "loss": 2.8622,
      "step": 10110
    },
    {
      "epoch": 1.1051654471988643,
      "grad_norm": 0.15358039736747742,
      "learning_rate": 4.4476356885442835e-05,
      "loss": 2.8623,
      "step": 10120
    },
    {
      "epoch": 1.1062575079174402,
      "grad_norm": 0.2651018798351288,
      "learning_rate": 4.4470896581849954e-05,
      "loss": 2.9141,
      "step": 10130
    },
    {
      "epoch": 1.1073495686360162,
      "grad_norm": 0.16983450949192047,
      "learning_rate": 4.4465436278257074e-05,
      "loss": 2.8889,
      "step": 10140
    },
    {
      "epoch": 1.108441629354592,
      "grad_norm": 0.2421373724937439,
      "learning_rate": 4.445997597466419e-05,
      "loss": 2.8535,
      "step": 10150
    },
    {
      "epoch": 1.109533690073168,
      "grad_norm": 0.9783491492271423,
      "learning_rate": 4.445451567107131e-05,
      "loss": 2.886,
      "step": 10160
    },
    {
      "epoch": 1.110625750791744,
      "grad_norm": 0.16407319903373718,
      "learning_rate": 4.444905536747843e-05,
      "loss": 2.9376,
      "step": 10170
    },
    {
      "epoch": 1.11171781151032,
      "grad_norm": 0.23803706467151642,
      "learning_rate": 4.444359506388556e-05,
      "loss": 2.8805,
      "step": 10180
    },
    {
      "epoch": 1.112809872228896,
      "grad_norm": 0.18806056678295135,
      "learning_rate": 4.443813476029268e-05,
      "loss": 2.9048,
      "step": 10190
    },
    {
      "epoch": 1.113901932947472,
      "grad_norm": 0.12411818653345108,
      "learning_rate": 4.44326744566998e-05,
      "loss": 2.8793,
      "step": 10200
    },
    {
      "epoch": 1.1149939936660478,
      "grad_norm": 0.31610479950904846,
      "learning_rate": 4.4427214153106916e-05,
      "loss": 2.8971,
      "step": 10210
    },
    {
      "epoch": 1.1160860543846238,
      "grad_norm": 0.35768914222717285,
      "learning_rate": 4.4421753849514036e-05,
      "loss": 2.8738,
      "step": 10220
    },
    {
      "epoch": 1.1171781151031996,
      "grad_norm": 0.2258564978837967,
      "learning_rate": 4.4416293545921155e-05,
      "loss": 2.8997,
      "step": 10230
    },
    {
      "epoch": 1.1182701758217757,
      "grad_norm": 0.15910403430461884,
      "learning_rate": 4.4410833242328275e-05,
      "loss": 2.8531,
      "step": 10240
    },
    {
      "epoch": 1.1193622365403517,
      "grad_norm": 0.474134236574173,
      "learning_rate": 4.4405372938735394e-05,
      "loss": 2.8772,
      "step": 10250
    },
    {
      "epoch": 1.1204542972589275,
      "grad_norm": 0.3702395558357239,
      "learning_rate": 4.439991263514251e-05,
      "loss": 2.8657,
      "step": 10260
    },
    {
      "epoch": 1.1215463579775036,
      "grad_norm": 0.1544666439294815,
      "learning_rate": 4.439445233154963e-05,
      "loss": 2.8612,
      "step": 10270
    },
    {
      "epoch": 1.1226384186960794,
      "grad_norm": 0.13689777255058289,
      "learning_rate": 4.438899202795675e-05,
      "loss": 2.8493,
      "step": 10280
    },
    {
      "epoch": 1.1237304794146554,
      "grad_norm": 0.5317530632019043,
      "learning_rate": 4.438353172436387e-05,
      "loss": 2.8996,
      "step": 10290
    },
    {
      "epoch": 1.1248225401332315,
      "grad_norm": 0.16116179525852203,
      "learning_rate": 4.4378071420771e-05,
      "loss": 2.8707,
      "step": 10300
    },
    {
      "epoch": 1.1259146008518073,
      "grad_norm": 0.265815794467926,
      "learning_rate": 4.437261111717812e-05,
      "loss": 2.8871,
      "step": 10310
    },
    {
      "epoch": 1.1270066615703833,
      "grad_norm": 0.16758079826831818,
      "learning_rate": 4.436715081358524e-05,
      "loss": 2.8502,
      "step": 10320
    },
    {
      "epoch": 1.1280987222889594,
      "grad_norm": 0.0972011461853981,
      "learning_rate": 4.436169050999236e-05,
      "loss": 2.8702,
      "step": 10330
    },
    {
      "epoch": 1.1291907830075352,
      "grad_norm": 0.32132163643836975,
      "learning_rate": 4.435623020639948e-05,
      "loss": 2.8484,
      "step": 10340
    },
    {
      "epoch": 1.1302828437261112,
      "grad_norm": 0.7599954009056091,
      "learning_rate": 4.43507699028066e-05,
      "loss": 2.8615,
      "step": 10350
    },
    {
      "epoch": 1.1313749044446872,
      "grad_norm": 0.22178131341934204,
      "learning_rate": 4.434530959921372e-05,
      "loss": 2.9153,
      "step": 10360
    },
    {
      "epoch": 1.132466965163263,
      "grad_norm": 0.12135827541351318,
      "learning_rate": 4.433984929562084e-05,
      "loss": 2.8585,
      "step": 10370
    },
    {
      "epoch": 1.133559025881839,
      "grad_norm": 0.16941170394420624,
      "learning_rate": 4.433438899202796e-05,
      "loss": 2.8845,
      "step": 10380
    },
    {
      "epoch": 1.134651086600415,
      "grad_norm": 0.2435552030801773,
      "learning_rate": 4.432892868843508e-05,
      "loss": 2.875,
      "step": 10390
    },
    {
      "epoch": 1.135743147318991,
      "grad_norm": 0.1682615429162979,
      "learning_rate": 4.43234683848422e-05,
      "loss": 2.8734,
      "step": 10400
    },
    {
      "epoch": 1.1368352080375668,
      "grad_norm": 0.3961694836616516,
      "learning_rate": 4.431800808124932e-05,
      "loss": 2.8743,
      "step": 10410
    },
    {
      "epoch": 1.1379272687561428,
      "grad_norm": 0.1608988493680954,
      "learning_rate": 4.431254777765644e-05,
      "loss": 2.8531,
      "step": 10420
    },
    {
      "epoch": 1.1390193294747188,
      "grad_norm": 0.21952320635318756,
      "learning_rate": 4.430708747406356e-05,
      "loss": 2.8494,
      "step": 10430
    },
    {
      "epoch": 1.1401113901932947,
      "grad_norm": 0.17190974950790405,
      "learning_rate": 4.4301627170470676e-05,
      "loss": 2.8892,
      "step": 10440
    },
    {
      "epoch": 1.1412034509118707,
      "grad_norm": 0.1136082261800766,
      "learning_rate": 4.42961668668778e-05,
      "loss": 2.8668,
      "step": 10450
    },
    {
      "epoch": 1.1422955116304467,
      "grad_norm": 1.5563620328903198,
      "learning_rate": 4.429070656328492e-05,
      "loss": 2.9124,
      "step": 10460
    },
    {
      "epoch": 1.1433875723490226,
      "grad_norm": 0.17018267512321472,
      "learning_rate": 4.428524625969204e-05,
      "loss": 2.8695,
      "step": 10470
    },
    {
      "epoch": 1.1444796330675986,
      "grad_norm": 0.14121313393115997,
      "learning_rate": 4.427978595609916e-05,
      "loss": 2.8702,
      "step": 10480
    },
    {
      "epoch": 1.1455716937861746,
      "grad_norm": 0.15501639246940613,
      "learning_rate": 4.427432565250628e-05,
      "loss": 2.8705,
      "step": 10490
    },
    {
      "epoch": 1.1466637545047504,
      "grad_norm": 0.19954845309257507,
      "learning_rate": 4.42688653489134e-05,
      "loss": 2.8511,
      "step": 10500
    },
    {
      "epoch": 1.1477558152233265,
      "grad_norm": 0.138383686542511,
      "learning_rate": 4.426340504532052e-05,
      "loss": 2.8567,
      "step": 10510
    },
    {
      "epoch": 1.1488478759419023,
      "grad_norm": 0.21165122091770172,
      "learning_rate": 4.425794474172764e-05,
      "loss": 2.8846,
      "step": 10520
    },
    {
      "epoch": 1.1499399366604783,
      "grad_norm": 0.4983291029930115,
      "learning_rate": 4.4252484438134765e-05,
      "loss": 2.8896,
      "step": 10530
    },
    {
      "epoch": 1.1510319973790544,
      "grad_norm": 0.42393431067466736,
      "learning_rate": 4.4247024134541884e-05,
      "loss": 2.904,
      "step": 10540
    },
    {
      "epoch": 1.1521240580976302,
      "grad_norm": 0.13592985272407532,
      "learning_rate": 4.4241563830949e-05,
      "loss": 2.8587,
      "step": 10550
    },
    {
      "epoch": 1.1532161188162062,
      "grad_norm": 0.18475013971328735,
      "learning_rate": 4.423610352735612e-05,
      "loss": 2.8658,
      "step": 10560
    },
    {
      "epoch": 1.154308179534782,
      "grad_norm": 0.13591255247592926,
      "learning_rate": 4.423064322376324e-05,
      "loss": 2.8866,
      "step": 10570
    },
    {
      "epoch": 1.155400240253358,
      "grad_norm": 0.23103100061416626,
      "learning_rate": 4.422518292017037e-05,
      "loss": 2.8617,
      "step": 10580
    },
    {
      "epoch": 1.1564923009719341,
      "grad_norm": 0.14958742260932922,
      "learning_rate": 4.421972261657749e-05,
      "loss": 2.8744,
      "step": 10590
    },
    {
      "epoch": 1.15758436169051,
      "grad_norm": 0.14963501691818237,
      "learning_rate": 4.421426231298461e-05,
      "loss": 2.982,
      "step": 10600
    },
    {
      "epoch": 1.158676422409086,
      "grad_norm": 0.2884026765823364,
      "learning_rate": 4.4208802009391727e-05,
      "loss": 2.8921,
      "step": 10610
    },
    {
      "epoch": 1.159768483127662,
      "grad_norm": 0.25955525040626526,
      "learning_rate": 4.4203341705798846e-05,
      "loss": 2.8935,
      "step": 10620
    },
    {
      "epoch": 1.1608605438462378,
      "grad_norm": 0.1452605277299881,
      "learning_rate": 4.4197881402205965e-05,
      "loss": 2.8689,
      "step": 10630
    },
    {
      "epoch": 1.1619526045648139,
      "grad_norm": 0.29747170209884644,
      "learning_rate": 4.4192421098613085e-05,
      "loss": 2.831,
      "step": 10640
    },
    {
      "epoch": 1.1630446652833897,
      "grad_norm": 0.386310875415802,
      "learning_rate": 4.4186960795020204e-05,
      "loss": 2.8874,
      "step": 10650
    },
    {
      "epoch": 1.1641367260019657,
      "grad_norm": 0.20302532613277435,
      "learning_rate": 4.4181500491427324e-05,
      "loss": 2.8489,
      "step": 10660
    },
    {
      "epoch": 1.1652287867205418,
      "grad_norm": 0.5030813813209534,
      "learning_rate": 4.417604018783444e-05,
      "loss": 2.8843,
      "step": 10670
    },
    {
      "epoch": 1.1663208474391176,
      "grad_norm": 0.22808977961540222,
      "learning_rate": 4.417057988424156e-05,
      "loss": 2.8598,
      "step": 10680
    },
    {
      "epoch": 1.1674129081576936,
      "grad_norm": 0.17831900715827942,
      "learning_rate": 4.416511958064868e-05,
      "loss": 2.8904,
      "step": 10690
    },
    {
      "epoch": 1.1685049688762694,
      "grad_norm": 0.38883039355278015,
      "learning_rate": 4.41596592770558e-05,
      "loss": 2.873,
      "step": 10700
    },
    {
      "epoch": 1.1695970295948455,
      "grad_norm": 0.0922757163643837,
      "learning_rate": 4.415419897346293e-05,
      "loss": 2.8962,
      "step": 10710
    },
    {
      "epoch": 1.1706890903134215,
      "grad_norm": 0.16424180567264557,
      "learning_rate": 4.414873866987005e-05,
      "loss": 2.8379,
      "step": 10720
    },
    {
      "epoch": 1.1717811510319973,
      "grad_norm": 0.17223210632801056,
      "learning_rate": 4.4143278366277166e-05,
      "loss": 2.8519,
      "step": 10730
    },
    {
      "epoch": 1.1728732117505734,
      "grad_norm": 0.30315321683883667,
      "learning_rate": 4.4137818062684286e-05,
      "loss": 2.91,
      "step": 10740
    },
    {
      "epoch": 1.1739652724691494,
      "grad_norm": 0.41103196144104004,
      "learning_rate": 4.4132357759091405e-05,
      "loss": 2.8356,
      "step": 10750
    },
    {
      "epoch": 1.1750573331877252,
      "grad_norm": 0.230826735496521,
      "learning_rate": 4.412689745549853e-05,
      "loss": 2.8701,
      "step": 10760
    },
    {
      "epoch": 1.1761493939063012,
      "grad_norm": 0.15060925483703613,
      "learning_rate": 4.412143715190565e-05,
      "loss": 2.8477,
      "step": 10770
    },
    {
      "epoch": 1.177241454624877,
      "grad_norm": 0.3304377496242523,
      "learning_rate": 4.411597684831277e-05,
      "loss": 2.8677,
      "step": 10780
    },
    {
      "epoch": 1.178333515343453,
      "grad_norm": 0.3131745755672455,
      "learning_rate": 4.411051654471989e-05,
      "loss": 2.9014,
      "step": 10790
    },
    {
      "epoch": 1.1794255760620291,
      "grad_norm": 0.2982882857322693,
      "learning_rate": 4.410505624112701e-05,
      "loss": 2.8643,
      "step": 10800
    },
    {
      "epoch": 1.180517636780605,
      "grad_norm": 0.1444038599729538,
      "learning_rate": 4.409959593753413e-05,
      "loss": 2.8515,
      "step": 10810
    },
    {
      "epoch": 1.181609697499181,
      "grad_norm": 0.1667843908071518,
      "learning_rate": 4.409413563394125e-05,
      "loss": 2.8662,
      "step": 10820
    },
    {
      "epoch": 1.1827017582177568,
      "grad_norm": 0.6064040064811707,
      "learning_rate": 4.408867533034837e-05,
      "loss": 2.8709,
      "step": 10830
    },
    {
      "epoch": 1.1837938189363328,
      "grad_norm": 0.20711767673492432,
      "learning_rate": 4.408321502675549e-05,
      "loss": 2.856,
      "step": 10840
    },
    {
      "epoch": 1.1848858796549089,
      "grad_norm": 0.14613249897956848,
      "learning_rate": 4.407775472316261e-05,
      "loss": 2.8713,
      "step": 10850
    },
    {
      "epoch": 1.1859779403734847,
      "grad_norm": 0.2543288767337799,
      "learning_rate": 4.407229441956973e-05,
      "loss": 2.8792,
      "step": 10860
    },
    {
      "epoch": 1.1870700010920607,
      "grad_norm": 0.760931670665741,
      "learning_rate": 4.406683411597685e-05,
      "loss": 2.8866,
      "step": 10870
    },
    {
      "epoch": 1.1881620618106368,
      "grad_norm": 1.468165636062622,
      "learning_rate": 4.406137381238397e-05,
      "loss": 2.9616,
      "step": 10880
    },
    {
      "epoch": 1.1892541225292126,
      "grad_norm": 0.42698147892951965,
      "learning_rate": 4.405591350879109e-05,
      "loss": 2.8561,
      "step": 10890
    },
    {
      "epoch": 1.1903461832477886,
      "grad_norm": 0.2810900807380676,
      "learning_rate": 4.405045320519821e-05,
      "loss": 2.8596,
      "step": 10900
    },
    {
      "epoch": 1.1914382439663644,
      "grad_norm": 0.3039090037345886,
      "learning_rate": 4.404499290160533e-05,
      "loss": 2.856,
      "step": 10910
    },
    {
      "epoch": 1.1925303046849405,
      "grad_norm": 0.1497955322265625,
      "learning_rate": 4.403953259801245e-05,
      "loss": 2.8914,
      "step": 10920
    },
    {
      "epoch": 1.1936223654035165,
      "grad_norm": 0.1749580353498459,
      "learning_rate": 4.403407229441957e-05,
      "loss": 2.8333,
      "step": 10930
    },
    {
      "epoch": 1.1947144261220923,
      "grad_norm": 0.12151555716991425,
      "learning_rate": 4.402861199082669e-05,
      "loss": 2.8591,
      "step": 10940
    },
    {
      "epoch": 1.1958064868406684,
      "grad_norm": 0.17289283871650696,
      "learning_rate": 4.402315168723381e-05,
      "loss": 2.9217,
      "step": 10950
    },
    {
      "epoch": 1.1968985475592442,
      "grad_norm": 0.2856307625770569,
      "learning_rate": 4.401769138364093e-05,
      "loss": 2.8708,
      "step": 10960
    },
    {
      "epoch": 1.1979906082778202,
      "grad_norm": 0.2464221566915512,
      "learning_rate": 4.401223108004805e-05,
      "loss": 2.8752,
      "step": 10970
    },
    {
      "epoch": 1.1990826689963963,
      "grad_norm": 0.19061261415481567,
      "learning_rate": 4.400677077645517e-05,
      "loss": 2.853,
      "step": 10980
    },
    {
      "epoch": 1.200174729714972,
      "grad_norm": 0.29900506138801575,
      "learning_rate": 4.40013104728623e-05,
      "loss": 2.8969,
      "step": 10990
    },
    {
      "epoch": 1.2012667904335481,
      "grad_norm": 0.14959684014320374,
      "learning_rate": 4.399585016926942e-05,
      "loss": 2.8898,
      "step": 11000
    },
    {
      "epoch": 1.2023588511521242,
      "grad_norm": 0.26314014196395874,
      "learning_rate": 4.399038986567654e-05,
      "loss": 2.8739,
      "step": 11010
    },
    {
      "epoch": 1.2034509118707,
      "grad_norm": 0.14078941941261292,
      "learning_rate": 4.3984929562083656e-05,
      "loss": 2.9086,
      "step": 11020
    },
    {
      "epoch": 1.204542972589276,
      "grad_norm": 0.6481391191482544,
      "learning_rate": 4.3979469258490776e-05,
      "loss": 2.8626,
      "step": 11030
    },
    {
      "epoch": 1.2056350333078518,
      "grad_norm": 0.378927081823349,
      "learning_rate": 4.3974008954897895e-05,
      "loss": 2.919,
      "step": 11040
    },
    {
      "epoch": 1.2067270940264279,
      "grad_norm": 0.13150043785572052,
      "learning_rate": 4.3968548651305015e-05,
      "loss": 2.8692,
      "step": 11050
    },
    {
      "epoch": 1.207819154745004,
      "grad_norm": 0.20376341044902802,
      "learning_rate": 4.3963088347712134e-05,
      "loss": 2.8381,
      "step": 11060
    },
    {
      "epoch": 1.2089112154635797,
      "grad_norm": 0.2057316154241562,
      "learning_rate": 4.395762804411925e-05,
      "loss": 2.9041,
      "step": 11070
    },
    {
      "epoch": 1.2100032761821558,
      "grad_norm": 0.176612988114357,
      "learning_rate": 4.395216774052637e-05,
      "loss": 2.985,
      "step": 11080
    },
    {
      "epoch": 1.2110953369007316,
      "grad_norm": 0.179096519947052,
      "learning_rate": 4.394670743693349e-05,
      "loss": 2.8665,
      "step": 11090
    },
    {
      "epoch": 1.2121873976193076,
      "grad_norm": 0.09130823612213135,
      "learning_rate": 4.394124713334062e-05,
      "loss": 2.8681,
      "step": 11100
    },
    {
      "epoch": 1.2132794583378836,
      "grad_norm": 0.14236696064472198,
      "learning_rate": 4.393578682974774e-05,
      "loss": 2.838,
      "step": 11110
    },
    {
      "epoch": 1.2143715190564595,
      "grad_norm": 0.09562862664461136,
      "learning_rate": 4.393032652615486e-05,
      "loss": 2.8281,
      "step": 11120
    },
    {
      "epoch": 1.2154635797750355,
      "grad_norm": 0.1419745534658432,
      "learning_rate": 4.3924866222561977e-05,
      "loss": 2.8493,
      "step": 11130
    },
    {
      "epoch": 1.2165556404936115,
      "grad_norm": 0.3417430520057678,
      "learning_rate": 4.3919405918969096e-05,
      "loss": 2.884,
      "step": 11140
    },
    {
      "epoch": 1.2176477012121873,
      "grad_norm": 0.11758564412593842,
      "learning_rate": 4.3913945615376215e-05,
      "loss": 2.8655,
      "step": 11150
    },
    {
      "epoch": 1.2187397619307634,
      "grad_norm": 0.2811226546764374,
      "learning_rate": 4.3908485311783335e-05,
      "loss": 2.8902,
      "step": 11160
    },
    {
      "epoch": 1.2198318226493394,
      "grad_norm": 0.20287534594535828,
      "learning_rate": 4.3903025008190454e-05,
      "loss": 2.8856,
      "step": 11170
    },
    {
      "epoch": 1.2209238833679152,
      "grad_norm": 0.13552294671535492,
      "learning_rate": 4.3897564704597574e-05,
      "loss": 2.8694,
      "step": 11180
    },
    {
      "epoch": 1.2220159440864913,
      "grad_norm": 0.13361050188541412,
      "learning_rate": 4.38921044010047e-05,
      "loss": 2.8817,
      "step": 11190
    },
    {
      "epoch": 1.223108004805067,
      "grad_norm": 0.1307043582201004,
      "learning_rate": 4.388664409741182e-05,
      "loss": 2.8426,
      "step": 11200
    },
    {
      "epoch": 1.2242000655236431,
      "grad_norm": 0.1932465136051178,
      "learning_rate": 4.388118379381894e-05,
      "loss": 2.8379,
      "step": 11210
    },
    {
      "epoch": 1.225292126242219,
      "grad_norm": 0.2530709505081177,
      "learning_rate": 4.387572349022606e-05,
      "loss": 2.8678,
      "step": 11220
    },
    {
      "epoch": 1.226384186960795,
      "grad_norm": 0.27735400199890137,
      "learning_rate": 4.3870263186633184e-05,
      "loss": 2.8836,
      "step": 11230
    },
    {
      "epoch": 1.227476247679371,
      "grad_norm": 0.10060533136129379,
      "learning_rate": 4.3864802883040304e-05,
      "loss": 2.8709,
      "step": 11240
    },
    {
      "epoch": 1.2285683083979468,
      "grad_norm": 0.08956442773342133,
      "learning_rate": 4.385934257944742e-05,
      "loss": 2.8793,
      "step": 11250
    },
    {
      "epoch": 1.2296603691165229,
      "grad_norm": 0.2601095139980316,
      "learning_rate": 4.385388227585454e-05,
      "loss": 2.858,
      "step": 11260
    },
    {
      "epoch": 1.230752429835099,
      "grad_norm": 0.12529321014881134,
      "learning_rate": 4.384842197226166e-05,
      "loss": 2.9254,
      "step": 11270
    },
    {
      "epoch": 1.2318444905536747,
      "grad_norm": 0.30117157101631165,
      "learning_rate": 4.384296166866878e-05,
      "loss": 2.8673,
      "step": 11280
    },
    {
      "epoch": 1.2329365512722508,
      "grad_norm": 0.19787082076072693,
      "learning_rate": 4.38375013650759e-05,
      "loss": 2.881,
      "step": 11290
    },
    {
      "epoch": 1.2340286119908268,
      "grad_norm": 0.14991697669029236,
      "learning_rate": 4.383204106148302e-05,
      "loss": 2.861,
      "step": 11300
    },
    {
      "epoch": 1.2351206727094026,
      "grad_norm": 0.23803767561912537,
      "learning_rate": 4.382658075789014e-05,
      "loss": 2.8641,
      "step": 11310
    },
    {
      "epoch": 1.2362127334279787,
      "grad_norm": 0.18523555994033813,
      "learning_rate": 4.382112045429726e-05,
      "loss": 2.8647,
      "step": 11320
    },
    {
      "epoch": 1.2373047941465545,
      "grad_norm": 0.2747333347797394,
      "learning_rate": 4.381566015070438e-05,
      "loss": 2.8547,
      "step": 11330
    },
    {
      "epoch": 1.2383968548651305,
      "grad_norm": 0.3963906168937683,
      "learning_rate": 4.38101998471115e-05,
      "loss": 2.8779,
      "step": 11340
    },
    {
      "epoch": 1.2394889155837063,
      "grad_norm": 0.171804279088974,
      "learning_rate": 4.380473954351862e-05,
      "loss": 2.8952,
      "step": 11350
    },
    {
      "epoch": 1.2405809763022824,
      "grad_norm": 0.2677041292190552,
      "learning_rate": 4.379927923992574e-05,
      "loss": 2.8628,
      "step": 11360
    },
    {
      "epoch": 1.2416730370208584,
      "grad_norm": 0.2089606523513794,
      "learning_rate": 4.379381893633286e-05,
      "loss": 2.8651,
      "step": 11370
    },
    {
      "epoch": 1.2427650977394342,
      "grad_norm": 0.3175446093082428,
      "learning_rate": 4.378835863273998e-05,
      "loss": 2.8552,
      "step": 11380
    },
    {
      "epoch": 1.2438571584580103,
      "grad_norm": 0.1378428041934967,
      "learning_rate": 4.37828983291471e-05,
      "loss": 2.8585,
      "step": 11390
    },
    {
      "epoch": 1.2449492191765863,
      "grad_norm": 0.1222560703754425,
      "learning_rate": 4.377743802555422e-05,
      "loss": 2.9033,
      "step": 11400
    },
    {
      "epoch": 1.2460412798951621,
      "grad_norm": 0.13204118609428406,
      "learning_rate": 4.377197772196134e-05,
      "loss": 2.8564,
      "step": 11410
    },
    {
      "epoch": 1.2471333406137382,
      "grad_norm": 0.2434903085231781,
      "learning_rate": 4.3766517418368467e-05,
      "loss": 2.8952,
      "step": 11420
    },
    {
      "epoch": 1.2482254013323142,
      "grad_norm": 0.2793861925601959,
      "learning_rate": 4.3761057114775586e-05,
      "loss": 2.9004,
      "step": 11430
    },
    {
      "epoch": 1.24931746205089,
      "grad_norm": 0.1225714161992073,
      "learning_rate": 4.3755596811182705e-05,
      "loss": 2.8554,
      "step": 11440
    },
    {
      "epoch": 1.250409522769466,
      "grad_norm": 0.3286387324333191,
      "learning_rate": 4.3750136507589825e-05,
      "loss": 2.9546,
      "step": 11450
    },
    {
      "epoch": 1.2515015834880419,
      "grad_norm": 0.29129406809806824,
      "learning_rate": 4.3744676203996944e-05,
      "loss": 2.8592,
      "step": 11460
    },
    {
      "epoch": 1.252593644206618,
      "grad_norm": 0.2521384656429291,
      "learning_rate": 4.3739215900404064e-05,
      "loss": 2.9222,
      "step": 11470
    },
    {
      "epoch": 1.2536857049251937,
      "grad_norm": 0.12119099497795105,
      "learning_rate": 4.373375559681118e-05,
      "loss": 2.864,
      "step": 11480
    },
    {
      "epoch": 1.2547777656437697,
      "grad_norm": 0.12598149478435516,
      "learning_rate": 4.372829529321831e-05,
      "loss": 2.8654,
      "step": 11490
    },
    {
      "epoch": 1.2558698263623458,
      "grad_norm": 0.19665183126926422,
      "learning_rate": 4.372283498962543e-05,
      "loss": 2.8742,
      "step": 11500
    },
    {
      "epoch": 1.2569618870809216,
      "grad_norm": 0.19863803684711456,
      "learning_rate": 4.371737468603255e-05,
      "loss": 2.8632,
      "step": 11510
    },
    {
      "epoch": 1.2580539477994976,
      "grad_norm": 0.80402672290802,
      "learning_rate": 4.371191438243967e-05,
      "loss": 2.897,
      "step": 11520
    },
    {
      "epoch": 1.2591460085180737,
      "grad_norm": 0.1977103054523468,
      "learning_rate": 4.370645407884679e-05,
      "loss": 2.8573,
      "step": 11530
    },
    {
      "epoch": 1.2602380692366495,
      "grad_norm": 0.6542448401451111,
      "learning_rate": 4.3700993775253906e-05,
      "loss": 2.8588,
      "step": 11540
    },
    {
      "epoch": 1.2613301299552255,
      "grad_norm": 0.22624798119068146,
      "learning_rate": 4.3695533471661026e-05,
      "loss": 2.9366,
      "step": 11550
    },
    {
      "epoch": 1.2624221906738016,
      "grad_norm": 0.14803984761238098,
      "learning_rate": 4.3690073168068145e-05,
      "loss": 2.8724,
      "step": 11560
    },
    {
      "epoch": 1.2635142513923774,
      "grad_norm": 0.16170711815357208,
      "learning_rate": 4.3684612864475264e-05,
      "loss": 2.8596,
      "step": 11570
    },
    {
      "epoch": 1.2646063121109534,
      "grad_norm": 0.1328173726797104,
      "learning_rate": 4.3679152560882384e-05,
      "loss": 2.9333,
      "step": 11580
    },
    {
      "epoch": 1.2656983728295292,
      "grad_norm": 0.7159251570701599,
      "learning_rate": 4.36736922572895e-05,
      "loss": 2.8651,
      "step": 11590
    },
    {
      "epoch": 1.2667904335481053,
      "grad_norm": 0.495183527469635,
      "learning_rate": 4.366823195369662e-05,
      "loss": 2.8933,
      "step": 11600
    },
    {
      "epoch": 1.267882494266681,
      "grad_norm": 0.26194220781326294,
      "learning_rate": 4.366277165010374e-05,
      "loss": 2.8688,
      "step": 11610
    },
    {
      "epoch": 1.2689745549852571,
      "grad_norm": 0.11281658709049225,
      "learning_rate": 4.365731134651087e-05,
      "loss": 2.8653,
      "step": 11620
    },
    {
      "epoch": 1.2700666157038332,
      "grad_norm": 0.2670552432537079,
      "learning_rate": 4.365185104291799e-05,
      "loss": 2.8935,
      "step": 11630
    },
    {
      "epoch": 1.271158676422409,
      "grad_norm": 0.22316774725914001,
      "learning_rate": 4.364639073932511e-05,
      "loss": 2.8998,
      "step": 11640
    },
    {
      "epoch": 1.272250737140985,
      "grad_norm": 0.6256574392318726,
      "learning_rate": 4.364093043573223e-05,
      "loss": 2.8654,
      "step": 11650
    },
    {
      "epoch": 1.273342797859561,
      "grad_norm": 0.15588222444057465,
      "learning_rate": 4.363547013213935e-05,
      "loss": 2.8668,
      "step": 11660
    },
    {
      "epoch": 1.2744348585781369,
      "grad_norm": 0.5378559827804565,
      "learning_rate": 4.363000982854647e-05,
      "loss": 2.8612,
      "step": 11670
    },
    {
      "epoch": 1.275526919296713,
      "grad_norm": 0.4372357726097107,
      "learning_rate": 4.362454952495359e-05,
      "loss": 2.9236,
      "step": 11680
    },
    {
      "epoch": 1.276618980015289,
      "grad_norm": 0.13899950683116913,
      "learning_rate": 4.361908922136071e-05,
      "loss": 2.8533,
      "step": 11690
    },
    {
      "epoch": 1.2777110407338648,
      "grad_norm": 0.12605740129947662,
      "learning_rate": 4.361362891776783e-05,
      "loss": 2.8549,
      "step": 11700
    },
    {
      "epoch": 1.2788031014524408,
      "grad_norm": 0.15409326553344727,
      "learning_rate": 4.360816861417495e-05,
      "loss": 2.9041,
      "step": 11710
    },
    {
      "epoch": 1.2798951621710168,
      "grad_norm": 0.19138354063034058,
      "learning_rate": 4.360270831058207e-05,
      "loss": 2.8599,
      "step": 11720
    },
    {
      "epoch": 1.2809872228895927,
      "grad_norm": 0.1971602737903595,
      "learning_rate": 4.359724800698919e-05,
      "loss": 2.839,
      "step": 11730
    },
    {
      "epoch": 1.2820792836081687,
      "grad_norm": 0.7803690433502197,
      "learning_rate": 4.359178770339631e-05,
      "loss": 2.8856,
      "step": 11740
    },
    {
      "epoch": 1.2831713443267445,
      "grad_norm": 0.18686823546886444,
      "learning_rate": 4.3586327399803434e-05,
      "loss": 2.8454,
      "step": 11750
    },
    {
      "epoch": 1.2842634050453205,
      "grad_norm": 0.13629156351089478,
      "learning_rate": 4.3580867096210554e-05,
      "loss": 2.8643,
      "step": 11760
    },
    {
      "epoch": 1.2853554657638964,
      "grad_norm": 0.1643848866224289,
      "learning_rate": 4.357540679261767e-05,
      "loss": 2.847,
      "step": 11770
    },
    {
      "epoch": 1.2864475264824724,
      "grad_norm": 0.11748121678829193,
      "learning_rate": 4.356994648902479e-05,
      "loss": 2.8556,
      "step": 11780
    },
    {
      "epoch": 1.2875395872010484,
      "grad_norm": 0.16429656744003296,
      "learning_rate": 4.356448618543191e-05,
      "loss": 2.9104,
      "step": 11790
    },
    {
      "epoch": 1.2886316479196243,
      "grad_norm": 0.19669179618358612,
      "learning_rate": 4.355902588183903e-05,
      "loss": 2.8785,
      "step": 11800
    },
    {
      "epoch": 1.2897237086382003,
      "grad_norm": 0.16357414424419403,
      "learning_rate": 4.355356557824615e-05,
      "loss": 2.859,
      "step": 11810
    },
    {
      "epoch": 1.2908157693567763,
      "grad_norm": 0.0886525958776474,
      "learning_rate": 4.354810527465327e-05,
      "loss": 2.8611,
      "step": 11820
    },
    {
      "epoch": 1.2919078300753521,
      "grad_norm": 0.12206144630908966,
      "learning_rate": 4.354264497106039e-05,
      "loss": 2.8781,
      "step": 11830
    },
    {
      "epoch": 1.2929998907939282,
      "grad_norm": 0.11467606574296951,
      "learning_rate": 4.353718466746751e-05,
      "loss": 2.8522,
      "step": 11840
    },
    {
      "epoch": 1.2940919515125042,
      "grad_norm": 0.4673117399215698,
      "learning_rate": 4.3531724363874635e-05,
      "loss": 2.8497,
      "step": 11850
    },
    {
      "epoch": 1.29518401223108,
      "grad_norm": 0.17202125489711761,
      "learning_rate": 4.3526264060281754e-05,
      "loss": 2.878,
      "step": 11860
    },
    {
      "epoch": 1.296276072949656,
      "grad_norm": 0.20098884403705597,
      "learning_rate": 4.3520803756688874e-05,
      "loss": 2.9065,
      "step": 11870
    },
    {
      "epoch": 1.297368133668232,
      "grad_norm": 0.11008930206298828,
      "learning_rate": 4.3515343453096e-05,
      "loss": 2.8688,
      "step": 11880
    },
    {
      "epoch": 1.298460194386808,
      "grad_norm": 0.3917856812477112,
      "learning_rate": 4.350988314950312e-05,
      "loss": 2.8486,
      "step": 11890
    },
    {
      "epoch": 1.2995522551053837,
      "grad_norm": 0.19272921979427338,
      "learning_rate": 4.350442284591024e-05,
      "loss": 2.8727,
      "step": 11900
    },
    {
      "epoch": 1.3006443158239598,
      "grad_norm": 0.26143789291381836,
      "learning_rate": 4.349896254231736e-05,
      "loss": 2.8757,
      "step": 11910
    },
    {
      "epoch": 1.3017363765425358,
      "grad_norm": 0.21455465257167816,
      "learning_rate": 4.349350223872448e-05,
      "loss": 2.8841,
      "step": 11920
    },
    {
      "epoch": 1.3028284372611116,
      "grad_norm": 0.2155991941690445,
      "learning_rate": 4.34880419351316e-05,
      "loss": 2.852,
      "step": 11930
    },
    {
      "epoch": 1.3039204979796877,
      "grad_norm": 0.12968333065509796,
      "learning_rate": 4.3482581631538717e-05,
      "loss": 2.8454,
      "step": 11940
    },
    {
      "epoch": 1.3050125586982637,
      "grad_norm": 0.3446919620037079,
      "learning_rate": 4.3477121327945836e-05,
      "loss": 2.8785,
      "step": 11950
    },
    {
      "epoch": 1.3061046194168395,
      "grad_norm": 0.404920756816864,
      "learning_rate": 4.3471661024352955e-05,
      "loss": 2.8745,
      "step": 11960
    },
    {
      "epoch": 1.3071966801354156,
      "grad_norm": 0.18500414490699768,
      "learning_rate": 4.3466200720760075e-05,
      "loss": 2.8605,
      "step": 11970
    },
    {
      "epoch": 1.3082887408539916,
      "grad_norm": 0.19553518295288086,
      "learning_rate": 4.3460740417167194e-05,
      "loss": 2.8518,
      "step": 11980
    },
    {
      "epoch": 1.3093808015725674,
      "grad_norm": 0.28438377380371094,
      "learning_rate": 4.3455280113574314e-05,
      "loss": 2.8534,
      "step": 11990
    },
    {
      "epoch": 1.3104728622911435,
      "grad_norm": 0.2547468841075897,
      "learning_rate": 4.344981980998143e-05,
      "loss": 2.8603,
      "step": 12000
    },
    {
      "epoch": 1.3115649230097193,
      "grad_norm": 0.13754916191101074,
      "learning_rate": 4.344435950638856e-05,
      "loss": 2.8483,
      "step": 12010
    },
    {
      "epoch": 1.3126569837282953,
      "grad_norm": 0.3934912383556366,
      "learning_rate": 4.343889920279568e-05,
      "loss": 2.869,
      "step": 12020
    },
    {
      "epoch": 1.3137490444468711,
      "grad_norm": 0.13206785917282104,
      "learning_rate": 4.34334388992028e-05,
      "loss": 2.8352,
      "step": 12030
    },
    {
      "epoch": 1.3148411051654472,
      "grad_norm": 0.08388976752758026,
      "learning_rate": 4.342797859560992e-05,
      "loss": 2.8569,
      "step": 12040
    },
    {
      "epoch": 1.3159331658840232,
      "grad_norm": 0.24567300081253052,
      "learning_rate": 4.342251829201704e-05,
      "loss": 2.8391,
      "step": 12050
    },
    {
      "epoch": 1.317025226602599,
      "grad_norm": 0.13477039337158203,
      "learning_rate": 4.3417057988424156e-05,
      "loss": 2.8535,
      "step": 12060
    },
    {
      "epoch": 1.318117287321175,
      "grad_norm": 0.26740726828575134,
      "learning_rate": 4.3411597684831276e-05,
      "loss": 2.8613,
      "step": 12070
    },
    {
      "epoch": 1.319209348039751,
      "grad_norm": 0.14827659726142883,
      "learning_rate": 4.34061373812384e-05,
      "loss": 2.8709,
      "step": 12080
    },
    {
      "epoch": 1.320301408758327,
      "grad_norm": 0.12977436184883118,
      "learning_rate": 4.340067707764552e-05,
      "loss": 2.8607,
      "step": 12090
    },
    {
      "epoch": 1.321393469476903,
      "grad_norm": 0.10184350609779358,
      "learning_rate": 4.339521677405264e-05,
      "loss": 2.897,
      "step": 12100
    },
    {
      "epoch": 1.322485530195479,
      "grad_norm": 0.245711550116539,
      "learning_rate": 4.338975647045976e-05,
      "loss": 2.8895,
      "step": 12110
    },
    {
      "epoch": 1.3235775909140548,
      "grad_norm": 0.16923528909683228,
      "learning_rate": 4.338429616686688e-05,
      "loss": 2.8697,
      "step": 12120
    },
    {
      "epoch": 1.3246696516326308,
      "grad_norm": 0.1664131134748459,
      "learning_rate": 4.3378835863274e-05,
      "loss": 2.8629,
      "step": 12130
    },
    {
      "epoch": 1.3257617123512067,
      "grad_norm": 0.20571742951869965,
      "learning_rate": 4.3373375559681125e-05,
      "loss": 2.8735,
      "step": 12140
    },
    {
      "epoch": 1.3268537730697827,
      "grad_norm": 0.30569952726364136,
      "learning_rate": 4.3367915256088244e-05,
      "loss": 2.8464,
      "step": 12150
    },
    {
      "epoch": 1.3279458337883585,
      "grad_norm": 0.21764850616455078,
      "learning_rate": 4.3362454952495364e-05,
      "loss": 2.8615,
      "step": 12160
    },
    {
      "epoch": 1.3290378945069345,
      "grad_norm": 0.2051064372062683,
      "learning_rate": 4.335699464890248e-05,
      "loss": 2.8593,
      "step": 12170
    },
    {
      "epoch": 1.3301299552255106,
      "grad_norm": 0.23347102105617523,
      "learning_rate": 4.33515343453096e-05,
      "loss": 2.893,
      "step": 12180
    },
    {
      "epoch": 1.3312220159440864,
      "grad_norm": 0.1133984699845314,
      "learning_rate": 4.334607404171672e-05,
      "loss": 2.8758,
      "step": 12190
    },
    {
      "epoch": 1.3323140766626624,
      "grad_norm": 0.11918993294239044,
      "learning_rate": 4.334061373812384e-05,
      "loss": 2.8657,
      "step": 12200
    },
    {
      "epoch": 1.3334061373812385,
      "grad_norm": 0.09540309756994247,
      "learning_rate": 4.333515343453096e-05,
      "loss": 2.8877,
      "step": 12210
    },
    {
      "epoch": 1.3344981980998143,
      "grad_norm": 0.4462389647960663,
      "learning_rate": 4.332969313093808e-05,
      "loss": 2.8472,
      "step": 12220
    },
    {
      "epoch": 1.3355902588183903,
      "grad_norm": 0.27390074729919434,
      "learning_rate": 4.33242328273452e-05,
      "loss": 2.8779,
      "step": 12230
    },
    {
      "epoch": 1.3366823195369664,
      "grad_norm": 0.20881129801273346,
      "learning_rate": 4.331877252375232e-05,
      "loss": 2.9098,
      "step": 12240
    },
    {
      "epoch": 1.3377743802555422,
      "grad_norm": 0.209402397274971,
      "learning_rate": 4.331331222015944e-05,
      "loss": 2.8664,
      "step": 12250
    },
    {
      "epoch": 1.3388664409741182,
      "grad_norm": 0.33126261830329895,
      "learning_rate": 4.330785191656656e-05,
      "loss": 2.9014,
      "step": 12260
    },
    {
      "epoch": 1.339958501692694,
      "grad_norm": 0.13143455982208252,
      "learning_rate": 4.3302391612973684e-05,
      "loss": 2.8597,
      "step": 12270
    },
    {
      "epoch": 1.34105056241127,
      "grad_norm": 0.19099007546901703,
      "learning_rate": 4.3296931309380804e-05,
      "loss": 2.8892,
      "step": 12280
    },
    {
      "epoch": 1.342142623129846,
      "grad_norm": 0.17829370498657227,
      "learning_rate": 4.329147100578792e-05,
      "loss": 2.8477,
      "step": 12290
    },
    {
      "epoch": 1.343234683848422,
      "grad_norm": 0.10793313384056091,
      "learning_rate": 4.328601070219504e-05,
      "loss": 2.9204,
      "step": 12300
    },
    {
      "epoch": 1.344326744566998,
      "grad_norm": 0.1883673518896103,
      "learning_rate": 4.328055039860217e-05,
      "loss": 2.8736,
      "step": 12310
    },
    {
      "epoch": 1.3454188052855738,
      "grad_norm": 0.30604955554008484,
      "learning_rate": 4.327509009500929e-05,
      "loss": 2.893,
      "step": 12320
    },
    {
      "epoch": 1.3465108660041498,
      "grad_norm": 1.1232731342315674,
      "learning_rate": 4.326962979141641e-05,
      "loss": 2.8939,
      "step": 12330
    },
    {
      "epoch": 1.3476029267227259,
      "grad_norm": 0.20330603420734406,
      "learning_rate": 4.326416948782353e-05,
      "loss": 2.8552,
      "step": 12340
    },
    {
      "epoch": 1.3486949874413017,
      "grad_norm": 0.16755418479442596,
      "learning_rate": 4.3258709184230646e-05,
      "loss": 2.8388,
      "step": 12350
    },
    {
      "epoch": 1.3497870481598777,
      "grad_norm": 0.2184687852859497,
      "learning_rate": 4.3253248880637766e-05,
      "loss": 2.8858,
      "step": 12360
    },
    {
      "epoch": 1.3508791088784537,
      "grad_norm": 0.1564742922782898,
      "learning_rate": 4.3247788577044885e-05,
      "loss": 2.8826,
      "step": 12370
    },
    {
      "epoch": 1.3519711695970296,
      "grad_norm": 0.15602585673332214,
      "learning_rate": 4.3242328273452004e-05,
      "loss": 2.8583,
      "step": 12380
    },
    {
      "epoch": 1.3530632303156056,
      "grad_norm": 0.2516014873981476,
      "learning_rate": 4.3236867969859124e-05,
      "loss": 2.867,
      "step": 12390
    },
    {
      "epoch": 1.3541552910341814,
      "grad_norm": 0.13736513257026672,
      "learning_rate": 4.323140766626624e-05,
      "loss": 2.9089,
      "step": 12400
    },
    {
      "epoch": 1.3552473517527575,
      "grad_norm": 0.3210202753543854,
      "learning_rate": 4.322594736267337e-05,
      "loss": 2.8482,
      "step": 12410
    },
    {
      "epoch": 1.3563394124713333,
      "grad_norm": 0.2467673271894455,
      "learning_rate": 4.322048705908049e-05,
      "loss": 2.869,
      "step": 12420
    },
    {
      "epoch": 1.3574314731899093,
      "grad_norm": 0.2102079689502716,
      "learning_rate": 4.321502675548761e-05,
      "loss": 2.8789,
      "step": 12430
    },
    {
      "epoch": 1.3585235339084853,
      "grad_norm": 0.26224133372306824,
      "learning_rate": 4.320956645189473e-05,
      "loss": 2.862,
      "step": 12440
    },
    {
      "epoch": 1.3596155946270612,
      "grad_norm": 0.4536416530609131,
      "learning_rate": 4.320410614830185e-05,
      "loss": 2.9358,
      "step": 12450
    },
    {
      "epoch": 1.3607076553456372,
      "grad_norm": 0.16670387983322144,
      "learning_rate": 4.3198645844708966e-05,
      "loss": 2.857,
      "step": 12460
    },
    {
      "epoch": 1.3617997160642132,
      "grad_norm": 0.3732021152973175,
      "learning_rate": 4.3193185541116086e-05,
      "loss": 2.8441,
      "step": 12470
    },
    {
      "epoch": 1.362891776782789,
      "grad_norm": 0.10736484080553055,
      "learning_rate": 4.3187725237523205e-05,
      "loss": 2.8427,
      "step": 12480
    },
    {
      "epoch": 1.363983837501365,
      "grad_norm": 0.3979552984237671,
      "learning_rate": 4.3182264933930325e-05,
      "loss": 2.8913,
      "step": 12490
    },
    {
      "epoch": 1.3650758982199411,
      "grad_norm": 0.14544928073883057,
      "learning_rate": 4.3176804630337444e-05,
      "loss": 2.8777,
      "step": 12500
    },
    {
      "epoch": 1.366167958938517,
      "grad_norm": 0.14205053448677063,
      "learning_rate": 4.317134432674457e-05,
      "loss": 2.8426,
      "step": 12510
    },
    {
      "epoch": 1.367260019657093,
      "grad_norm": 0.8546369671821594,
      "learning_rate": 4.316588402315169e-05,
      "loss": 2.928,
      "step": 12520
    },
    {
      "epoch": 1.368352080375669,
      "grad_norm": 0.27623414993286133,
      "learning_rate": 4.316042371955881e-05,
      "loss": 2.8491,
      "step": 12530
    },
    {
      "epoch": 1.3694441410942448,
      "grad_norm": 0.22953593730926514,
      "learning_rate": 4.3154963415965935e-05,
      "loss": 2.8885,
      "step": 12540
    },
    {
      "epoch": 1.3705362018128209,
      "grad_norm": 0.13698330521583557,
      "learning_rate": 4.3149503112373055e-05,
      "loss": 2.8545,
      "step": 12550
    },
    {
      "epoch": 1.3716282625313967,
      "grad_norm": 0.08023342490196228,
      "learning_rate": 4.3144042808780174e-05,
      "loss": 2.8507,
      "step": 12560
    },
    {
      "epoch": 1.3727203232499727,
      "grad_norm": 0.18469595909118652,
      "learning_rate": 4.3138582505187294e-05,
      "loss": 2.8813,
      "step": 12570
    },
    {
      "epoch": 1.3738123839685485,
      "grad_norm": 0.3721877932548523,
      "learning_rate": 4.313312220159441e-05,
      "loss": 2.8972,
      "step": 12580
    },
    {
      "epoch": 1.3749044446871246,
      "grad_norm": 0.36663684248924255,
      "learning_rate": 4.312766189800153e-05,
      "loss": 2.848,
      "step": 12590
    },
    {
      "epoch": 1.3759965054057006,
      "grad_norm": 0.2662660479545593,
      "learning_rate": 4.312220159440865e-05,
      "loss": 2.8866,
      "step": 12600
    },
    {
      "epoch": 1.3770885661242764,
      "grad_norm": 0.23808614909648895,
      "learning_rate": 4.311674129081577e-05,
      "loss": 2.8576,
      "step": 12610
    },
    {
      "epoch": 1.3781806268428525,
      "grad_norm": 0.20294681191444397,
      "learning_rate": 4.311128098722289e-05,
      "loss": 2.8585,
      "step": 12620
    },
    {
      "epoch": 1.3792726875614285,
      "grad_norm": 0.1554308384656906,
      "learning_rate": 4.310582068363001e-05,
      "loss": 2.8841,
      "step": 12630
    },
    {
      "epoch": 1.3803647482800043,
      "grad_norm": 0.19534704089164734,
      "learning_rate": 4.310036038003713e-05,
      "loss": 2.8835,
      "step": 12640
    },
    {
      "epoch": 1.3814568089985804,
      "grad_norm": 0.24902082979679108,
      "learning_rate": 4.309490007644425e-05,
      "loss": 2.9633,
      "step": 12650
    },
    {
      "epoch": 1.3825488697171564,
      "grad_norm": 0.1473262757062912,
      "learning_rate": 4.308943977285137e-05,
      "loss": 2.8656,
      "step": 12660
    },
    {
      "epoch": 1.3836409304357322,
      "grad_norm": 0.20375727117061615,
      "learning_rate": 4.3083979469258494e-05,
      "loss": 2.9056,
      "step": 12670
    },
    {
      "epoch": 1.3847329911543083,
      "grad_norm": 0.4082622528076172,
      "learning_rate": 4.3078519165665614e-05,
      "loss": 2.8531,
      "step": 12680
    },
    {
      "epoch": 1.385825051872884,
      "grad_norm": 0.3548346757888794,
      "learning_rate": 4.307305886207273e-05,
      "loss": 2.9078,
      "step": 12690
    },
    {
      "epoch": 1.38691711259146,
      "grad_norm": 1.2334073781967163,
      "learning_rate": 4.306759855847985e-05,
      "loss": 2.9064,
      "step": 12700
    },
    {
      "epoch": 1.388009173310036,
      "grad_norm": 0.21620239317417145,
      "learning_rate": 4.306213825488697e-05,
      "loss": 2.848,
      "step": 12710
    },
    {
      "epoch": 1.389101234028612,
      "grad_norm": 0.10951297730207443,
      "learning_rate": 4.305667795129409e-05,
      "loss": 2.9059,
      "step": 12720
    },
    {
      "epoch": 1.390193294747188,
      "grad_norm": 0.27849888801574707,
      "learning_rate": 4.305121764770121e-05,
      "loss": 2.8661,
      "step": 12730
    },
    {
      "epoch": 1.3912853554657638,
      "grad_norm": 0.263071209192276,
      "learning_rate": 4.304575734410834e-05,
      "loss": 2.9163,
      "step": 12740
    },
    {
      "epoch": 1.3923774161843399,
      "grad_norm": 0.1634640246629715,
      "learning_rate": 4.3040297040515456e-05,
      "loss": 2.8464,
      "step": 12750
    },
    {
      "epoch": 1.393469476902916,
      "grad_norm": 0.2897883355617523,
      "learning_rate": 4.3034836736922576e-05,
      "loss": 2.852,
      "step": 12760
    },
    {
      "epoch": 1.3945615376214917,
      "grad_norm": 0.2953348755836487,
      "learning_rate": 4.3029376433329695e-05,
      "loss": 2.8981,
      "step": 12770
    },
    {
      "epoch": 1.3956535983400677,
      "grad_norm": 0.17058080434799194,
      "learning_rate": 4.3023916129736815e-05,
      "loss": 2.9118,
      "step": 12780
    },
    {
      "epoch": 1.3967456590586438,
      "grad_norm": 0.2784838378429413,
      "learning_rate": 4.3018455826143934e-05,
      "loss": 2.8492,
      "step": 12790
    },
    {
      "epoch": 1.3978377197772196,
      "grad_norm": 0.12246913462877274,
      "learning_rate": 4.301299552255106e-05,
      "loss": 2.9207,
      "step": 12800
    },
    {
      "epoch": 1.3989297804957956,
      "grad_norm": 0.14340166747570038,
      "learning_rate": 4.300753521895818e-05,
      "loss": 2.8376,
      "step": 12810
    },
    {
      "epoch": 1.4000218412143715,
      "grad_norm": 0.13945889472961426,
      "learning_rate": 4.30020749153653e-05,
      "loss": 2.8576,
      "step": 12820
    },
    {
      "epoch": 1.4011139019329475,
      "grad_norm": 0.24648091197013855,
      "learning_rate": 4.299661461177242e-05,
      "loss": 2.8413,
      "step": 12830
    },
    {
      "epoch": 1.4022059626515233,
      "grad_norm": 0.14480289816856384,
      "learning_rate": 4.299115430817954e-05,
      "loss": 2.8608,
      "step": 12840
    },
    {
      "epoch": 1.4032980233700993,
      "grad_norm": 0.2407304048538208,
      "learning_rate": 4.298569400458666e-05,
      "loss": 2.8569,
      "step": 12850
    },
    {
      "epoch": 1.4043900840886754,
      "grad_norm": 0.13140320777893066,
      "learning_rate": 4.298023370099378e-05,
      "loss": 2.8351,
      "step": 12860
    },
    {
      "epoch": 1.4054821448072512,
      "grad_norm": 0.11348933726549149,
      "learning_rate": 4.2974773397400896e-05,
      "loss": 2.8532,
      "step": 12870
    },
    {
      "epoch": 1.4065742055258272,
      "grad_norm": 0.15831652283668518,
      "learning_rate": 4.2969313093808016e-05,
      "loss": 2.8794,
      "step": 12880
    },
    {
      "epoch": 1.4076662662444033,
      "grad_norm": 0.4280388653278351,
      "learning_rate": 4.2963852790215135e-05,
      "loss": 2.9061,
      "step": 12890
    },
    {
      "epoch": 1.408758326962979,
      "grad_norm": 0.20815511047840118,
      "learning_rate": 4.2958392486622254e-05,
      "loss": 2.8954,
      "step": 12900
    },
    {
      "epoch": 1.4098503876815551,
      "grad_norm": 0.15741941332817078,
      "learning_rate": 4.2952932183029374e-05,
      "loss": 2.8746,
      "step": 12910
    },
    {
      "epoch": 1.4109424484001312,
      "grad_norm": 0.12351874262094498,
      "learning_rate": 4.294747187943649e-05,
      "loss": 2.9119,
      "step": 12920
    },
    {
      "epoch": 1.412034509118707,
      "grad_norm": 0.12560446560382843,
      "learning_rate": 4.294201157584362e-05,
      "loss": 2.8643,
      "step": 12930
    },
    {
      "epoch": 1.413126569837283,
      "grad_norm": 0.1529836654663086,
      "learning_rate": 4.293655127225074e-05,
      "loss": 2.8443,
      "step": 12940
    },
    {
      "epoch": 1.4142186305558588,
      "grad_norm": 0.1864757388830185,
      "learning_rate": 4.293109096865786e-05,
      "loss": 2.8565,
      "step": 12950
    },
    {
      "epoch": 1.4153106912744349,
      "grad_norm": 0.11455193907022476,
      "learning_rate": 4.292563066506498e-05,
      "loss": 2.8446,
      "step": 12960
    },
    {
      "epoch": 1.4164027519930107,
      "grad_norm": 0.13303406536579132,
      "learning_rate": 4.2920170361472104e-05,
      "loss": 2.8462,
      "step": 12970
    },
    {
      "epoch": 1.4174948127115867,
      "grad_norm": 0.7646567225456238,
      "learning_rate": 4.291471005787922e-05,
      "loss": 2.886,
      "step": 12980
    },
    {
      "epoch": 1.4185868734301628,
      "grad_norm": 0.22669686377048492,
      "learning_rate": 4.290924975428634e-05,
      "loss": 2.8768,
      "step": 12990
    },
    {
      "epoch": 1.4196789341487386,
      "grad_norm": 0.11622533202171326,
      "learning_rate": 4.290378945069346e-05,
      "loss": 2.87,
      "step": 13000
    },
    {
      "epoch": 1.4207709948673146,
      "grad_norm": 0.3189658522605896,
      "learning_rate": 4.289832914710058e-05,
      "loss": 2.8573,
      "step": 13010
    },
    {
      "epoch": 1.4218630555858907,
      "grad_norm": 0.15009145438671112,
      "learning_rate": 4.28928688435077e-05,
      "loss": 2.8528,
      "step": 13020
    },
    {
      "epoch": 1.4229551163044665,
      "grad_norm": 0.19772085547447205,
      "learning_rate": 4.288740853991482e-05,
      "loss": 2.9192,
      "step": 13030
    },
    {
      "epoch": 1.4240471770230425,
      "grad_norm": 0.3114478886127472,
      "learning_rate": 4.288194823632194e-05,
      "loss": 2.87,
      "step": 13040
    },
    {
      "epoch": 1.4251392377416185,
      "grad_norm": 0.10881113260984421,
      "learning_rate": 4.287648793272906e-05,
      "loss": 2.8519,
      "step": 13050
    },
    {
      "epoch": 1.4262312984601944,
      "grad_norm": 0.2695281505584717,
      "learning_rate": 4.2871027629136185e-05,
      "loss": 2.8925,
      "step": 13060
    },
    {
      "epoch": 1.4273233591787704,
      "grad_norm": 0.12668339908123016,
      "learning_rate": 4.2865567325543305e-05,
      "loss": 2.9018,
      "step": 13070
    },
    {
      "epoch": 1.4284154198973462,
      "grad_norm": 0.24643181264400482,
      "learning_rate": 4.2860107021950424e-05,
      "loss": 2.8777,
      "step": 13080
    },
    {
      "epoch": 1.4295074806159223,
      "grad_norm": 0.5999863147735596,
      "learning_rate": 4.2854646718357544e-05,
      "loss": 2.8743,
      "step": 13090
    },
    {
      "epoch": 1.430599541334498,
      "grad_norm": 0.3445606827735901,
      "learning_rate": 4.284918641476466e-05,
      "loss": 2.8756,
      "step": 13100
    },
    {
      "epoch": 1.431691602053074,
      "grad_norm": 0.31544119119644165,
      "learning_rate": 4.284372611117178e-05,
      "loss": 2.9003,
      "step": 13110
    },
    {
      "epoch": 1.4327836627716501,
      "grad_norm": 0.15641078352928162,
      "learning_rate": 4.28382658075789e-05,
      "loss": 2.8358,
      "step": 13120
    },
    {
      "epoch": 1.433875723490226,
      "grad_norm": 0.10668535530567169,
      "learning_rate": 4.283280550398602e-05,
      "loss": 2.8674,
      "step": 13130
    },
    {
      "epoch": 1.434967784208802,
      "grad_norm": 0.48885056376457214,
      "learning_rate": 4.282734520039314e-05,
      "loss": 2.8439,
      "step": 13140
    },
    {
      "epoch": 1.436059844927378,
      "grad_norm": 0.3348633348941803,
      "learning_rate": 4.282188489680026e-05,
      "loss": 2.8338,
      "step": 13150
    },
    {
      "epoch": 1.4371519056459539,
      "grad_norm": 0.2696739435195923,
      "learning_rate": 4.281642459320738e-05,
      "loss": 2.9189,
      "step": 13160
    },
    {
      "epoch": 1.43824396636453,
      "grad_norm": 0.14164604246616364,
      "learning_rate": 4.2810964289614506e-05,
      "loss": 2.8741,
      "step": 13170
    },
    {
      "epoch": 1.439336027083106,
      "grad_norm": 0.12811516225337982,
      "learning_rate": 4.2805503986021625e-05,
      "loss": 2.8506,
      "step": 13180
    },
    {
      "epoch": 1.4404280878016817,
      "grad_norm": 0.29771625995635986,
      "learning_rate": 4.2800043682428744e-05,
      "loss": 2.8501,
      "step": 13190
    },
    {
      "epoch": 1.4415201485202578,
      "grad_norm": 0.20147135853767395,
      "learning_rate": 4.279458337883587e-05,
      "loss": 2.9222,
      "step": 13200
    },
    {
      "epoch": 1.4426122092388336,
      "grad_norm": 0.16027428209781647,
      "learning_rate": 4.278912307524299e-05,
      "loss": 2.8635,
      "step": 13210
    },
    {
      "epoch": 1.4437042699574096,
      "grad_norm": 0.11926952749490738,
      "learning_rate": 4.278366277165011e-05,
      "loss": 2.8736,
      "step": 13220
    },
    {
      "epoch": 1.4447963306759855,
      "grad_norm": 0.17764246463775635,
      "learning_rate": 4.277820246805723e-05,
      "loss": 2.8479,
      "step": 13230
    },
    {
      "epoch": 1.4458883913945615,
      "grad_norm": 0.11348429322242737,
      "learning_rate": 4.277274216446435e-05,
      "loss": 2.8554,
      "step": 13240
    },
    {
      "epoch": 1.4469804521131375,
      "grad_norm": 0.8533483743667603,
      "learning_rate": 4.276728186087147e-05,
      "loss": 2.8866,
      "step": 13250
    },
    {
      "epoch": 1.4480725128317133,
      "grad_norm": 0.21847862005233765,
      "learning_rate": 4.276182155727859e-05,
      "loss": 2.8505,
      "step": 13260
    },
    {
      "epoch": 1.4491645735502894,
      "grad_norm": 0.1248939111828804,
      "learning_rate": 4.2756361253685706e-05,
      "loss": 2.8532,
      "step": 13270
    },
    {
      "epoch": 1.4502566342688654,
      "grad_norm": 0.502661943435669,
      "learning_rate": 4.2750900950092826e-05,
      "loss": 2.8772,
      "step": 13280
    },
    {
      "epoch": 1.4513486949874412,
      "grad_norm": 0.44852688908576965,
      "learning_rate": 4.2745440646499945e-05,
      "loss": 2.9418,
      "step": 13290
    },
    {
      "epoch": 1.4524407557060173,
      "grad_norm": 0.1725406050682068,
      "learning_rate": 4.2739980342907065e-05,
      "loss": 2.8797,
      "step": 13300
    },
    {
      "epoch": 1.4535328164245933,
      "grad_norm": 0.18612553179264069,
      "learning_rate": 4.2734520039314184e-05,
      "loss": 2.8917,
      "step": 13310
    },
    {
      "epoch": 1.4546248771431691,
      "grad_norm": 0.3914656937122345,
      "learning_rate": 4.272905973572131e-05,
      "loss": 2.9251,
      "step": 13320
    },
    {
      "epoch": 1.4557169378617452,
      "grad_norm": 0.16062599420547485,
      "learning_rate": 4.272359943212843e-05,
      "loss": 2.8581,
      "step": 13330
    },
    {
      "epoch": 1.4568089985803212,
      "grad_norm": 0.33666732907295227,
      "learning_rate": 4.271813912853555e-05,
      "loss": 2.8838,
      "step": 13340
    },
    {
      "epoch": 1.457901059298897,
      "grad_norm": 0.17945539951324463,
      "learning_rate": 4.271267882494267e-05,
      "loss": 2.8666,
      "step": 13350
    },
    {
      "epoch": 1.4589931200174728,
      "grad_norm": 0.2596416175365448,
      "learning_rate": 4.270721852134979e-05,
      "loss": 2.8871,
      "step": 13360
    },
    {
      "epoch": 1.4600851807360489,
      "grad_norm": 0.4579014480113983,
      "learning_rate": 4.270175821775691e-05,
      "loss": 2.8569,
      "step": 13370
    },
    {
      "epoch": 1.461177241454625,
      "grad_norm": 0.19701288640499115,
      "learning_rate": 4.269629791416403e-05,
      "loss": 2.8796,
      "step": 13380
    },
    {
      "epoch": 1.4622693021732007,
      "grad_norm": 0.8101076483726501,
      "learning_rate": 4.2690837610571146e-05,
      "loss": 2.8968,
      "step": 13390
    },
    {
      "epoch": 1.4633613628917768,
      "grad_norm": 1.1097575426101685,
      "learning_rate": 4.268537730697827e-05,
      "loss": 2.9163,
      "step": 13400
    },
    {
      "epoch": 1.4644534236103528,
      "grad_norm": 0.15963995456695557,
      "learning_rate": 4.267991700338539e-05,
      "loss": 2.8565,
      "step": 13410
    },
    {
      "epoch": 1.4655454843289286,
      "grad_norm": 0.32941287755966187,
      "learning_rate": 4.267445669979251e-05,
      "loss": 2.8816,
      "step": 13420
    },
    {
      "epoch": 1.4666375450475047,
      "grad_norm": 0.14482125639915466,
      "learning_rate": 4.266899639619963e-05,
      "loss": 2.8778,
      "step": 13430
    },
    {
      "epoch": 1.4677296057660807,
      "grad_norm": 0.25875842571258545,
      "learning_rate": 4.266353609260675e-05,
      "loss": 2.8761,
      "step": 13440
    },
    {
      "epoch": 1.4688216664846565,
      "grad_norm": 0.18683384358882904,
      "learning_rate": 4.2658075789013876e-05,
      "loss": 2.8607,
      "step": 13450
    },
    {
      "epoch": 1.4699137272032325,
      "grad_norm": 0.22980467975139618,
      "learning_rate": 4.2652615485420996e-05,
      "loss": 2.8598,
      "step": 13460
    },
    {
      "epoch": 1.4710057879218086,
      "grad_norm": 0.2792733311653137,
      "learning_rate": 4.2647155181828115e-05,
      "loss": 2.8616,
      "step": 13470
    },
    {
      "epoch": 1.4720978486403844,
      "grad_norm": 0.19572143256664276,
      "learning_rate": 4.2641694878235234e-05,
      "loss": 2.8891,
      "step": 13480
    },
    {
      "epoch": 1.4731899093589604,
      "grad_norm": 0.09857015311717987,
      "learning_rate": 4.2636234574642354e-05,
      "loss": 2.8686,
      "step": 13490
    },
    {
      "epoch": 1.4742819700775363,
      "grad_norm": 0.23479825258255005,
      "learning_rate": 4.263077427104947e-05,
      "loss": 2.9137,
      "step": 13500
    },
    {
      "epoch": 1.4753740307961123,
      "grad_norm": 0.11246318370103836,
      "learning_rate": 4.262531396745659e-05,
      "loss": 2.8473,
      "step": 13510
    },
    {
      "epoch": 1.476466091514688,
      "grad_norm": 0.2508476972579956,
      "learning_rate": 4.261985366386371e-05,
      "loss": 2.8569,
      "step": 13520
    },
    {
      "epoch": 1.4775581522332641,
      "grad_norm": 0.315745085477829,
      "learning_rate": 4.261439336027083e-05,
      "loss": 2.8505,
      "step": 13530
    },
    {
      "epoch": 1.4786502129518402,
      "grad_norm": 0.5787069797515869,
      "learning_rate": 4.260893305667795e-05,
      "loss": 2.8666,
      "step": 13540
    },
    {
      "epoch": 1.479742273670416,
      "grad_norm": 0.12875814735889435,
      "learning_rate": 4.260347275308507e-05,
      "loss": 2.8428,
      "step": 13550
    },
    {
      "epoch": 1.480834334388992,
      "grad_norm": 0.12248716503381729,
      "learning_rate": 4.259801244949219e-05,
      "loss": 2.8797,
      "step": 13560
    },
    {
      "epoch": 1.481926395107568,
      "grad_norm": 0.23397918045520782,
      "learning_rate": 4.259255214589931e-05,
      "loss": 2.8464,
      "step": 13570
    },
    {
      "epoch": 1.4830184558261439,
      "grad_norm": 0.3770757019519806,
      "learning_rate": 4.2587091842306435e-05,
      "loss": 2.8878,
      "step": 13580
    },
    {
      "epoch": 1.48411051654472,
      "grad_norm": 0.2323850691318512,
      "learning_rate": 4.2581631538713555e-05,
      "loss": 2.84,
      "step": 13590
    },
    {
      "epoch": 1.485202577263296,
      "grad_norm": 0.1656835526227951,
      "learning_rate": 4.2576171235120674e-05,
      "loss": 2.8607,
      "step": 13600
    },
    {
      "epoch": 1.4862946379818718,
      "grad_norm": 0.17030347883701324,
      "learning_rate": 4.2570710931527793e-05,
      "loss": 2.8774,
      "step": 13610
    },
    {
      "epoch": 1.4873866987004478,
      "grad_norm": 0.1853225976228714,
      "learning_rate": 4.256525062793491e-05,
      "loss": 2.8792,
      "step": 13620
    },
    {
      "epoch": 1.4884787594190236,
      "grad_norm": 0.13504180312156677,
      "learning_rate": 4.255979032434204e-05,
      "loss": 2.8625,
      "step": 13630
    },
    {
      "epoch": 1.4895708201375997,
      "grad_norm": 0.1678691953420639,
      "learning_rate": 4.255433002074916e-05,
      "loss": 2.8465,
      "step": 13640
    },
    {
      "epoch": 1.4906628808561755,
      "grad_norm": 0.12603148818016052,
      "learning_rate": 4.254886971715628e-05,
      "loss": 2.8908,
      "step": 13650
    },
    {
      "epoch": 1.4917549415747515,
      "grad_norm": 0.15920083224773407,
      "learning_rate": 4.25434094135634e-05,
      "loss": 2.8689,
      "step": 13660
    },
    {
      "epoch": 1.4928470022933276,
      "grad_norm": 0.10109120607376099,
      "learning_rate": 4.253794910997052e-05,
      "loss": 2.8428,
      "step": 13670
    },
    {
      "epoch": 1.4939390630119034,
      "grad_norm": 0.10511395335197449,
      "learning_rate": 4.2532488806377636e-05,
      "loss": 2.8662,
      "step": 13680
    },
    {
      "epoch": 1.4950311237304794,
      "grad_norm": 0.22359329462051392,
      "learning_rate": 4.2527028502784756e-05,
      "loss": 2.9122,
      "step": 13690
    },
    {
      "epoch": 1.4961231844490555,
      "grad_norm": 0.12897361814975739,
      "learning_rate": 4.2521568199191875e-05,
      "loss": 2.8713,
      "step": 13700
    },
    {
      "epoch": 1.4972152451676313,
      "grad_norm": 0.20845380425453186,
      "learning_rate": 4.2516107895599e-05,
      "loss": 2.8635,
      "step": 13710
    },
    {
      "epoch": 1.4983073058862073,
      "grad_norm": 0.18658119440078735,
      "learning_rate": 4.251064759200612e-05,
      "loss": 2.8746,
      "step": 13720
    },
    {
      "epoch": 1.4993993666047833,
      "grad_norm": 0.16249820590019226,
      "learning_rate": 4.250518728841324e-05,
      "loss": 2.914,
      "step": 13730
    },
    {
      "epoch": 1.5004914273233592,
      "grad_norm": 0.22608843445777893,
      "learning_rate": 4.249972698482036e-05,
      "loss": 2.875,
      "step": 13740
    },
    {
      "epoch": 1.501583488041935,
      "grad_norm": 0.16957394778728485,
      "learning_rate": 4.249426668122748e-05,
      "loss": 2.8542,
      "step": 13750
    },
    {
      "epoch": 1.5026755487605112,
      "grad_norm": 0.19016487896442413,
      "learning_rate": 4.24888063776346e-05,
      "loss": 2.8458,
      "step": 13760
    },
    {
      "epoch": 1.503767609479087,
      "grad_norm": 0.1662558615207672,
      "learning_rate": 4.248334607404172e-05,
      "loss": 2.8464,
      "step": 13770
    },
    {
      "epoch": 1.5048596701976629,
      "grad_norm": 0.10487768799066544,
      "learning_rate": 4.247788577044884e-05,
      "loss": 2.8816,
      "step": 13780
    },
    {
      "epoch": 1.505951730916239,
      "grad_norm": 0.17425945401191711,
      "learning_rate": 4.2472425466855956e-05,
      "loss": 2.8651,
      "step": 13790
    },
    {
      "epoch": 1.507043791634815,
      "grad_norm": 0.5418670773506165,
      "learning_rate": 4.2466965163263076e-05,
      "loss": 2.8671,
      "step": 13800
    },
    {
      "epoch": 1.5081358523533908,
      "grad_norm": 0.15634551644325256,
      "learning_rate": 4.2461504859670195e-05,
      "loss": 2.8613,
      "step": 13810
    },
    {
      "epoch": 1.5092279130719668,
      "grad_norm": 0.2628833055496216,
      "learning_rate": 4.2456044556077315e-05,
      "loss": 2.8992,
      "step": 13820
    },
    {
      "epoch": 1.5103199737905428,
      "grad_norm": 0.22209930419921875,
      "learning_rate": 4.245058425248444e-05,
      "loss": 2.899,
      "step": 13830
    },
    {
      "epoch": 1.5114120345091187,
      "grad_norm": 0.21782205998897552,
      "learning_rate": 4.244512394889156e-05,
      "loss": 2.8453,
      "step": 13840
    },
    {
      "epoch": 1.5125040952276947,
      "grad_norm": 0.2893824875354767,
      "learning_rate": 4.243966364529868e-05,
      "loss": 2.8783,
      "step": 13850
    },
    {
      "epoch": 1.5135961559462707,
      "grad_norm": 0.30879563093185425,
      "learning_rate": 4.2434203341705806e-05,
      "loss": 2.8663,
      "step": 13860
    },
    {
      "epoch": 1.5146882166648465,
      "grad_norm": 0.1958855539560318,
      "learning_rate": 4.2428743038112925e-05,
      "loss": 2.8514,
      "step": 13870
    },
    {
      "epoch": 1.5157802773834224,
      "grad_norm": 0.12481363862752914,
      "learning_rate": 4.2423282734520045e-05,
      "loss": 2.9042,
      "step": 13880
    },
    {
      "epoch": 1.5168723381019986,
      "grad_norm": 0.3587145507335663,
      "learning_rate": 4.2417822430927164e-05,
      "loss": 2.9338,
      "step": 13890
    },
    {
      "epoch": 1.5179643988205744,
      "grad_norm": 0.8497103452682495,
      "learning_rate": 4.2412362127334283e-05,
      "loss": 2.8886,
      "step": 13900
    },
    {
      "epoch": 1.5190564595391503,
      "grad_norm": 0.14908647537231445,
      "learning_rate": 4.24069018237414e-05,
      "loss": 2.8551,
      "step": 13910
    },
    {
      "epoch": 1.5201485202577263,
      "grad_norm": 0.19325405359268188,
      "learning_rate": 4.240144152014852e-05,
      "loss": 2.8364,
      "step": 13920
    },
    {
      "epoch": 1.5212405809763023,
      "grad_norm": 0.18681494891643524,
      "learning_rate": 4.239598121655564e-05,
      "loss": 2.8811,
      "step": 13930
    },
    {
      "epoch": 1.5223326416948781,
      "grad_norm": 0.19516314566135406,
      "learning_rate": 4.239052091296276e-05,
      "loss": 2.8684,
      "step": 13940
    },
    {
      "epoch": 1.5234247024134542,
      "grad_norm": 1.0302577018737793,
      "learning_rate": 4.238506060936988e-05,
      "loss": 2.8651,
      "step": 13950
    },
    {
      "epoch": 1.5245167631320302,
      "grad_norm": 0.19349108636379242,
      "learning_rate": 4.2379600305777e-05,
      "loss": 2.8536,
      "step": 13960
    },
    {
      "epoch": 1.525608823850606,
      "grad_norm": 0.22764915227890015,
      "learning_rate": 4.2374140002184126e-05,
      "loss": 2.8847,
      "step": 13970
    },
    {
      "epoch": 1.526700884569182,
      "grad_norm": 0.11575009673833847,
      "learning_rate": 4.2368679698591246e-05,
      "loss": 2.8441,
      "step": 13980
    },
    {
      "epoch": 1.527792945287758,
      "grad_norm": 0.11152192205190659,
      "learning_rate": 4.2363219394998365e-05,
      "loss": 2.8613,
      "step": 13990
    },
    {
      "epoch": 1.528885006006334,
      "grad_norm": 0.16979344189167023,
      "learning_rate": 4.2357759091405484e-05,
      "loss": 2.8416,
      "step": 14000
    },
    {
      "epoch": 1.5299770667249097,
      "grad_norm": 0.4249345660209656,
      "learning_rate": 4.2352298787812604e-05,
      "loss": 2.9479,
      "step": 14010
    },
    {
      "epoch": 1.531069127443486,
      "grad_norm": 0.21701492369174957,
      "learning_rate": 4.234683848421972e-05,
      "loss": 2.8524,
      "step": 14020
    },
    {
      "epoch": 1.5321611881620618,
      "grad_norm": 0.222224622964859,
      "learning_rate": 4.234137818062684e-05,
      "loss": 2.8867,
      "step": 14030
    },
    {
      "epoch": 1.5332532488806376,
      "grad_norm": 0.25220492482185364,
      "learning_rate": 4.233591787703396e-05,
      "loss": 2.9006,
      "step": 14040
    },
    {
      "epoch": 1.5343453095992137,
      "grad_norm": 0.11601453274488449,
      "learning_rate": 4.233045757344108e-05,
      "loss": 2.8294,
      "step": 14050
    },
    {
      "epoch": 1.5354373703177897,
      "grad_norm": 0.18912017345428467,
      "learning_rate": 4.232499726984821e-05,
      "loss": 2.9249,
      "step": 14060
    },
    {
      "epoch": 1.5365294310363655,
      "grad_norm": 0.26220473647117615,
      "learning_rate": 4.231953696625533e-05,
      "loss": 2.8764,
      "step": 14070
    },
    {
      "epoch": 1.5376214917549416,
      "grad_norm": 0.25066789984703064,
      "learning_rate": 4.2314076662662446e-05,
      "loss": 2.8752,
      "step": 14080
    },
    {
      "epoch": 1.5387135524735176,
      "grad_norm": 0.21280096471309662,
      "learning_rate": 4.2308616359069566e-05,
      "loss": 2.851,
      "step": 14090
    },
    {
      "epoch": 1.5398056131920934,
      "grad_norm": 0.4736572206020355,
      "learning_rate": 4.230315605547669e-05,
      "loss": 2.8594,
      "step": 14100
    },
    {
      "epoch": 1.5408976739106695,
      "grad_norm": 0.13209812343120575,
      "learning_rate": 4.229769575188381e-05,
      "loss": 2.9068,
      "step": 14110
    },
    {
      "epoch": 1.5419897346292455,
      "grad_norm": 0.19478893280029297,
      "learning_rate": 4.229223544829093e-05,
      "loss": 2.877,
      "step": 14120
    },
    {
      "epoch": 1.5430817953478213,
      "grad_norm": 0.2464655190706253,
      "learning_rate": 4.228677514469805e-05,
      "loss": 2.871,
      "step": 14130
    },
    {
      "epoch": 1.5441738560663973,
      "grad_norm": 0.14343716204166412,
      "learning_rate": 4.228131484110517e-05,
      "loss": 2.8285,
      "step": 14140
    },
    {
      "epoch": 1.5452659167849734,
      "grad_norm": 0.18046411871910095,
      "learning_rate": 4.227585453751229e-05,
      "loss": 2.8559,
      "step": 14150
    },
    {
      "epoch": 1.5463579775035492,
      "grad_norm": 0.11044663190841675,
      "learning_rate": 4.227039423391941e-05,
      "loss": 2.8591,
      "step": 14160
    },
    {
      "epoch": 1.547450038222125,
      "grad_norm": 0.24448086321353912,
      "learning_rate": 4.226493393032653e-05,
      "loss": 2.8828,
      "step": 14170
    },
    {
      "epoch": 1.548542098940701,
      "grad_norm": 0.11799697577953339,
      "learning_rate": 4.225947362673365e-05,
      "loss": 2.8652,
      "step": 14180
    },
    {
      "epoch": 1.549634159659277,
      "grad_norm": 0.09203232079744339,
      "learning_rate": 4.225401332314077e-05,
      "loss": 2.8824,
      "step": 14190
    },
    {
      "epoch": 1.550726220377853,
      "grad_norm": 0.5366477370262146,
      "learning_rate": 4.2248553019547886e-05,
      "loss": 2.8456,
      "step": 14200
    },
    {
      "epoch": 1.551818281096429,
      "grad_norm": 0.16253112256526947,
      "learning_rate": 4.2243092715955006e-05,
      "loss": 2.8381,
      "step": 14210
    },
    {
      "epoch": 1.552910341815005,
      "grad_norm": 0.819837749004364,
      "learning_rate": 4.2237632412362125e-05,
      "loss": 2.911,
      "step": 14220
    },
    {
      "epoch": 1.5540024025335808,
      "grad_norm": 0.6385115385055542,
      "learning_rate": 4.223217210876925e-05,
      "loss": 2.8589,
      "step": 14230
    },
    {
      "epoch": 1.5550944632521568,
      "grad_norm": 0.2285064458847046,
      "learning_rate": 4.222671180517637e-05,
      "loss": 2.8835,
      "step": 14240
    },
    {
      "epoch": 1.5561865239707329,
      "grad_norm": 0.35909488797187805,
      "learning_rate": 4.222125150158349e-05,
      "loss": 2.8868,
      "step": 14250
    },
    {
      "epoch": 1.5572785846893087,
      "grad_norm": 0.2659146785736084,
      "learning_rate": 4.221579119799061e-05,
      "loss": 2.9151,
      "step": 14260
    },
    {
      "epoch": 1.5583706454078847,
      "grad_norm": 0.1304575502872467,
      "learning_rate": 4.221033089439773e-05,
      "loss": 2.896,
      "step": 14270
    },
    {
      "epoch": 1.5594627061264608,
      "grad_norm": 0.18748201429843903,
      "learning_rate": 4.220487059080485e-05,
      "loss": 2.8548,
      "step": 14280
    },
    {
      "epoch": 1.5605547668450366,
      "grad_norm": 0.10434875637292862,
      "learning_rate": 4.2199410287211974e-05,
      "loss": 2.8972,
      "step": 14290
    },
    {
      "epoch": 1.5616468275636124,
      "grad_norm": 0.21660661697387695,
      "learning_rate": 4.2193949983619094e-05,
      "loss": 2.8674,
      "step": 14300
    },
    {
      "epoch": 1.5627388882821887,
      "grad_norm": 0.17227999866008759,
      "learning_rate": 4.218848968002621e-05,
      "loss": 2.8469,
      "step": 14310
    },
    {
      "epoch": 1.5638309490007645,
      "grad_norm": 0.1994193196296692,
      "learning_rate": 4.218302937643333e-05,
      "loss": 2.8495,
      "step": 14320
    },
    {
      "epoch": 1.5649230097193403,
      "grad_norm": 0.336355596780777,
      "learning_rate": 4.217756907284045e-05,
      "loss": 2.925,
      "step": 14330
    },
    {
      "epoch": 1.5660150704379163,
      "grad_norm": 0.2567495107650757,
      "learning_rate": 4.217210876924757e-05,
      "loss": 2.8627,
      "step": 14340
    },
    {
      "epoch": 1.5671071311564924,
      "grad_norm": 0.17799291014671326,
      "learning_rate": 4.216664846565469e-05,
      "loss": 2.8591,
      "step": 14350
    },
    {
      "epoch": 1.5681991918750682,
      "grad_norm": 0.12348265945911407,
      "learning_rate": 4.216118816206182e-05,
      "loss": 2.854,
      "step": 14360
    },
    {
      "epoch": 1.5692912525936442,
      "grad_norm": 0.17760184407234192,
      "learning_rate": 4.2155727858468936e-05,
      "loss": 2.8365,
      "step": 14370
    },
    {
      "epoch": 1.5703833133122203,
      "grad_norm": 0.21149297058582306,
      "learning_rate": 4.2150267554876056e-05,
      "loss": 2.8564,
      "step": 14380
    },
    {
      "epoch": 1.571475374030796,
      "grad_norm": 0.24663305282592773,
      "learning_rate": 4.2144807251283175e-05,
      "loss": 2.8788,
      "step": 14390
    },
    {
      "epoch": 1.572567434749372,
      "grad_norm": 0.9795837998390198,
      "learning_rate": 4.2139346947690295e-05,
      "loss": 2.8722,
      "step": 14400
    },
    {
      "epoch": 1.5736594954679481,
      "grad_norm": 0.32662487030029297,
      "learning_rate": 4.2133886644097414e-05,
      "loss": 2.8651,
      "step": 14410
    },
    {
      "epoch": 1.574751556186524,
      "grad_norm": 0.27781811356544495,
      "learning_rate": 4.2128426340504533e-05,
      "loss": 2.875,
      "step": 14420
    },
    {
      "epoch": 1.5758436169050998,
      "grad_norm": 0.1942574381828308,
      "learning_rate": 4.212296603691165e-05,
      "loss": 2.8745,
      "step": 14430
    },
    {
      "epoch": 1.576935677623676,
      "grad_norm": 0.6077312231063843,
      "learning_rate": 4.211750573331877e-05,
      "loss": 2.8673,
      "step": 14440
    },
    {
      "epoch": 1.5780277383422519,
      "grad_norm": 0.2390892058610916,
      "learning_rate": 4.211204542972589e-05,
      "loss": 2.8625,
      "step": 14450
    },
    {
      "epoch": 1.5791197990608277,
      "grad_norm": 0.12008096277713776,
      "learning_rate": 4.210658512613301e-05,
      "loss": 2.872,
      "step": 14460
    },
    {
      "epoch": 1.5802118597794037,
      "grad_norm": 0.25100618600845337,
      "learning_rate": 4.210112482254013e-05,
      "loss": 2.8852,
      "step": 14470
    },
    {
      "epoch": 1.5813039204979797,
      "grad_norm": 0.15264752507209778,
      "learning_rate": 4.209566451894725e-05,
      "loss": 2.8598,
      "step": 14480
    },
    {
      "epoch": 1.5823959812165556,
      "grad_norm": 0.12524501979351044,
      "learning_rate": 4.2090204215354376e-05,
      "loss": 2.8829,
      "step": 14490
    },
    {
      "epoch": 1.5834880419351316,
      "grad_norm": 0.1861918866634369,
      "learning_rate": 4.2084743911761495e-05,
      "loss": 2.8435,
      "step": 14500
    },
    {
      "epoch": 1.5845801026537076,
      "grad_norm": 0.09929265081882477,
      "learning_rate": 4.2079283608168615e-05,
      "loss": 2.8408,
      "step": 14510
    },
    {
      "epoch": 1.5856721633722834,
      "grad_norm": 0.7563318014144897,
      "learning_rate": 4.207382330457574e-05,
      "loss": 2.8431,
      "step": 14520
    },
    {
      "epoch": 1.5867642240908595,
      "grad_norm": 0.28256121277809143,
      "learning_rate": 4.206836300098286e-05,
      "loss": 2.8508,
      "step": 14530
    },
    {
      "epoch": 1.5878562848094355,
      "grad_norm": 0.1585153192281723,
      "learning_rate": 4.206290269738998e-05,
      "loss": 2.8951,
      "step": 14540
    },
    {
      "epoch": 1.5889483455280113,
      "grad_norm": 0.13530439138412476,
      "learning_rate": 4.20574423937971e-05,
      "loss": 2.8458,
      "step": 14550
    },
    {
      "epoch": 1.5900404062465872,
      "grad_norm": 0.1028703823685646,
      "learning_rate": 4.205198209020422e-05,
      "loss": 2.8658,
      "step": 14560
    },
    {
      "epoch": 1.5911324669651634,
      "grad_norm": 0.1735296994447708,
      "learning_rate": 4.204652178661134e-05,
      "loss": 2.9009,
      "step": 14570
    },
    {
      "epoch": 1.5922245276837392,
      "grad_norm": 0.11933103203773499,
      "learning_rate": 4.204106148301846e-05,
      "loss": 2.8386,
      "step": 14580
    },
    {
      "epoch": 1.593316588402315,
      "grad_norm": 0.139532208442688,
      "learning_rate": 4.203560117942558e-05,
      "loss": 2.8708,
      "step": 14590
    },
    {
      "epoch": 1.594408649120891,
      "grad_norm": 0.28337475657463074,
      "learning_rate": 4.2030140875832696e-05,
      "loss": 2.8734,
      "step": 14600
    },
    {
      "epoch": 1.5955007098394671,
      "grad_norm": 0.09522213786840439,
      "learning_rate": 4.2024680572239816e-05,
      "loss": 2.8286,
      "step": 14610
    },
    {
      "epoch": 1.596592770558043,
      "grad_norm": 0.13903984427452087,
      "learning_rate": 4.2019220268646935e-05,
      "loss": 2.8712,
      "step": 14620
    },
    {
      "epoch": 1.597684831276619,
      "grad_norm": 0.41701409220695496,
      "learning_rate": 4.201375996505406e-05,
      "loss": 2.8508,
      "step": 14630
    },
    {
      "epoch": 1.598776891995195,
      "grad_norm": 0.17630521953105927,
      "learning_rate": 4.200829966146118e-05,
      "loss": 2.8935,
      "step": 14640
    },
    {
      "epoch": 1.5998689527137708,
      "grad_norm": 0.14912138879299164,
      "learning_rate": 4.20028393578683e-05,
      "loss": 2.9052,
      "step": 14650
    },
    {
      "epoch": 1.6009610134323469,
      "grad_norm": 0.15672460198402405,
      "learning_rate": 4.199737905427542e-05,
      "loss": 2.8666,
      "step": 14660
    },
    {
      "epoch": 1.602053074150923,
      "grad_norm": 0.4015132784843445,
      "learning_rate": 4.199191875068254e-05,
      "loss": 2.9008,
      "step": 14670
    },
    {
      "epoch": 1.6031451348694987,
      "grad_norm": 0.18399329483509064,
      "learning_rate": 4.198645844708966e-05,
      "loss": 2.8812,
      "step": 14680
    },
    {
      "epoch": 1.6042371955880745,
      "grad_norm": 0.4069599211215973,
      "learning_rate": 4.198099814349678e-05,
      "loss": 2.9327,
      "step": 14690
    },
    {
      "epoch": 1.6053292563066508,
      "grad_norm": 0.18238455057144165,
      "learning_rate": 4.19755378399039e-05,
      "loss": 2.9629,
      "step": 14700
    },
    {
      "epoch": 1.6064213170252266,
      "grad_norm": 0.18492597341537476,
      "learning_rate": 4.197007753631102e-05,
      "loss": 2.8871,
      "step": 14710
    },
    {
      "epoch": 1.6075133777438024,
      "grad_norm": 0.1442507952451706,
      "learning_rate": 4.196461723271814e-05,
      "loss": 2.8638,
      "step": 14720
    },
    {
      "epoch": 1.6086054384623785,
      "grad_norm": 0.1398061215877533,
      "learning_rate": 4.195915692912526e-05,
      "loss": 2.8501,
      "step": 14730
    },
    {
      "epoch": 1.6096974991809545,
      "grad_norm": 0.317063570022583,
      "learning_rate": 4.195369662553238e-05,
      "loss": 2.8558,
      "step": 14740
    },
    {
      "epoch": 1.6107895598995303,
      "grad_norm": 0.16429884731769562,
      "learning_rate": 4.19482363219395e-05,
      "loss": 2.9158,
      "step": 14750
    },
    {
      "epoch": 1.6118816206181064,
      "grad_norm": 0.20773544907569885,
      "learning_rate": 4.194277601834663e-05,
      "loss": 2.8608,
      "step": 14760
    },
    {
      "epoch": 1.6129736813366824,
      "grad_norm": 0.3013315498828888,
      "learning_rate": 4.193731571475375e-05,
      "loss": 2.8363,
      "step": 14770
    },
    {
      "epoch": 1.6140657420552582,
      "grad_norm": 0.3219946324825287,
      "learning_rate": 4.1931855411160866e-05,
      "loss": 2.8766,
      "step": 14780
    },
    {
      "epoch": 1.6151578027738343,
      "grad_norm": 0.19638094305992126,
      "learning_rate": 4.1926395107567985e-05,
      "loss": 2.8661,
      "step": 14790
    },
    {
      "epoch": 1.6162498634924103,
      "grad_norm": 0.13543733954429626,
      "learning_rate": 4.1920934803975105e-05,
      "loss": 2.8658,
      "step": 14800
    },
    {
      "epoch": 1.617341924210986,
      "grad_norm": 0.3254547119140625,
      "learning_rate": 4.1915474500382224e-05,
      "loss": 2.9522,
      "step": 14810
    },
    {
      "epoch": 1.618433984929562,
      "grad_norm": 0.2176693081855774,
      "learning_rate": 4.1910014196789344e-05,
      "loss": 2.885,
      "step": 14820
    },
    {
      "epoch": 1.6195260456481382,
      "grad_norm": 0.12503337860107422,
      "learning_rate": 4.190455389319646e-05,
      "loss": 2.8902,
      "step": 14830
    },
    {
      "epoch": 1.620618106366714,
      "grad_norm": 0.16795508563518524,
      "learning_rate": 4.189909358960358e-05,
      "loss": 2.8572,
      "step": 14840
    },
    {
      "epoch": 1.6217101670852898,
      "grad_norm": 0.2731671929359436,
      "learning_rate": 4.18936332860107e-05,
      "loss": 2.853,
      "step": 14850
    },
    {
      "epoch": 1.6228022278038658,
      "grad_norm": 0.2498876005411148,
      "learning_rate": 4.188817298241782e-05,
      "loss": 2.876,
      "step": 14860
    },
    {
      "epoch": 1.6238942885224419,
      "grad_norm": 0.083476722240448,
      "learning_rate": 4.188271267882494e-05,
      "loss": 2.8325,
      "step": 14870
    },
    {
      "epoch": 1.6249863492410177,
      "grad_norm": 0.3111414611339569,
      "learning_rate": 4.187725237523206e-05,
      "loss": 2.9106,
      "step": 14880
    },
    {
      "epoch": 1.6260784099595937,
      "grad_norm": 0.16596543788909912,
      "learning_rate": 4.1871792071639186e-05,
      "loss": 2.8403,
      "step": 14890
    },
    {
      "epoch": 1.6271704706781698,
      "grad_norm": 0.277775377035141,
      "learning_rate": 4.1866331768046306e-05,
      "loss": 2.8612,
      "step": 14900
    },
    {
      "epoch": 1.6282625313967456,
      "grad_norm": 0.4288156032562256,
      "learning_rate": 4.1860871464453425e-05,
      "loss": 2.8665,
      "step": 14910
    },
    {
      "epoch": 1.6293545921153216,
      "grad_norm": 0.6179216504096985,
      "learning_rate": 4.1855411160860545e-05,
      "loss": 2.873,
      "step": 14920
    },
    {
      "epoch": 1.6304466528338977,
      "grad_norm": 0.157755047082901,
      "learning_rate": 4.1849950857267664e-05,
      "loss": 2.8727,
      "step": 14930
    },
    {
      "epoch": 1.6315387135524735,
      "grad_norm": 0.16696950793266296,
      "learning_rate": 4.1844490553674783e-05,
      "loss": 2.8553,
      "step": 14940
    },
    {
      "epoch": 1.6326307742710495,
      "grad_norm": 0.13374000787734985,
      "learning_rate": 4.183903025008191e-05,
      "loss": 2.8702,
      "step": 14950
    },
    {
      "epoch": 1.6337228349896256,
      "grad_norm": 0.4340151250362396,
      "learning_rate": 4.183356994648903e-05,
      "loss": 2.9066,
      "step": 14960
    },
    {
      "epoch": 1.6348148957082014,
      "grad_norm": 0.42249396443367004,
      "learning_rate": 4.182810964289615e-05,
      "loss": 2.8444,
      "step": 14970
    },
    {
      "epoch": 1.6359069564267772,
      "grad_norm": 0.14719770848751068,
      "learning_rate": 4.182264933930327e-05,
      "loss": 2.9157,
      "step": 14980
    },
    {
      "epoch": 1.6369990171453532,
      "grad_norm": 0.1475832164287567,
      "learning_rate": 4.181718903571039e-05,
      "loss": 2.8896,
      "step": 14990
    },
    {
      "epoch": 1.6380910778639293,
      "grad_norm": 0.18093550205230713,
      "learning_rate": 4.181172873211751e-05,
      "loss": 2.8805,
      "step": 15000
    },
    {
      "epoch": 1.639183138582505,
      "grad_norm": 0.29318374395370483,
      "learning_rate": 4.1806268428524626e-05,
      "loss": 2.904,
      "step": 15010
    },
    {
      "epoch": 1.6402751993010811,
      "grad_norm": 0.16763079166412354,
      "learning_rate": 4.180080812493175e-05,
      "loss": 2.871,
      "step": 15020
    },
    {
      "epoch": 1.6413672600196572,
      "grad_norm": 0.24166366457939148,
      "learning_rate": 4.179534782133887e-05,
      "loss": 2.8696,
      "step": 15030
    },
    {
      "epoch": 1.642459320738233,
      "grad_norm": 0.1868285834789276,
      "learning_rate": 4.178988751774599e-05,
      "loss": 2.8611,
      "step": 15040
    },
    {
      "epoch": 1.643551381456809,
      "grad_norm": 0.16529829800128937,
      "learning_rate": 4.178442721415311e-05,
      "loss": 2.8563,
      "step": 15050
    },
    {
      "epoch": 1.644643442175385,
      "grad_norm": 0.1320754885673523,
      "learning_rate": 4.177896691056023e-05,
      "loss": 2.8743,
      "step": 15060
    },
    {
      "epoch": 1.6457355028939609,
      "grad_norm": 0.10912580043077469,
      "learning_rate": 4.177350660696735e-05,
      "loss": 2.8899,
      "step": 15070
    },
    {
      "epoch": 1.646827563612537,
      "grad_norm": 0.12312473356723785,
      "learning_rate": 4.176804630337447e-05,
      "loss": 2.8637,
      "step": 15080
    },
    {
      "epoch": 1.647919624331113,
      "grad_norm": 0.11898357421159744,
      "learning_rate": 4.176258599978159e-05,
      "loss": 2.8624,
      "step": 15090
    },
    {
      "epoch": 1.6490116850496888,
      "grad_norm": 0.15655137598514557,
      "learning_rate": 4.175712569618871e-05,
      "loss": 2.8489,
      "step": 15100
    },
    {
      "epoch": 1.6501037457682646,
      "grad_norm": 0.2604411840438843,
      "learning_rate": 4.175166539259583e-05,
      "loss": 2.9135,
      "step": 15110
    },
    {
      "epoch": 1.6511958064868408,
      "grad_norm": 0.1278924196958542,
      "learning_rate": 4.1746205089002946e-05,
      "loss": 2.8527,
      "step": 15120
    },
    {
      "epoch": 1.6522878672054166,
      "grad_norm": 0.2092270851135254,
      "learning_rate": 4.1740744785410066e-05,
      "loss": 2.84,
      "step": 15130
    },
    {
      "epoch": 1.6533799279239925,
      "grad_norm": 0.08716750890016556,
      "learning_rate": 4.1735284481817185e-05,
      "loss": 2.8251,
      "step": 15140
    },
    {
      "epoch": 1.6544719886425685,
      "grad_norm": 0.2373584657907486,
      "learning_rate": 4.172982417822431e-05,
      "loss": 2.8738,
      "step": 15150
    },
    {
      "epoch": 1.6555640493611445,
      "grad_norm": 0.11004668474197388,
      "learning_rate": 4.172436387463143e-05,
      "loss": 2.8308,
      "step": 15160
    },
    {
      "epoch": 1.6566561100797204,
      "grad_norm": 0.47852611541748047,
      "learning_rate": 4.171890357103855e-05,
      "loss": 2.9227,
      "step": 15170
    },
    {
      "epoch": 1.6577481707982964,
      "grad_norm": 0.1052238941192627,
      "learning_rate": 4.1713443267445676e-05,
      "loss": 2.8782,
      "step": 15180
    },
    {
      "epoch": 1.6588402315168724,
      "grad_norm": 0.14197611808776855,
      "learning_rate": 4.1707982963852796e-05,
      "loss": 2.8716,
      "step": 15190
    },
    {
      "epoch": 1.6599322922354482,
      "grad_norm": 0.14997364580631256,
      "learning_rate": 4.1702522660259915e-05,
      "loss": 2.8903,
      "step": 15200
    },
    {
      "epoch": 1.6610243529540243,
      "grad_norm": 0.156330406665802,
      "learning_rate": 4.1697062356667035e-05,
      "loss": 2.8514,
      "step": 15210
    },
    {
      "epoch": 1.6621164136726003,
      "grad_norm": 0.13198484480381012,
      "learning_rate": 4.1691602053074154e-05,
      "loss": 2.872,
      "step": 15220
    },
    {
      "epoch": 1.6632084743911761,
      "grad_norm": 0.15961651504039764,
      "learning_rate": 4.1686141749481273e-05,
      "loss": 2.867,
      "step": 15230
    },
    {
      "epoch": 1.664300535109752,
      "grad_norm": 0.1254279613494873,
      "learning_rate": 4.168068144588839e-05,
      "loss": 2.8756,
      "step": 15240
    },
    {
      "epoch": 1.6653925958283282,
      "grad_norm": 0.9191133379936218,
      "learning_rate": 4.167522114229551e-05,
      "loss": 2.8811,
      "step": 15250
    },
    {
      "epoch": 1.666484656546904,
      "grad_norm": 0.37403765320777893,
      "learning_rate": 4.166976083870263e-05,
      "loss": 2.939,
      "step": 15260
    },
    {
      "epoch": 1.6675767172654798,
      "grad_norm": 0.2648868262767792,
      "learning_rate": 4.166430053510975e-05,
      "loss": 2.889,
      "step": 15270
    },
    {
      "epoch": 1.6686687779840559,
      "grad_norm": 0.21872079372406006,
      "learning_rate": 4.165884023151688e-05,
      "loss": 2.8649,
      "step": 15280
    },
    {
      "epoch": 1.669760838702632,
      "grad_norm": 0.28678810596466064,
      "learning_rate": 4.1653379927924e-05,
      "loss": 2.8828,
      "step": 15290
    },
    {
      "epoch": 1.6708528994212077,
      "grad_norm": 0.11854328215122223,
      "learning_rate": 4.1647919624331116e-05,
      "loss": 2.8566,
      "step": 15300
    },
    {
      "epoch": 1.6719449601397838,
      "grad_norm": 0.21958719193935394,
      "learning_rate": 4.1642459320738235e-05,
      "loss": 2.8498,
      "step": 15310
    },
    {
      "epoch": 1.6730370208583598,
      "grad_norm": 0.3097864091396332,
      "learning_rate": 4.1636999017145355e-05,
      "loss": 2.8872,
      "step": 15320
    },
    {
      "epoch": 1.6741290815769356,
      "grad_norm": 0.16969716548919678,
      "learning_rate": 4.1631538713552474e-05,
      "loss": 2.854,
      "step": 15330
    },
    {
      "epoch": 1.6752211422955117,
      "grad_norm": 0.1879425048828125,
      "learning_rate": 4.1626078409959594e-05,
      "loss": 2.8745,
      "step": 15340
    },
    {
      "epoch": 1.6763132030140877,
      "grad_norm": 0.17193490266799927,
      "learning_rate": 4.162061810636671e-05,
      "loss": 2.8453,
      "step": 15350
    },
    {
      "epoch": 1.6774052637326635,
      "grad_norm": 0.21745407581329346,
      "learning_rate": 4.161515780277383e-05,
      "loss": 2.8643,
      "step": 15360
    },
    {
      "epoch": 1.6784973244512393,
      "grad_norm": 0.8805167078971863,
      "learning_rate": 4.160969749918095e-05,
      "loss": 2.9037,
      "step": 15370
    },
    {
      "epoch": 1.6795893851698156,
      "grad_norm": 0.19829058647155762,
      "learning_rate": 4.160423719558808e-05,
      "loss": 2.8974,
      "step": 15380
    },
    {
      "epoch": 1.6806814458883914,
      "grad_norm": 0.21734009683132172,
      "learning_rate": 4.15987768919952e-05,
      "loss": 2.8993,
      "step": 15390
    },
    {
      "epoch": 1.6817735066069672,
      "grad_norm": 0.38473084568977356,
      "learning_rate": 4.159331658840232e-05,
      "loss": 2.8565,
      "step": 15400
    },
    {
      "epoch": 1.6828655673255433,
      "grad_norm": 0.1588710993528366,
      "learning_rate": 4.158785628480944e-05,
      "loss": 2.8436,
      "step": 15410
    },
    {
      "epoch": 1.6839576280441193,
      "grad_norm": 0.1529221087694168,
      "learning_rate": 4.158239598121656e-05,
      "loss": 2.8739,
      "step": 15420
    },
    {
      "epoch": 1.6850496887626951,
      "grad_norm": 0.671467661857605,
      "learning_rate": 4.157693567762368e-05,
      "loss": 2.8875,
      "step": 15430
    },
    {
      "epoch": 1.6861417494812712,
      "grad_norm": 0.3109177350997925,
      "learning_rate": 4.15714753740308e-05,
      "loss": 2.8782,
      "step": 15440
    },
    {
      "epoch": 1.6872338101998472,
      "grad_norm": 0.5457100868225098,
      "learning_rate": 4.156601507043792e-05,
      "loss": 2.861,
      "step": 15450
    },
    {
      "epoch": 1.688325870918423,
      "grad_norm": 0.24751366674900055,
      "learning_rate": 4.156055476684504e-05,
      "loss": 2.8782,
      "step": 15460
    },
    {
      "epoch": 1.689417931636999,
      "grad_norm": 0.2270420789718628,
      "learning_rate": 4.155509446325216e-05,
      "loss": 2.8591,
      "step": 15470
    },
    {
      "epoch": 1.690509992355575,
      "grad_norm": 0.16720226407051086,
      "learning_rate": 4.154963415965928e-05,
      "loss": 2.9655,
      "step": 15480
    },
    {
      "epoch": 1.691602053074151,
      "grad_norm": 0.20635689795017242,
      "learning_rate": 4.15441738560664e-05,
      "loss": 2.8792,
      "step": 15490
    },
    {
      "epoch": 1.6926941137927267,
      "grad_norm": 0.2038746476173401,
      "learning_rate": 4.153871355247352e-05,
      "loss": 2.8593,
      "step": 15500
    },
    {
      "epoch": 1.693786174511303,
      "grad_norm": 0.4953509271144867,
      "learning_rate": 4.153325324888064e-05,
      "loss": 2.9046,
      "step": 15510
    },
    {
      "epoch": 1.6948782352298788,
      "grad_norm": 0.14527128636837006,
      "learning_rate": 4.1527792945287757e-05,
      "loss": 2.897,
      "step": 15520
    },
    {
      "epoch": 1.6959702959484546,
      "grad_norm": 0.44985586404800415,
      "learning_rate": 4.1522332641694876e-05,
      "loss": 2.8728,
      "step": 15530
    },
    {
      "epoch": 1.6970623566670306,
      "grad_norm": 0.18227408826351166,
      "learning_rate": 4.1516872338102e-05,
      "loss": 2.8335,
      "step": 15540
    },
    {
      "epoch": 1.6981544173856067,
      "grad_norm": 0.14505331218242645,
      "learning_rate": 4.151141203450912e-05,
      "loss": 2.8391,
      "step": 15550
    },
    {
      "epoch": 1.6992464781041825,
      "grad_norm": 0.13928334414958954,
      "learning_rate": 4.150595173091624e-05,
      "loss": 2.9108,
      "step": 15560
    },
    {
      "epoch": 1.7003385388227585,
      "grad_norm": 0.3294704854488373,
      "learning_rate": 4.150049142732336e-05,
      "loss": 2.8587,
      "step": 15570
    },
    {
      "epoch": 1.7014305995413346,
      "grad_norm": 0.14876294136047363,
      "learning_rate": 4.149503112373048e-05,
      "loss": 2.8803,
      "step": 15580
    },
    {
      "epoch": 1.7025226602599104,
      "grad_norm": 0.12578006088733673,
      "learning_rate": 4.14895708201376e-05,
      "loss": 2.8377,
      "step": 15590
    },
    {
      "epoch": 1.7036147209784864,
      "grad_norm": 0.10071994364261627,
      "learning_rate": 4.148411051654472e-05,
      "loss": 2.8674,
      "step": 15600
    },
    {
      "epoch": 1.7047067816970625,
      "grad_norm": 0.2276199460029602,
      "learning_rate": 4.1478650212951845e-05,
      "loss": 2.841,
      "step": 15610
    },
    {
      "epoch": 1.7057988424156383,
      "grad_norm": 0.13085763156414032,
      "learning_rate": 4.1473189909358964e-05,
      "loss": 2.8692,
      "step": 15620
    },
    {
      "epoch": 1.706890903134214,
      "grad_norm": 0.27359437942504883,
      "learning_rate": 4.1467729605766084e-05,
      "loss": 2.8406,
      "step": 15630
    },
    {
      "epoch": 1.7079829638527904,
      "grad_norm": 0.21411851048469543,
      "learning_rate": 4.14622693021732e-05,
      "loss": 2.8838,
      "step": 15640
    },
    {
      "epoch": 1.7090750245713662,
      "grad_norm": 0.19709733128547668,
      "learning_rate": 4.145680899858032e-05,
      "loss": 2.8823,
      "step": 15650
    },
    {
      "epoch": 1.710167085289942,
      "grad_norm": 0.23273152112960815,
      "learning_rate": 4.145134869498744e-05,
      "loss": 2.8951,
      "step": 15660
    },
    {
      "epoch": 1.711259146008518,
      "grad_norm": 0.31267493963241577,
      "learning_rate": 4.144588839139457e-05,
      "loss": 2.8542,
      "step": 15670
    },
    {
      "epoch": 1.712351206727094,
      "grad_norm": 0.14490556716918945,
      "learning_rate": 4.144042808780169e-05,
      "loss": 2.8982,
      "step": 15680
    },
    {
      "epoch": 1.7134432674456699,
      "grad_norm": 0.29984402656555176,
      "learning_rate": 4.143496778420881e-05,
      "loss": 2.8504,
      "step": 15690
    },
    {
      "epoch": 1.714535328164246,
      "grad_norm": 0.2578221261501312,
      "learning_rate": 4.1429507480615926e-05,
      "loss": 2.8611,
      "step": 15700
    },
    {
      "epoch": 1.715627388882822,
      "grad_norm": 0.0978461354970932,
      "learning_rate": 4.1424047177023046e-05,
      "loss": 2.9004,
      "step": 15710
    },
    {
      "epoch": 1.7167194496013978,
      "grad_norm": 0.123377226293087,
      "learning_rate": 4.1418586873430165e-05,
      "loss": 2.8349,
      "step": 15720
    },
    {
      "epoch": 1.7178115103199738,
      "grad_norm": 0.30045297741889954,
      "learning_rate": 4.1413126569837285e-05,
      "loss": 2.8784,
      "step": 15730
    },
    {
      "epoch": 1.7189035710385498,
      "grad_norm": 0.22342492640018463,
      "learning_rate": 4.1407666266244404e-05,
      "loss": 2.8539,
      "step": 15740
    },
    {
      "epoch": 1.7199956317571257,
      "grad_norm": 0.33260422945022583,
      "learning_rate": 4.140220596265152e-05,
      "loss": 2.8432,
      "step": 15750
    },
    {
      "epoch": 1.7210876924757017,
      "grad_norm": 0.5216262340545654,
      "learning_rate": 4.139674565905864e-05,
      "loss": 2.8801,
      "step": 15760
    },
    {
      "epoch": 1.7221797531942777,
      "grad_norm": 0.18329794704914093,
      "learning_rate": 4.139128535546576e-05,
      "loss": 2.8991,
      "step": 15770
    },
    {
      "epoch": 1.7232718139128536,
      "grad_norm": 0.22309736907482147,
      "learning_rate": 4.138582505187288e-05,
      "loss": 2.891,
      "step": 15780
    },
    {
      "epoch": 1.7243638746314294,
      "grad_norm": 0.2767999768257141,
      "learning_rate": 4.138036474828e-05,
      "loss": 2.964,
      "step": 15790
    },
    {
      "epoch": 1.7254559353500054,
      "grad_norm": 0.11222237348556519,
      "learning_rate": 4.137490444468713e-05,
      "loss": 2.8657,
      "step": 15800
    },
    {
      "epoch": 1.7265479960685814,
      "grad_norm": 0.37906137108802795,
      "learning_rate": 4.1369444141094247e-05,
      "loss": 2.881,
      "step": 15810
    },
    {
      "epoch": 1.7276400567871573,
      "grad_norm": 0.13551802933216095,
      "learning_rate": 4.1363983837501366e-05,
      "loss": 2.88,
      "step": 15820
    },
    {
      "epoch": 1.7287321175057333,
      "grad_norm": 0.16681790351867676,
      "learning_rate": 4.1358523533908485e-05,
      "loss": 2.8586,
      "step": 15830
    },
    {
      "epoch": 1.7298241782243093,
      "grad_norm": 0.09152109920978546,
      "learning_rate": 4.135306323031561e-05,
      "loss": 2.8617,
      "step": 15840
    },
    {
      "epoch": 1.7309162389428852,
      "grad_norm": 0.9703966379165649,
      "learning_rate": 4.134760292672273e-05,
      "loss": 2.9222,
      "step": 15850
    },
    {
      "epoch": 1.7320082996614612,
      "grad_norm": 0.376151442527771,
      "learning_rate": 4.134214262312985e-05,
      "loss": 2.8498,
      "step": 15860
    },
    {
      "epoch": 1.7331003603800372,
      "grad_norm": 0.47381842136383057,
      "learning_rate": 4.133668231953697e-05,
      "loss": 2.9501,
      "step": 15870
    },
    {
      "epoch": 1.734192421098613,
      "grad_norm": 0.15144230425357819,
      "learning_rate": 4.133122201594409e-05,
      "loss": 2.8415,
      "step": 15880
    },
    {
      "epoch": 1.735284481817189,
      "grad_norm": 0.3808157742023468,
      "learning_rate": 4.132576171235121e-05,
      "loss": 2.8516,
      "step": 15890
    },
    {
      "epoch": 1.7363765425357651,
      "grad_norm": 0.25310730934143066,
      "learning_rate": 4.132030140875833e-05,
      "loss": 2.8472,
      "step": 15900
    },
    {
      "epoch": 1.737468603254341,
      "grad_norm": 0.14271487295627594,
      "learning_rate": 4.131484110516545e-05,
      "loss": 2.866,
      "step": 15910
    },
    {
      "epoch": 1.7385606639729168,
      "grad_norm": 0.2804289162158966,
      "learning_rate": 4.130938080157257e-05,
      "loss": 2.8666,
      "step": 15920
    },
    {
      "epoch": 1.739652724691493,
      "grad_norm": 0.2467953860759735,
      "learning_rate": 4.130392049797969e-05,
      "loss": 2.9176,
      "step": 15930
    },
    {
      "epoch": 1.7407447854100688,
      "grad_norm": 0.13767699897289276,
      "learning_rate": 4.129846019438681e-05,
      "loss": 2.8417,
      "step": 15940
    },
    {
      "epoch": 1.7418368461286446,
      "grad_norm": 0.2695741057395935,
      "learning_rate": 4.129299989079393e-05,
      "loss": 2.89,
      "step": 15950
    },
    {
      "epoch": 1.7429289068472207,
      "grad_norm": 0.2789399325847626,
      "learning_rate": 4.128753958720105e-05,
      "loss": 2.8738,
      "step": 15960
    },
    {
      "epoch": 1.7440209675657967,
      "grad_norm": 0.13240599632263184,
      "learning_rate": 4.128207928360817e-05,
      "loss": 2.862,
      "step": 15970
    },
    {
      "epoch": 1.7451130282843725,
      "grad_norm": 0.20101772248744965,
      "learning_rate": 4.127661898001529e-05,
      "loss": 2.855,
      "step": 15980
    },
    {
      "epoch": 1.7462050890029486,
      "grad_norm": 0.3062577247619629,
      "learning_rate": 4.127115867642241e-05,
      "loss": 2.8851,
      "step": 15990
    },
    {
      "epoch": 1.7472971497215246,
      "grad_norm": 0.2085835337638855,
      "learning_rate": 4.126569837282953e-05,
      "loss": 2.8417,
      "step": 16000
    },
    {
      "epoch": 1.7483892104401004,
      "grad_norm": 0.10426823794841766,
      "learning_rate": 4.126023806923665e-05,
      "loss": 2.859,
      "step": 16010
    },
    {
      "epoch": 1.7494812711586765,
      "grad_norm": 0.2400476038455963,
      "learning_rate": 4.125477776564377e-05,
      "loss": 2.8692,
      "step": 16020
    },
    {
      "epoch": 1.7505733318772525,
      "grad_norm": 0.15707524120807648,
      "learning_rate": 4.124931746205089e-05,
      "loss": 2.8767,
      "step": 16030
    },
    {
      "epoch": 1.7516653925958283,
      "grad_norm": 0.742813229560852,
      "learning_rate": 4.124385715845801e-05,
      "loss": 2.8609,
      "step": 16040
    },
    {
      "epoch": 1.7527574533144041,
      "grad_norm": 0.6844467520713806,
      "learning_rate": 4.123839685486513e-05,
      "loss": 2.8811,
      "step": 16050
    },
    {
      "epoch": 1.7538495140329804,
      "grad_norm": 0.09864421933889389,
      "learning_rate": 4.123293655127225e-05,
      "loss": 2.8545,
      "step": 16060
    },
    {
      "epoch": 1.7549415747515562,
      "grad_norm": 0.11070208996534348,
      "learning_rate": 4.122747624767938e-05,
      "loss": 2.8814,
      "step": 16070
    },
    {
      "epoch": 1.756033635470132,
      "grad_norm": 0.12457341700792313,
      "learning_rate": 4.12220159440865e-05,
      "loss": 2.8578,
      "step": 16080
    },
    {
      "epoch": 1.757125696188708,
      "grad_norm": 0.32782360911369324,
      "learning_rate": 4.121655564049362e-05,
      "loss": 2.851,
      "step": 16090
    },
    {
      "epoch": 1.758217756907284,
      "grad_norm": 0.21187494695186615,
      "learning_rate": 4.1211641367260024e-05,
      "loss": 2.8653,
      "step": 16100
    },
    {
      "epoch": 1.75930981762586,
      "grad_norm": 0.21047179400920868,
      "learning_rate": 4.120618106366714e-05,
      "loss": 2.9047,
      "step": 16110
    },
    {
      "epoch": 1.760401878344436,
      "grad_norm": 0.10448925197124481,
      "learning_rate": 4.120072076007426e-05,
      "loss": 2.8414,
      "step": 16120
    },
    {
      "epoch": 1.761493939063012,
      "grad_norm": 0.14939230680465698,
      "learning_rate": 4.119526045648138e-05,
      "loss": 2.8605,
      "step": 16130
    },
    {
      "epoch": 1.7625859997815878,
      "grad_norm": 0.1715175211429596,
      "learning_rate": 4.11898001528885e-05,
      "loss": 2.8579,
      "step": 16140
    },
    {
      "epoch": 1.7636780605001638,
      "grad_norm": 0.20698031783103943,
      "learning_rate": 4.118433984929562e-05,
      "loss": 2.8732,
      "step": 16150
    },
    {
      "epoch": 1.7647701212187399,
      "grad_norm": 0.3059464395046234,
      "learning_rate": 4.117887954570274e-05,
      "loss": 2.9069,
      "step": 16160
    },
    {
      "epoch": 1.7658621819373157,
      "grad_norm": 0.16089269518852234,
      "learning_rate": 4.117341924210986e-05,
      "loss": 2.941,
      "step": 16170
    },
    {
      "epoch": 1.7669542426558915,
      "grad_norm": 0.1454644650220871,
      "learning_rate": 4.116795893851698e-05,
      "loss": 2.8627,
      "step": 16180
    },
    {
      "epoch": 1.7680463033744678,
      "grad_norm": 0.1314690262079239,
      "learning_rate": 4.11624986349241e-05,
      "loss": 2.8763,
      "step": 16190
    },
    {
      "epoch": 1.7691383640930436,
      "grad_norm": 0.10067670792341232,
      "learning_rate": 4.115703833133122e-05,
      "loss": 2.8742,
      "step": 16200
    },
    {
      "epoch": 1.7702304248116194,
      "grad_norm": 0.5491948127746582,
      "learning_rate": 4.1151578027738344e-05,
      "loss": 2.8888,
      "step": 16210
    },
    {
      "epoch": 1.7713224855301954,
      "grad_norm": 0.11490987986326218,
      "learning_rate": 4.1146117724145464e-05,
      "loss": 2.832,
      "step": 16220
    },
    {
      "epoch": 1.7724145462487715,
      "grad_norm": 0.48815664649009705,
      "learning_rate": 4.114065742055258e-05,
      "loss": 2.8675,
      "step": 16230
    },
    {
      "epoch": 1.7735066069673473,
      "grad_norm": 0.3258272111415863,
      "learning_rate": 4.113519711695971e-05,
      "loss": 2.8615,
      "step": 16240
    },
    {
      "epoch": 1.7745986676859233,
      "grad_norm": 0.28681159019470215,
      "learning_rate": 4.112973681336683e-05,
      "loss": 2.8585,
      "step": 16250
    },
    {
      "epoch": 1.7756907284044994,
      "grad_norm": 0.23400989174842834,
      "learning_rate": 4.112427650977395e-05,
      "loss": 2.8848,
      "step": 16260
    },
    {
      "epoch": 1.7767827891230752,
      "grad_norm": 0.20201830565929413,
      "learning_rate": 4.111881620618107e-05,
      "loss": 2.8689,
      "step": 16270
    },
    {
      "epoch": 1.7778748498416512,
      "grad_norm": 0.6411125659942627,
      "learning_rate": 4.111335590258819e-05,
      "loss": 2.8636,
      "step": 16280
    },
    {
      "epoch": 1.7789669105602273,
      "grad_norm": 0.179067000746727,
      "learning_rate": 4.1107895598995306e-05,
      "loss": 2.8567,
      "step": 16290
    },
    {
      "epoch": 1.780058971278803,
      "grad_norm": 0.20737533271312714,
      "learning_rate": 4.1102435295402426e-05,
      "loss": 2.9017,
      "step": 16300
    },
    {
      "epoch": 1.781151031997379,
      "grad_norm": 0.27501213550567627,
      "learning_rate": 4.1096974991809545e-05,
      "loss": 2.92,
      "step": 16310
    },
    {
      "epoch": 1.7822430927159552,
      "grad_norm": 0.3411715030670166,
      "learning_rate": 4.1091514688216665e-05,
      "loss": 2.928,
      "step": 16320
    },
    {
      "epoch": 1.783335153434531,
      "grad_norm": 0.11891543865203857,
      "learning_rate": 4.1086054384623784e-05,
      "loss": 2.8637,
      "step": 16330
    },
    {
      "epoch": 1.7844272141531068,
      "grad_norm": 0.10860411077737808,
      "learning_rate": 4.108059408103091e-05,
      "loss": 2.882,
      "step": 16340
    },
    {
      "epoch": 1.7855192748716828,
      "grad_norm": 0.29719868302345276,
      "learning_rate": 4.107513377743803e-05,
      "loss": 2.8751,
      "step": 16350
    },
    {
      "epoch": 1.7866113355902589,
      "grad_norm": 0.1728152334690094,
      "learning_rate": 4.106967347384515e-05,
      "loss": 2.863,
      "step": 16360
    },
    {
      "epoch": 1.7877033963088347,
      "grad_norm": 0.3191945552825928,
      "learning_rate": 4.106421317025227e-05,
      "loss": 2.8468,
      "step": 16370
    },
    {
      "epoch": 1.7887954570274107,
      "grad_norm": 0.37644821405410767,
      "learning_rate": 4.105875286665939e-05,
      "loss": 2.9081,
      "step": 16380
    },
    {
      "epoch": 1.7898875177459868,
      "grad_norm": 0.1783371865749359,
      "learning_rate": 4.105329256306651e-05,
      "loss": 2.8665,
      "step": 16390
    },
    {
      "epoch": 1.7909795784645626,
      "grad_norm": 0.15206855535507202,
      "learning_rate": 4.1047832259473627e-05,
      "loss": 2.8465,
      "step": 16400
    },
    {
      "epoch": 1.7920716391831386,
      "grad_norm": 0.8624040484428406,
      "learning_rate": 4.1042371955880746e-05,
      "loss": 2.8902,
      "step": 16410
    },
    {
      "epoch": 1.7931636999017146,
      "grad_norm": 0.2345641553401947,
      "learning_rate": 4.1036911652287865e-05,
      "loss": 2.8866,
      "step": 16420
    },
    {
      "epoch": 1.7942557606202905,
      "grad_norm": 0.1314607709646225,
      "learning_rate": 4.1031451348694985e-05,
      "loss": 2.8629,
      "step": 16430
    },
    {
      "epoch": 1.7953478213388663,
      "grad_norm": 0.12153049558401108,
      "learning_rate": 4.102599104510211e-05,
      "loss": 2.8575,
      "step": 16440
    },
    {
      "epoch": 1.7964398820574425,
      "grad_norm": 0.12553969025611877,
      "learning_rate": 4.102053074150923e-05,
      "loss": 2.8918,
      "step": 16450
    },
    {
      "epoch": 1.7975319427760184,
      "grad_norm": 0.11712878942489624,
      "learning_rate": 4.101507043791635e-05,
      "loss": 2.8357,
      "step": 16460
    },
    {
      "epoch": 1.7986240034945942,
      "grad_norm": 0.32302170991897583,
      "learning_rate": 4.1009610134323476e-05,
      "loss": 2.8669,
      "step": 16470
    },
    {
      "epoch": 1.7997160642131702,
      "grad_norm": 0.21061351895332336,
      "learning_rate": 4.1004149830730595e-05,
      "loss": 2.9143,
      "step": 16480
    },
    {
      "epoch": 1.8008081249317462,
      "grad_norm": 0.12345121800899506,
      "learning_rate": 4.0998689527137715e-05,
      "loss": 2.8675,
      "step": 16490
    },
    {
      "epoch": 1.801900185650322,
      "grad_norm": 0.18818166851997375,
      "learning_rate": 4.0993229223544834e-05,
      "loss": 2.8569,
      "step": 16500
    },
    {
      "epoch": 1.802992246368898,
      "grad_norm": 0.36627209186553955,
      "learning_rate": 4.0987768919951954e-05,
      "loss": 2.9436,
      "step": 16510
    },
    {
      "epoch": 1.8040843070874741,
      "grad_norm": 0.39482229948043823,
      "learning_rate": 4.098230861635907e-05,
      "loss": 2.8666,
      "step": 16520
    },
    {
      "epoch": 1.80517636780605,
      "grad_norm": 0.18472474813461304,
      "learning_rate": 4.097684831276619e-05,
      "loss": 2.8497,
      "step": 16530
    },
    {
      "epoch": 1.806268428524626,
      "grad_norm": 0.5061404705047607,
      "learning_rate": 4.097138800917331e-05,
      "loss": 2.8429,
      "step": 16540
    },
    {
      "epoch": 1.807360489243202,
      "grad_norm": 0.1464349776506424,
      "learning_rate": 4.096592770558043e-05,
      "loss": 2.8331,
      "step": 16550
    },
    {
      "epoch": 1.8084525499617778,
      "grad_norm": 0.41614222526550293,
      "learning_rate": 4.096046740198755e-05,
      "loss": 2.8843,
      "step": 16560
    },
    {
      "epoch": 1.8095446106803539,
      "grad_norm": 0.15183261036872864,
      "learning_rate": 4.095500709839467e-05,
      "loss": 2.828,
      "step": 16570
    },
    {
      "epoch": 1.81063667139893,
      "grad_norm": 0.15516939759254456,
      "learning_rate": 4.094954679480179e-05,
      "loss": 2.8778,
      "step": 16580
    },
    {
      "epoch": 1.8117287321175057,
      "grad_norm": 0.1371675729751587,
      "learning_rate": 4.094408649120891e-05,
      "loss": 2.8611,
      "step": 16590
    },
    {
      "epoch": 1.8128207928360816,
      "grad_norm": 0.15186306834220886,
      "learning_rate": 4.0938626187616035e-05,
      "loss": 2.8673,
      "step": 16600
    },
    {
      "epoch": 1.8139128535546576,
      "grad_norm": 0.22530938684940338,
      "learning_rate": 4.0933165884023155e-05,
      "loss": 2.8259,
      "step": 16610
    },
    {
      "epoch": 1.8150049142732336,
      "grad_norm": 0.6104866862297058,
      "learning_rate": 4.0927705580430274e-05,
      "loss": 2.9081,
      "step": 16620
    },
    {
      "epoch": 1.8160969749918094,
      "grad_norm": 0.22106942534446716,
      "learning_rate": 4.092224527683739e-05,
      "loss": 2.9232,
      "step": 16630
    },
    {
      "epoch": 1.8171890357103855,
      "grad_norm": 0.11247522383928299,
      "learning_rate": 4.091678497324451e-05,
      "loss": 2.8575,
      "step": 16640
    },
    {
      "epoch": 1.8182810964289615,
      "grad_norm": 0.22583846747875214,
      "learning_rate": 4.091132466965163e-05,
      "loss": 2.9118,
      "step": 16650
    },
    {
      "epoch": 1.8193731571475373,
      "grad_norm": 0.18876075744628906,
      "learning_rate": 4.090586436605875e-05,
      "loss": 2.8714,
      "step": 16660
    },
    {
      "epoch": 1.8204652178661134,
      "grad_norm": 0.13458630442619324,
      "learning_rate": 4.090040406246588e-05,
      "loss": 2.8914,
      "step": 16670
    },
    {
      "epoch": 1.8215572785846894,
      "grad_norm": 0.24307021498680115,
      "learning_rate": 4.0894943758873e-05,
      "loss": 2.9237,
      "step": 16680
    },
    {
      "epoch": 1.8226493393032652,
      "grad_norm": 0.27376073598861694,
      "learning_rate": 4.0889483455280117e-05,
      "loss": 2.881,
      "step": 16690
    },
    {
      "epoch": 1.8237414000218413,
      "grad_norm": 0.10640257596969604,
      "learning_rate": 4.0884023151687236e-05,
      "loss": 2.8474,
      "step": 16700
    },
    {
      "epoch": 1.8248334607404173,
      "grad_norm": 0.1258828192949295,
      "learning_rate": 4.0878562848094355e-05,
      "loss": 2.8789,
      "step": 16710
    },
    {
      "epoch": 1.8259255214589931,
      "grad_norm": 0.11492281407117844,
      "learning_rate": 4.0873102544501475e-05,
      "loss": 2.839,
      "step": 16720
    },
    {
      "epoch": 1.827017582177569,
      "grad_norm": 0.16442717611789703,
      "learning_rate": 4.08676422409086e-05,
      "loss": 2.8784,
      "step": 16730
    },
    {
      "epoch": 1.828109642896145,
      "grad_norm": 0.16733700037002563,
      "learning_rate": 4.086218193731572e-05,
      "loss": 2.8464,
      "step": 16740
    },
    {
      "epoch": 1.829201703614721,
      "grad_norm": 0.2194519340991974,
      "learning_rate": 4.085672163372284e-05,
      "loss": 2.9965,
      "step": 16750
    },
    {
      "epoch": 1.8302937643332968,
      "grad_norm": 0.11664623022079468,
      "learning_rate": 4.085126133012996e-05,
      "loss": 2.8492,
      "step": 16760
    },
    {
      "epoch": 1.8313858250518729,
      "grad_norm": 0.11011240631341934,
      "learning_rate": 4.084580102653708e-05,
      "loss": 2.8292,
      "step": 16770
    },
    {
      "epoch": 1.832477885770449,
      "grad_norm": 0.16165830194950104,
      "learning_rate": 4.08403407229442e-05,
      "loss": 2.8793,
      "step": 16780
    },
    {
      "epoch": 1.8335699464890247,
      "grad_norm": 0.183681458234787,
      "learning_rate": 4.083488041935132e-05,
      "loss": 2.866,
      "step": 16790
    },
    {
      "epoch": 1.8346620072076008,
      "grad_norm": 0.37108922004699707,
      "learning_rate": 4.082942011575844e-05,
      "loss": 2.8715,
      "step": 16800
    },
    {
      "epoch": 1.8357540679261768,
      "grad_norm": 0.15980657935142517,
      "learning_rate": 4.0823959812165556e-05,
      "loss": 2.833,
      "step": 16810
    },
    {
      "epoch": 1.8368461286447526,
      "grad_norm": 0.21646341681480408,
      "learning_rate": 4.0818499508572676e-05,
      "loss": 2.8725,
      "step": 16820
    },
    {
      "epoch": 1.8379381893633286,
      "grad_norm": 0.16803410649299622,
      "learning_rate": 4.0813039204979795e-05,
      "loss": 2.909,
      "step": 16830
    },
    {
      "epoch": 1.8390302500819047,
      "grad_norm": 0.1698305606842041,
      "learning_rate": 4.0807578901386914e-05,
      "loss": 2.8648,
      "step": 16840
    },
    {
      "epoch": 1.8401223108004805,
      "grad_norm": 0.13676384091377258,
      "learning_rate": 4.0802118597794034e-05,
      "loss": 2.8587,
      "step": 16850
    },
    {
      "epoch": 1.8412143715190563,
      "grad_norm": 0.10523612797260284,
      "learning_rate": 4.079665829420116e-05,
      "loss": 2.8423,
      "step": 16860
    },
    {
      "epoch": 1.8423064322376326,
      "grad_norm": 0.22197483479976654,
      "learning_rate": 4.079119799060828e-05,
      "loss": 2.8981,
      "step": 16870
    },
    {
      "epoch": 1.8433984929562084,
      "grad_norm": 0.2706510126590729,
      "learning_rate": 4.07857376870154e-05,
      "loss": 2.8436,
      "step": 16880
    },
    {
      "epoch": 1.8444905536747842,
      "grad_norm": 0.19830501079559326,
      "learning_rate": 4.078027738342252e-05,
      "loss": 2.8551,
      "step": 16890
    },
    {
      "epoch": 1.8455826143933602,
      "grad_norm": 0.24601300060749054,
      "learning_rate": 4.0774817079829645e-05,
      "loss": 2.8397,
      "step": 16900
    },
    {
      "epoch": 1.8466746751119363,
      "grad_norm": 0.12363999336957932,
      "learning_rate": 4.0769356776236764e-05,
      "loss": 2.8641,
      "step": 16910
    },
    {
      "epoch": 1.847766735830512,
      "grad_norm": 0.2330225110054016,
      "learning_rate": 4.076389647264388e-05,
      "loss": 2.8634,
      "step": 16920
    },
    {
      "epoch": 1.8488587965490881,
      "grad_norm": 0.3396073877811432,
      "learning_rate": 4.0758436169051e-05,
      "loss": 2.8965,
      "step": 16930
    },
    {
      "epoch": 1.8499508572676642,
      "grad_norm": 0.15220801532268524,
      "learning_rate": 4.075297586545812e-05,
      "loss": 2.874,
      "step": 16940
    },
    {
      "epoch": 1.85104291798624,
      "grad_norm": 0.1601368486881256,
      "learning_rate": 4.074751556186524e-05,
      "loss": 2.8398,
      "step": 16950
    },
    {
      "epoch": 1.852134978704816,
      "grad_norm": 1.1374598741531372,
      "learning_rate": 4.074205525827236e-05,
      "loss": 2.8897,
      "step": 16960
    },
    {
      "epoch": 1.853227039423392,
      "grad_norm": 0.30345413088798523,
      "learning_rate": 4.073659495467948e-05,
      "loss": 2.8677,
      "step": 16970
    },
    {
      "epoch": 1.8543191001419679,
      "grad_norm": 0.24867770075798035,
      "learning_rate": 4.07311346510866e-05,
      "loss": 2.9035,
      "step": 16980
    },
    {
      "epoch": 1.8554111608605437,
      "grad_norm": 0.1577821522951126,
      "learning_rate": 4.0725674347493726e-05,
      "loss": 2.8523,
      "step": 16990
    },
    {
      "epoch": 1.85650322157912,
      "grad_norm": 0.12028899788856506,
      "learning_rate": 4.0720214043900845e-05,
      "loss": 2.8767,
      "step": 17000
    },
    {
      "epoch": 1.8575952822976958,
      "grad_norm": 0.1088939979672432,
      "learning_rate": 4.0714753740307965e-05,
      "loss": 2.8439,
      "step": 17010
    },
    {
      "epoch": 1.8586873430162716,
      "grad_norm": 0.3132420778274536,
      "learning_rate": 4.0709293436715084e-05,
      "loss": 2.8508,
      "step": 17020
    },
    {
      "epoch": 1.8597794037348476,
      "grad_norm": 0.31130003929138184,
      "learning_rate": 4.0703833133122204e-05,
      "loss": 2.8997,
      "step": 17030
    },
    {
      "epoch": 1.8608714644534237,
      "grad_norm": 0.11489426344633102,
      "learning_rate": 4.069837282952932e-05,
      "loss": 2.8559,
      "step": 17040
    },
    {
      "epoch": 1.8619635251719995,
      "grad_norm": 0.1802031248807907,
      "learning_rate": 4.069291252593644e-05,
      "loss": 2.8886,
      "step": 17050
    },
    {
      "epoch": 1.8630555858905755,
      "grad_norm": 0.15530411899089813,
      "learning_rate": 4.068745222234356e-05,
      "loss": 2.8655,
      "step": 17060
    },
    {
      "epoch": 1.8641476466091516,
      "grad_norm": 0.24512505531311035,
      "learning_rate": 4.068199191875068e-05,
      "loss": 2.863,
      "step": 17070
    },
    {
      "epoch": 1.8652397073277274,
      "grad_norm": 0.26572704315185547,
      "learning_rate": 4.06765316151578e-05,
      "loss": 2.8735,
      "step": 17080
    },
    {
      "epoch": 1.8663317680463034,
      "grad_norm": 0.41229575872421265,
      "learning_rate": 4.067107131156492e-05,
      "loss": 2.8879,
      "step": 17090
    },
    {
      "epoch": 1.8674238287648794,
      "grad_norm": 0.09363182634115219,
      "learning_rate": 4.0665611007972046e-05,
      "loss": 2.8246,
      "step": 17100
    },
    {
      "epoch": 1.8685158894834553,
      "grad_norm": 0.14641515910625458,
      "learning_rate": 4.0660150704379166e-05,
      "loss": 2.8554,
      "step": 17110
    },
    {
      "epoch": 1.869607950202031,
      "grad_norm": 0.2820383310317993,
      "learning_rate": 4.0654690400786285e-05,
      "loss": 2.8572,
      "step": 17120
    },
    {
      "epoch": 1.8707000109206073,
      "grad_norm": 0.1356891542673111,
      "learning_rate": 4.064923009719341e-05,
      "loss": 2.8684,
      "step": 17130
    },
    {
      "epoch": 1.8717920716391832,
      "grad_norm": 0.4289934039115906,
      "learning_rate": 4.064376979360053e-05,
      "loss": 2.8505,
      "step": 17140
    },
    {
      "epoch": 1.872884132357759,
      "grad_norm": 0.11740920692682266,
      "learning_rate": 4.063830949000765e-05,
      "loss": 2.8751,
      "step": 17150
    },
    {
      "epoch": 1.873976193076335,
      "grad_norm": 0.1981692910194397,
      "learning_rate": 4.063284918641477e-05,
      "loss": 2.8869,
      "step": 17160
    },
    {
      "epoch": 1.875068253794911,
      "grad_norm": 0.849593997001648,
      "learning_rate": 4.062738888282189e-05,
      "loss": 2.8748,
      "step": 17170
    },
    {
      "epoch": 1.8761603145134869,
      "grad_norm": 0.3182450830936432,
      "learning_rate": 4.062192857922901e-05,
      "loss": 2.8626,
      "step": 17180
    },
    {
      "epoch": 1.877252375232063,
      "grad_norm": 0.15271443128585815,
      "learning_rate": 4.061646827563613e-05,
      "loss": 2.8486,
      "step": 17190
    },
    {
      "epoch": 1.878344435950639,
      "grad_norm": 0.2737586200237274,
      "learning_rate": 4.061100797204325e-05,
      "loss": 2.8618,
      "step": 17200
    },
    {
      "epoch": 1.8794364966692148,
      "grad_norm": 0.22215190529823303,
      "learning_rate": 4.0605547668450367e-05,
      "loss": 2.8501,
      "step": 17210
    },
    {
      "epoch": 1.8805285573877908,
      "grad_norm": 0.12108135223388672,
      "learning_rate": 4.0600087364857486e-05,
      "loss": 2.9074,
      "step": 17220
    },
    {
      "epoch": 1.8816206181063668,
      "grad_norm": 0.3073970377445221,
      "learning_rate": 4.0594627061264605e-05,
      "loss": 2.8536,
      "step": 17230
    },
    {
      "epoch": 1.8827126788249426,
      "grad_norm": 0.10917995870113373,
      "learning_rate": 4.0589166757671725e-05,
      "loss": 2.8714,
      "step": 17240
    },
    {
      "epoch": 1.8838047395435185,
      "grad_norm": 0.11119339615106583,
      "learning_rate": 4.058370645407885e-05,
      "loss": 2.8431,
      "step": 17250
    },
    {
      "epoch": 1.8848968002620947,
      "grad_norm": 0.3226933777332306,
      "learning_rate": 4.057824615048597e-05,
      "loss": 2.9222,
      "step": 17260
    },
    {
      "epoch": 1.8859888609806705,
      "grad_norm": 0.12560641765594482,
      "learning_rate": 4.057278584689309e-05,
      "loss": 2.8669,
      "step": 17270
    },
    {
      "epoch": 1.8870809216992463,
      "grad_norm": 0.15212206542491913,
      "learning_rate": 4.056732554330021e-05,
      "loss": 2.8444,
      "step": 17280
    },
    {
      "epoch": 1.8881729824178224,
      "grad_norm": 0.6317272782325745,
      "learning_rate": 4.056186523970733e-05,
      "loss": 2.8422,
      "step": 17290
    },
    {
      "epoch": 1.8892650431363984,
      "grad_norm": 0.24239441752433777,
      "learning_rate": 4.055640493611445e-05,
      "loss": 3.0123,
      "step": 17300
    },
    {
      "epoch": 1.8903571038549742,
      "grad_norm": 0.4419422149658203,
      "learning_rate": 4.055094463252157e-05,
      "loss": 2.8646,
      "step": 17310
    },
    {
      "epoch": 1.8914491645735503,
      "grad_norm": 0.23270724713802338,
      "learning_rate": 4.054548432892869e-05,
      "loss": 2.8861,
      "step": 17320
    },
    {
      "epoch": 1.8925412252921263,
      "grad_norm": 0.710913896560669,
      "learning_rate": 4.054002402533581e-05,
      "loss": 2.887,
      "step": 17330
    },
    {
      "epoch": 1.8936332860107021,
      "grad_norm": 0.20632648468017578,
      "learning_rate": 4.053456372174293e-05,
      "loss": 2.849,
      "step": 17340
    },
    {
      "epoch": 1.8947253467292782,
      "grad_norm": 0.11288673430681229,
      "learning_rate": 4.052910341815005e-05,
      "loss": 2.8378,
      "step": 17350
    },
    {
      "epoch": 1.8958174074478542,
      "grad_norm": 0.08933773636817932,
      "learning_rate": 4.052364311455717e-05,
      "loss": 2.8348,
      "step": 17360
    },
    {
      "epoch": 1.89690946816643,
      "grad_norm": 0.4436459243297577,
      "learning_rate": 4.051818281096429e-05,
      "loss": 2.8921,
      "step": 17370
    },
    {
      "epoch": 1.898001528885006,
      "grad_norm": 0.09722032397985458,
      "learning_rate": 4.051272250737142e-05,
      "loss": 2.846,
      "step": 17380
    },
    {
      "epoch": 1.899093589603582,
      "grad_norm": 0.10214578360319138,
      "learning_rate": 4.0507262203778536e-05,
      "loss": 2.865,
      "step": 17390
    },
    {
      "epoch": 1.900185650322158,
      "grad_norm": 0.13489393889904022,
      "learning_rate": 4.0501801900185656e-05,
      "loss": 2.8474,
      "step": 17400
    },
    {
      "epoch": 1.9012777110407337,
      "grad_norm": 0.2598664164543152,
      "learning_rate": 4.0496341596592775e-05,
      "loss": 2.8571,
      "step": 17410
    },
    {
      "epoch": 1.9023697717593098,
      "grad_norm": 0.1548607051372528,
      "learning_rate": 4.0490881292999894e-05,
      "loss": 2.868,
      "step": 17420
    },
    {
      "epoch": 1.9034618324778858,
      "grad_norm": 0.20895667374134064,
      "learning_rate": 4.0485420989407014e-05,
      "loss": 2.8674,
      "step": 17430
    },
    {
      "epoch": 1.9045538931964616,
      "grad_norm": 0.15893633663654327,
      "learning_rate": 4.047996068581413e-05,
      "loss": 3.0166,
      "step": 17440
    },
    {
      "epoch": 1.9056459539150377,
      "grad_norm": 0.3951389491558075,
      "learning_rate": 4.047450038222125e-05,
      "loss": 2.8735,
      "step": 17450
    },
    {
      "epoch": 1.9067380146336137,
      "grad_norm": 0.17577162384986877,
      "learning_rate": 4.046904007862837e-05,
      "loss": 2.8554,
      "step": 17460
    },
    {
      "epoch": 1.9078300753521895,
      "grad_norm": 0.2517333924770355,
      "learning_rate": 4.046357977503549e-05,
      "loss": 2.8712,
      "step": 17470
    },
    {
      "epoch": 1.9089221360707656,
      "grad_norm": 0.15780405700206757,
      "learning_rate": 4.045811947144261e-05,
      "loss": 2.8666,
      "step": 17480
    },
    {
      "epoch": 1.9100141967893416,
      "grad_norm": 0.2165547013282776,
      "learning_rate": 4.045265916784973e-05,
      "loss": 2.8692,
      "step": 17490
    },
    {
      "epoch": 1.9111062575079174,
      "grad_norm": 0.30474409461021423,
      "learning_rate": 4.044719886425685e-05,
      "loss": 2.8653,
      "step": 17500
    },
    {
      "epoch": 1.9121983182264934,
      "grad_norm": 0.3127927780151367,
      "learning_rate": 4.0441738560663976e-05,
      "loss": 2.9056,
      "step": 17510
    },
    {
      "epoch": 1.9132903789450695,
      "grad_norm": 0.15207017958164215,
      "learning_rate": 4.0436278257071095e-05,
      "loss": 2.8751,
      "step": 17520
    },
    {
      "epoch": 1.9143824396636453,
      "grad_norm": 0.0984334647655487,
      "learning_rate": 4.0430817953478215e-05,
      "loss": 2.9032,
      "step": 17530
    },
    {
      "epoch": 1.9154745003822211,
      "grad_norm": 0.1862078309059143,
      "learning_rate": 4.0425357649885334e-05,
      "loss": 2.8759,
      "step": 17540
    },
    {
      "epoch": 1.9165665611007972,
      "grad_norm": 0.28577518463134766,
      "learning_rate": 4.0419897346292454e-05,
      "loss": 2.8782,
      "step": 17550
    },
    {
      "epoch": 1.9176586218193732,
      "grad_norm": 0.1232382133603096,
      "learning_rate": 4.041443704269958e-05,
      "loss": 2.8642,
      "step": 17560
    },
    {
      "epoch": 1.918750682537949,
      "grad_norm": 0.12314548343420029,
      "learning_rate": 4.04089767391067e-05,
      "loss": 2.853,
      "step": 17570
    },
    {
      "epoch": 1.919842743256525,
      "grad_norm": 0.13530583679676056,
      "learning_rate": 4.040351643551382e-05,
      "loss": 2.8482,
      "step": 17580
    },
    {
      "epoch": 1.920934803975101,
      "grad_norm": 0.16236402094364166,
      "learning_rate": 4.039805613192094e-05,
      "loss": 2.8371,
      "step": 17590
    },
    {
      "epoch": 1.922026864693677,
      "grad_norm": 0.1532345861196518,
      "learning_rate": 4.039259582832806e-05,
      "loss": 2.9262,
      "step": 17600
    },
    {
      "epoch": 1.923118925412253,
      "grad_norm": 0.16402126848697662,
      "learning_rate": 4.038713552473518e-05,
      "loss": 2.9401,
      "step": 17610
    },
    {
      "epoch": 1.924210986130829,
      "grad_norm": 0.2449306845664978,
      "learning_rate": 4.0381675221142296e-05,
      "loss": 2.8482,
      "step": 17620
    },
    {
      "epoch": 1.9253030468494048,
      "grad_norm": 0.17750093340873718,
      "learning_rate": 4.0376214917549416e-05,
      "loss": 2.8845,
      "step": 17630
    },
    {
      "epoch": 1.9263951075679808,
      "grad_norm": 0.33387529850006104,
      "learning_rate": 4.037075461395654e-05,
      "loss": 2.8697,
      "step": 17640
    },
    {
      "epoch": 1.9274871682865569,
      "grad_norm": 0.3971046805381775,
      "learning_rate": 4.036529431036366e-05,
      "loss": 2.834,
      "step": 17650
    },
    {
      "epoch": 1.9285792290051327,
      "grad_norm": 0.23430146276950836,
      "learning_rate": 4.035983400677078e-05,
      "loss": 2.8853,
      "step": 17660
    },
    {
      "epoch": 1.9296712897237085,
      "grad_norm": 0.19627739489078522,
      "learning_rate": 4.03543737031779e-05,
      "loss": 2.8736,
      "step": 17670
    },
    {
      "epoch": 1.9307633504422848,
      "grad_norm": 0.21854661405086517,
      "learning_rate": 4.034891339958502e-05,
      "loss": 2.8528,
      "step": 17680
    },
    {
      "epoch": 1.9318554111608606,
      "grad_norm": 0.42759472131729126,
      "learning_rate": 4.034345309599214e-05,
      "loss": 2.8542,
      "step": 17690
    },
    {
      "epoch": 1.9329474718794364,
      "grad_norm": 0.2778475284576416,
      "learning_rate": 4.033799279239926e-05,
      "loss": 2.8506,
      "step": 17700
    },
    {
      "epoch": 1.9340395325980124,
      "grad_norm": 0.7531521320343018,
      "learning_rate": 4.033253248880638e-05,
      "loss": 3.0257,
      "step": 17710
    },
    {
      "epoch": 1.9351315933165885,
      "grad_norm": 0.18898381292819977,
      "learning_rate": 4.03270721852135e-05,
      "loss": 2.8412,
      "step": 17720
    },
    {
      "epoch": 1.9362236540351643,
      "grad_norm": 0.7329813241958618,
      "learning_rate": 4.0321611881620617e-05,
      "loss": 2.866,
      "step": 17730
    },
    {
      "epoch": 1.9373157147537403,
      "grad_norm": 0.36406850814819336,
      "learning_rate": 4.0316151578027736e-05,
      "loss": 2.8624,
      "step": 17740
    },
    {
      "epoch": 1.9384077754723164,
      "grad_norm": 0.16637636721134186,
      "learning_rate": 4.0310691274434855e-05,
      "loss": 2.8663,
      "step": 17750
    },
    {
      "epoch": 1.9394998361908922,
      "grad_norm": 0.3077593445777893,
      "learning_rate": 4.030523097084198e-05,
      "loss": 2.884,
      "step": 17760
    },
    {
      "epoch": 1.9405918969094682,
      "grad_norm": 0.17477913200855255,
      "learning_rate": 4.02997706672491e-05,
      "loss": 2.8485,
      "step": 17770
    },
    {
      "epoch": 1.9416839576280442,
      "grad_norm": 0.21960721909999847,
      "learning_rate": 4.029431036365622e-05,
      "loss": 2.8517,
      "step": 17780
    },
    {
      "epoch": 1.94277601834662,
      "grad_norm": 0.22100190818309784,
      "learning_rate": 4.0288850060063347e-05,
      "loss": 2.8301,
      "step": 17790
    },
    {
      "epoch": 1.9438680790651959,
      "grad_norm": 0.1554649919271469,
      "learning_rate": 4.0283389756470466e-05,
      "loss": 2.8693,
      "step": 17800
    },
    {
      "epoch": 1.9449601397837721,
      "grad_norm": 0.15774446725845337,
      "learning_rate": 4.0277929452877585e-05,
      "loss": 2.864,
      "step": 17810
    },
    {
      "epoch": 1.946052200502348,
      "grad_norm": 0.9082684516906738,
      "learning_rate": 4.0272469149284705e-05,
      "loss": 2.9291,
      "step": 17820
    },
    {
      "epoch": 1.9471442612209238,
      "grad_norm": 0.2843514084815979,
      "learning_rate": 4.0267008845691824e-05,
      "loss": 2.891,
      "step": 17830
    },
    {
      "epoch": 1.9482363219394998,
      "grad_norm": 0.13071323931217194,
      "learning_rate": 4.0261548542098944e-05,
      "loss": 2.8409,
      "step": 17840
    },
    {
      "epoch": 1.9493283826580758,
      "grad_norm": 0.19548094272613525,
      "learning_rate": 4.025608823850606e-05,
      "loss": 2.873,
      "step": 17850
    },
    {
      "epoch": 1.9504204433766517,
      "grad_norm": 0.1521691381931305,
      "learning_rate": 4.025062793491318e-05,
      "loss": 2.8625,
      "step": 17860
    },
    {
      "epoch": 1.9515125040952277,
      "grad_norm": 0.33964914083480835,
      "learning_rate": 4.02451676313203e-05,
      "loss": 2.8567,
      "step": 17870
    },
    {
      "epoch": 1.9526045648138037,
      "grad_norm": 0.1815594732761383,
      "learning_rate": 4.023970732772742e-05,
      "loss": 2.9053,
      "step": 17880
    },
    {
      "epoch": 1.9536966255323795,
      "grad_norm": 0.152192160487175,
      "learning_rate": 4.023424702413454e-05,
      "loss": 2.8651,
      "step": 17890
    },
    {
      "epoch": 1.9547886862509556,
      "grad_norm": 0.21420706808567047,
      "learning_rate": 4.022878672054166e-05,
      "loss": 2.8694,
      "step": 17900
    },
    {
      "epoch": 1.9558807469695316,
      "grad_norm": 0.6125242114067078,
      "learning_rate": 4.0223326416948786e-05,
      "loss": 2.8849,
      "step": 17910
    },
    {
      "epoch": 1.9569728076881074,
      "grad_norm": 0.15933704376220703,
      "learning_rate": 4.0217866113355906e-05,
      "loss": 2.8457,
      "step": 17920
    },
    {
      "epoch": 1.9580648684066833,
      "grad_norm": 0.21239881217479706,
      "learning_rate": 4.0212405809763025e-05,
      "loss": 2.8426,
      "step": 17930
    },
    {
      "epoch": 1.9591569291252595,
      "grad_norm": 0.2851305305957794,
      "learning_rate": 4.0206945506170144e-05,
      "loss": 2.8985,
      "step": 17940
    },
    {
      "epoch": 1.9602489898438353,
      "grad_norm": 0.10742322355508804,
      "learning_rate": 4.0201485202577264e-05,
      "loss": 2.8737,
      "step": 17950
    },
    {
      "epoch": 1.9613410505624111,
      "grad_norm": 0.3445069193840027,
      "learning_rate": 4.019602489898438e-05,
      "loss": 2.8688,
      "step": 17960
    },
    {
      "epoch": 1.9624331112809872,
      "grad_norm": 0.7664239406585693,
      "learning_rate": 4.01905645953915e-05,
      "loss": 2.8561,
      "step": 17970
    },
    {
      "epoch": 1.9635251719995632,
      "grad_norm": 0.16407392919063568,
      "learning_rate": 4.018510429179862e-05,
      "loss": 2.8567,
      "step": 17980
    },
    {
      "epoch": 1.964617232718139,
      "grad_norm": 0.16676518321037292,
      "learning_rate": 4.017964398820575e-05,
      "loss": 2.8763,
      "step": 17990
    },
    {
      "epoch": 1.965709293436715,
      "grad_norm": 0.09516854584217072,
      "learning_rate": 4.017418368461287e-05,
      "loss": 2.8282,
      "step": 18000
    },
    {
      "epoch": 1.9668013541552911,
      "grad_norm": 0.28620657324790955,
      "learning_rate": 4.016872338101999e-05,
      "loss": 2.8345,
      "step": 18010
    },
    {
      "epoch": 1.967893414873867,
      "grad_norm": 0.4646763205528259,
      "learning_rate": 4.0163263077427106e-05,
      "loss": 2.8517,
      "step": 18020
    },
    {
      "epoch": 1.968985475592443,
      "grad_norm": 0.6907607913017273,
      "learning_rate": 4.0157802773834226e-05,
      "loss": 2.8632,
      "step": 18030
    },
    {
      "epoch": 1.970077536311019,
      "grad_norm": 0.17825816571712494,
      "learning_rate": 4.015234247024135e-05,
      "loss": 2.8335,
      "step": 18040
    },
    {
      "epoch": 1.9711695970295948,
      "grad_norm": 0.8751311898231506,
      "learning_rate": 4.014688216664847e-05,
      "loss": 2.9534,
      "step": 18050
    },
    {
      "epoch": 1.9722616577481706,
      "grad_norm": 0.13915930688381195,
      "learning_rate": 4.014142186305559e-05,
      "loss": 2.8676,
      "step": 18060
    },
    {
      "epoch": 1.973353718466747,
      "grad_norm": 0.25821733474731445,
      "learning_rate": 4.013596155946271e-05,
      "loss": 2.988,
      "step": 18070
    },
    {
      "epoch": 1.9744457791853227,
      "grad_norm": 0.16618873178958893,
      "learning_rate": 4.013050125586983e-05,
      "loss": 2.8978,
      "step": 18080
    },
    {
      "epoch": 1.9755378399038985,
      "grad_norm": 0.25467246770858765,
      "learning_rate": 4.012504095227695e-05,
      "loss": 2.8813,
      "step": 18090
    },
    {
      "epoch": 1.9766299006224746,
      "grad_norm": 0.20572952926158905,
      "learning_rate": 4.011958064868407e-05,
      "loss": 2.8624,
      "step": 18100
    },
    {
      "epoch": 1.9777219613410506,
      "grad_norm": 0.19741806387901306,
      "learning_rate": 4.0114666375450475e-05,
      "loss": 2.9278,
      "step": 18110
    },
    {
      "epoch": 1.9788140220596264,
      "grad_norm": 0.29623258113861084,
      "learning_rate": 4.0109206071857595e-05,
      "loss": 2.8691,
      "step": 18120
    },
    {
      "epoch": 1.9799060827782025,
      "grad_norm": 0.36614757776260376,
      "learning_rate": 4.0103745768264714e-05,
      "loss": 2.901,
      "step": 18130
    },
    {
      "epoch": 1.9809981434967785,
      "grad_norm": 0.17100182175636292,
      "learning_rate": 4.0098285464671834e-05,
      "loss": 2.8869,
      "step": 18140
    },
    {
      "epoch": 1.9820902042153543,
      "grad_norm": 0.6381725668907166,
      "learning_rate": 4.009282516107895e-05,
      "loss": 2.8967,
      "step": 18150
    },
    {
      "epoch": 1.9831822649339303,
      "grad_norm": 0.5551803708076477,
      "learning_rate": 4.008736485748608e-05,
      "loss": 2.8611,
      "step": 18160
    },
    {
      "epoch": 1.9842743256525064,
      "grad_norm": 0.08370386809110641,
      "learning_rate": 4.00819045538932e-05,
      "loss": 2.8857,
      "step": 18170
    },
    {
      "epoch": 1.9853663863710822,
      "grad_norm": 0.11804277449846268,
      "learning_rate": 4.007644425030032e-05,
      "loss": 2.8638,
      "step": 18180
    },
    {
      "epoch": 1.986458447089658,
      "grad_norm": 0.19456416368484497,
      "learning_rate": 4.0070983946707444e-05,
      "loss": 2.8957,
      "step": 18190
    },
    {
      "epoch": 1.9875505078082343,
      "grad_norm": 0.1405353844165802,
      "learning_rate": 4.0065523643114564e-05,
      "loss": 2.8434,
      "step": 18200
    },
    {
      "epoch": 1.98864256852681,
      "grad_norm": 0.29026728868484497,
      "learning_rate": 4.006006333952168e-05,
      "loss": 2.8888,
      "step": 18210
    },
    {
      "epoch": 1.989734629245386,
      "grad_norm": 0.11745936423540115,
      "learning_rate": 4.00546030359288e-05,
      "loss": 2.854,
      "step": 18220
    },
    {
      "epoch": 1.990826689963962,
      "grad_norm": 0.3690474331378937,
      "learning_rate": 4.004914273233592e-05,
      "loss": 2.8666,
      "step": 18230
    },
    {
      "epoch": 1.991918750682538,
      "grad_norm": 0.26463326811790466,
      "learning_rate": 4.004368242874304e-05,
      "loss": 2.9471,
      "step": 18240
    },
    {
      "epoch": 1.9930108114011138,
      "grad_norm": 0.2417762726545334,
      "learning_rate": 4.003822212515016e-05,
      "loss": 2.8445,
      "step": 18250
    },
    {
      "epoch": 1.9941028721196898,
      "grad_norm": 0.12164682894945145,
      "learning_rate": 4.003276182155728e-05,
      "loss": 2.8318,
      "step": 18260
    },
    {
      "epoch": 1.9951949328382659,
      "grad_norm": 0.1846432387828827,
      "learning_rate": 4.00273015179644e-05,
      "loss": 2.8545,
      "step": 18270
    },
    {
      "epoch": 1.9962869935568417,
      "grad_norm": 0.31536269187927246,
      "learning_rate": 4.002184121437152e-05,
      "loss": 2.8721,
      "step": 18280
    },
    {
      "epoch": 1.9973790542754177,
      "grad_norm": 0.5234287977218628,
      "learning_rate": 4.001638091077864e-05,
      "loss": 2.8553,
      "step": 18290
    },
    {
      "epoch": 1.9984711149939938,
      "grad_norm": 0.15084592998027802,
      "learning_rate": 4.001092060718576e-05,
      "loss": 2.8654,
      "step": 18300
    },
    {
      "epoch": 1.9995631757125696,
      "grad_norm": 0.24727734923362732,
      "learning_rate": 4.0005460303592884e-05,
      "loss": 2.9551,
      "step": 18310
    },
    {
      "epoch": 2.0006552364311454,
      "grad_norm": 0.4811433255672455,
      "learning_rate": 4e-05,
      "loss": 2.8566,
      "step": 18320
    },
    {
      "epoch": 2.0017472971497217,
      "grad_norm": 0.28431326150894165,
      "learning_rate": 3.999453969640712e-05,
      "loss": 2.9087,
      "step": 18330
    },
    {
      "epoch": 2.0028393578682975,
      "grad_norm": 0.13982008397579193,
      "learning_rate": 3.998907939281424e-05,
      "loss": 2.9086,
      "step": 18340
    },
    {
      "epoch": 2.0039314185868733,
      "grad_norm": 0.18060517311096191,
      "learning_rate": 3.998361908922136e-05,
      "loss": 2.9008,
      "step": 18350
    },
    {
      "epoch": 2.0050234793054496,
      "grad_norm": 0.5179727077484131,
      "learning_rate": 3.997815878562848e-05,
      "loss": 2.8794,
      "step": 18360
    },
    {
      "epoch": 2.0061155400240254,
      "grad_norm": 0.1312413364648819,
      "learning_rate": 3.99726984820356e-05,
      "loss": 2.8868,
      "step": 18370
    },
    {
      "epoch": 2.007207600742601,
      "grad_norm": 0.19359897077083588,
      "learning_rate": 3.996723817844272e-05,
      "loss": 2.84,
      "step": 18380
    },
    {
      "epoch": 2.0082996614611774,
      "grad_norm": 0.15013588964939117,
      "learning_rate": 3.9961777874849846e-05,
      "loss": 2.8962,
      "step": 18390
    },
    {
      "epoch": 2.0093917221797533,
      "grad_norm": 0.24082845449447632,
      "learning_rate": 3.9956317571256965e-05,
      "loss": 2.8737,
      "step": 18400
    },
    {
      "epoch": 2.010483782898329,
      "grad_norm": 0.18201051652431488,
      "learning_rate": 3.9950857267664085e-05,
      "loss": 2.8535,
      "step": 18410
    },
    {
      "epoch": 2.011575843616905,
      "grad_norm": 0.33427008986473083,
      "learning_rate": 3.9945396964071204e-05,
      "loss": 2.8731,
      "step": 18420
    },
    {
      "epoch": 2.012667904335481,
      "grad_norm": 0.17511117458343506,
      "learning_rate": 3.9939936660478324e-05,
      "loss": 2.8496,
      "step": 18430
    },
    {
      "epoch": 2.013759965054057,
      "grad_norm": 0.33731237053871155,
      "learning_rate": 3.993447635688545e-05,
      "loss": 2.9492,
      "step": 18440
    },
    {
      "epoch": 2.014852025772633,
      "grad_norm": 0.24273458123207092,
      "learning_rate": 3.992901605329257e-05,
      "loss": 2.8882,
      "step": 18450
    },
    {
      "epoch": 2.015944086491209,
      "grad_norm": 0.12167495489120483,
      "learning_rate": 3.992355574969969e-05,
      "loss": 2.8859,
      "step": 18460
    },
    {
      "epoch": 2.017036147209785,
      "grad_norm": 0.2293728142976761,
      "learning_rate": 3.991809544610681e-05,
      "loss": 2.8648,
      "step": 18470
    },
    {
      "epoch": 2.0181282079283607,
      "grad_norm": 0.13049475848674774,
      "learning_rate": 3.991263514251393e-05,
      "loss": 2.8944,
      "step": 18480
    },
    {
      "epoch": 2.019220268646937,
      "grad_norm": 0.14807969331741333,
      "learning_rate": 3.990717483892105e-05,
      "loss": 2.8434,
      "step": 18490
    },
    {
      "epoch": 2.0203123293655127,
      "grad_norm": 0.30719465017318726,
      "learning_rate": 3.9901714535328166e-05,
      "loss": 2.8729,
      "step": 18500
    },
    {
      "epoch": 2.0214043900840886,
      "grad_norm": 0.16287899017333984,
      "learning_rate": 3.9896254231735286e-05,
      "loss": 2.8673,
      "step": 18510
    },
    {
      "epoch": 2.022496450802665,
      "grad_norm": 0.3429138660430908,
      "learning_rate": 3.9890793928142405e-05,
      "loss": 2.9299,
      "step": 18520
    },
    {
      "epoch": 2.0235885115212406,
      "grad_norm": 0.3026644289493561,
      "learning_rate": 3.9885333624549524e-05,
      "loss": 2.8569,
      "step": 18530
    },
    {
      "epoch": 2.0246805722398165,
      "grad_norm": 0.21457844972610474,
      "learning_rate": 3.9879873320956644e-05,
      "loss": 2.8631,
      "step": 18540
    },
    {
      "epoch": 2.0257726329583923,
      "grad_norm": 0.384389191865921,
      "learning_rate": 3.987441301736376e-05,
      "loss": 2.8523,
      "step": 18550
    },
    {
      "epoch": 2.0268646936769685,
      "grad_norm": 0.12562328577041626,
      "learning_rate": 3.986895271377088e-05,
      "loss": 2.8287,
      "step": 18560
    },
    {
      "epoch": 2.0279567543955443,
      "grad_norm": 0.14840330183506012,
      "learning_rate": 3.986349241017801e-05,
      "loss": 2.8626,
      "step": 18570
    },
    {
      "epoch": 2.02904881511412,
      "grad_norm": 0.4045800566673279,
      "learning_rate": 3.985803210658513e-05,
      "loss": 2.8528,
      "step": 18580
    },
    {
      "epoch": 2.0301408758326964,
      "grad_norm": 0.12146452814340591,
      "learning_rate": 3.985257180299225e-05,
      "loss": 2.8729,
      "step": 18590
    },
    {
      "epoch": 2.0312329365512722,
      "grad_norm": 0.10762841254472733,
      "learning_rate": 3.984711149939937e-05,
      "loss": 2.8803,
      "step": 18600
    },
    {
      "epoch": 2.032324997269848,
      "grad_norm": 0.27344149351119995,
      "learning_rate": 3.9841651195806486e-05,
      "loss": 2.8801,
      "step": 18610
    },
    {
      "epoch": 2.0334170579884243,
      "grad_norm": 0.19287735223770142,
      "learning_rate": 3.983619089221361e-05,
      "loss": 2.8641,
      "step": 18620
    },
    {
      "epoch": 2.034509118707,
      "grad_norm": 0.3771539032459259,
      "learning_rate": 3.983073058862073e-05,
      "loss": 2.8769,
      "step": 18630
    },
    {
      "epoch": 2.035601179425576,
      "grad_norm": 0.12034959346055984,
      "learning_rate": 3.982527028502785e-05,
      "loss": 2.8324,
      "step": 18640
    },
    {
      "epoch": 2.036693240144152,
      "grad_norm": 0.1587134748697281,
      "learning_rate": 3.981980998143497e-05,
      "loss": 2.8963,
      "step": 18650
    },
    {
      "epoch": 2.037785300862728,
      "grad_norm": 0.2794744074344635,
      "learning_rate": 3.981434967784209e-05,
      "loss": 2.8277,
      "step": 18660
    },
    {
      "epoch": 2.038877361581304,
      "grad_norm": 0.1726524531841278,
      "learning_rate": 3.980888937424921e-05,
      "loss": 2.8735,
      "step": 18670
    },
    {
      "epoch": 2.0399694222998797,
      "grad_norm": 0.1608765572309494,
      "learning_rate": 3.980342907065633e-05,
      "loss": 2.9963,
      "step": 18680
    },
    {
      "epoch": 2.041061483018456,
      "grad_norm": 0.1768808215856552,
      "learning_rate": 3.979796876706345e-05,
      "loss": 2.951,
      "step": 18690
    },
    {
      "epoch": 2.0421535437370317,
      "grad_norm": 0.22164779901504517,
      "learning_rate": 3.9792508463470575e-05,
      "loss": 2.8558,
      "step": 18700
    },
    {
      "epoch": 2.0432456044556075,
      "grad_norm": 0.2202673852443695,
      "learning_rate": 3.9787048159877694e-05,
      "loss": 2.869,
      "step": 18710
    },
    {
      "epoch": 2.044337665174184,
      "grad_norm": 0.10367890447378159,
      "learning_rate": 3.9781587856284814e-05,
      "loss": 2.8636,
      "step": 18720
    },
    {
      "epoch": 2.0454297258927596,
      "grad_norm": 0.22299480438232422,
      "learning_rate": 3.977612755269193e-05,
      "loss": 2.8863,
      "step": 18730
    },
    {
      "epoch": 2.0465217866113354,
      "grad_norm": 0.26002568006515503,
      "learning_rate": 3.977066724909905e-05,
      "loss": 2.8523,
      "step": 18740
    },
    {
      "epoch": 2.0476138473299117,
      "grad_norm": 0.1728794425725937,
      "learning_rate": 3.976520694550617e-05,
      "loss": 2.8754,
      "step": 18750
    },
    {
      "epoch": 2.0487059080484875,
      "grad_norm": 0.13829441368579865,
      "learning_rate": 3.975974664191329e-05,
      "loss": 2.8817,
      "step": 18760
    },
    {
      "epoch": 2.0497979687670633,
      "grad_norm": 0.18286289274692535,
      "learning_rate": 3.975428633832041e-05,
      "loss": 2.8838,
      "step": 18770
    },
    {
      "epoch": 2.0508900294856396,
      "grad_norm": 0.771540641784668,
      "learning_rate": 3.974882603472753e-05,
      "loss": 2.883,
      "step": 18780
    },
    {
      "epoch": 2.0519820902042154,
      "grad_norm": 0.14450474083423615,
      "learning_rate": 3.974336573113465e-05,
      "loss": 2.8664,
      "step": 18790
    },
    {
      "epoch": 2.053074150922791,
      "grad_norm": 0.21816515922546387,
      "learning_rate": 3.973790542754177e-05,
      "loss": 2.8698,
      "step": 18800
    },
    {
      "epoch": 2.054166211641367,
      "grad_norm": 0.4075011610984802,
      "learning_rate": 3.973244512394889e-05,
      "loss": 2.8479,
      "step": 18810
    },
    {
      "epoch": 2.0552582723599433,
      "grad_norm": 0.24883684515953064,
      "learning_rate": 3.9726984820356014e-05,
      "loss": 2.8972,
      "step": 18820
    },
    {
      "epoch": 2.056350333078519,
      "grad_norm": 0.19551849365234375,
      "learning_rate": 3.9721524516763134e-05,
      "loss": 2.8693,
      "step": 18830
    },
    {
      "epoch": 2.057442393797095,
      "grad_norm": 0.12012771517038345,
      "learning_rate": 3.971606421317025e-05,
      "loss": 2.8658,
      "step": 18840
    },
    {
      "epoch": 2.058534454515671,
      "grad_norm": 0.2488173395395279,
      "learning_rate": 3.971060390957738e-05,
      "loss": 2.8549,
      "step": 18850
    },
    {
      "epoch": 2.059626515234247,
      "grad_norm": 0.10983403027057648,
      "learning_rate": 3.97051436059845e-05,
      "loss": 2.8585,
      "step": 18860
    },
    {
      "epoch": 2.060718575952823,
      "grad_norm": 0.5419952273368835,
      "learning_rate": 3.969968330239162e-05,
      "loss": 2.8663,
      "step": 18870
    },
    {
      "epoch": 2.061810636671399,
      "grad_norm": 0.16226767003536224,
      "learning_rate": 3.969422299879874e-05,
      "loss": 2.8668,
      "step": 18880
    },
    {
      "epoch": 2.062902697389975,
      "grad_norm": 0.2344493716955185,
      "learning_rate": 3.968876269520586e-05,
      "loss": 2.8619,
      "step": 18890
    },
    {
      "epoch": 2.0639947581085507,
      "grad_norm": 0.16675125062465668,
      "learning_rate": 3.9683302391612976e-05,
      "loss": 2.8452,
      "step": 18900
    },
    {
      "epoch": 2.065086818827127,
      "grad_norm": 0.3063409626483917,
      "learning_rate": 3.9677842088020096e-05,
      "loss": 2.8741,
      "step": 18910
    },
    {
      "epoch": 2.066178879545703,
      "grad_norm": 0.12210247665643692,
      "learning_rate": 3.9672381784427215e-05,
      "loss": 2.8475,
      "step": 18920
    },
    {
      "epoch": 2.0672709402642786,
      "grad_norm": 0.2358987182378769,
      "learning_rate": 3.9666921480834335e-05,
      "loss": 2.9227,
      "step": 18930
    },
    {
      "epoch": 2.068363000982855,
      "grad_norm": 0.16686870157718658,
      "learning_rate": 3.9661461177241454e-05,
      "loss": 2.8784,
      "step": 18940
    },
    {
      "epoch": 2.0694550617014307,
      "grad_norm": 0.10894770175218582,
      "learning_rate": 3.9656000873648574e-05,
      "loss": 2.8389,
      "step": 18950
    },
    {
      "epoch": 2.0705471224200065,
      "grad_norm": 0.20173612236976624,
      "learning_rate": 3.965054057005569e-05,
      "loss": 2.9331,
      "step": 18960
    },
    {
      "epoch": 2.0716391831385823,
      "grad_norm": 0.2873218357563019,
      "learning_rate": 3.964508026646282e-05,
      "loss": 2.8644,
      "step": 18970
    },
    {
      "epoch": 2.0727312438571586,
      "grad_norm": 0.1462690681219101,
      "learning_rate": 3.963961996286994e-05,
      "loss": 2.8505,
      "step": 18980
    },
    {
      "epoch": 2.0738233045757344,
      "grad_norm": 0.25635606050491333,
      "learning_rate": 3.963415965927706e-05,
      "loss": 2.8756,
      "step": 18990
    },
    {
      "epoch": 2.07491536529431,
      "grad_norm": 0.17625540494918823,
      "learning_rate": 3.962869935568418e-05,
      "loss": 2.8323,
      "step": 19000
    },
    {
      "epoch": 2.0760074260128865,
      "grad_norm": 0.20882967114448547,
      "learning_rate": 3.96232390520913e-05,
      "loss": 2.8389,
      "step": 19010
    },
    {
      "epoch": 2.0770994867314623,
      "grad_norm": 0.2778509855270386,
      "learning_rate": 3.9617778748498416e-05,
      "loss": 2.8415,
      "step": 19020
    },
    {
      "epoch": 2.078191547450038,
      "grad_norm": 0.11287610232830048,
      "learning_rate": 3.9612318444905536e-05,
      "loss": 2.8686,
      "step": 19030
    },
    {
      "epoch": 2.0792836081686144,
      "grad_norm": 0.13942240178585052,
      "learning_rate": 3.9606858141312655e-05,
      "loss": 2.8714,
      "step": 19040
    },
    {
      "epoch": 2.08037566888719,
      "grad_norm": 0.18876880407333374,
      "learning_rate": 3.960139783771978e-05,
      "loss": 2.8399,
      "step": 19050
    },
    {
      "epoch": 2.081467729605766,
      "grad_norm": 0.16019269824028015,
      "learning_rate": 3.95959375341269e-05,
      "loss": 2.8653,
      "step": 19060
    },
    {
      "epoch": 2.0825597903243422,
      "grad_norm": 0.11214390397071838,
      "learning_rate": 3.959047723053402e-05,
      "loss": 2.8451,
      "step": 19070
    },
    {
      "epoch": 2.083651851042918,
      "grad_norm": 0.11442186683416367,
      "learning_rate": 3.958501692694114e-05,
      "loss": 2.8323,
      "step": 19080
    },
    {
      "epoch": 2.084743911761494,
      "grad_norm": 0.3353339433670044,
      "learning_rate": 3.957955662334826e-05,
      "loss": 2.8603,
      "step": 19090
    },
    {
      "epoch": 2.0858359724800697,
      "grad_norm": 0.2522489130496979,
      "learning_rate": 3.9574096319755385e-05,
      "loss": 2.8663,
      "step": 19100
    },
    {
      "epoch": 2.086928033198646,
      "grad_norm": 0.578889787197113,
      "learning_rate": 3.9568636016162504e-05,
      "loss": 2.878,
      "step": 19110
    },
    {
      "epoch": 2.0880200939172218,
      "grad_norm": 0.22973783314228058,
      "learning_rate": 3.9563175712569624e-05,
      "loss": 2.867,
      "step": 19120
    },
    {
      "epoch": 2.0891121546357976,
      "grad_norm": 0.13738201558589935,
      "learning_rate": 3.955771540897674e-05,
      "loss": 2.849,
      "step": 19130
    },
    {
      "epoch": 2.090204215354374,
      "grad_norm": 0.11795928329229355,
      "learning_rate": 3.955225510538386e-05,
      "loss": 2.8711,
      "step": 19140
    },
    {
      "epoch": 2.0912962760729497,
      "grad_norm": 0.1519690304994583,
      "learning_rate": 3.954679480179098e-05,
      "loss": 2.8741,
      "step": 19150
    },
    {
      "epoch": 2.0923883367915255,
      "grad_norm": 0.2743294835090637,
      "learning_rate": 3.95413344981981e-05,
      "loss": 2.8639,
      "step": 19160
    },
    {
      "epoch": 2.0934803975101017,
      "grad_norm": 0.12891532480716705,
      "learning_rate": 3.953587419460522e-05,
      "loss": 2.8503,
      "step": 19170
    },
    {
      "epoch": 2.0945724582286775,
      "grad_norm": 0.18909619748592377,
      "learning_rate": 3.953041389101234e-05,
      "loss": 2.8499,
      "step": 19180
    },
    {
      "epoch": 2.0956645189472534,
      "grad_norm": 0.6773161888122559,
      "learning_rate": 3.952495358741946e-05,
      "loss": 2.9039,
      "step": 19190
    },
    {
      "epoch": 2.0967565796658296,
      "grad_norm": 0.16557566821575165,
      "learning_rate": 3.951949328382658e-05,
      "loss": 2.8537,
      "step": 19200
    },
    {
      "epoch": 2.0978486403844054,
      "grad_norm": 0.15873651206493378,
      "learning_rate": 3.95140329802337e-05,
      "loss": 2.8809,
      "step": 19210
    },
    {
      "epoch": 2.0989407011029813,
      "grad_norm": 0.1034960001707077,
      "learning_rate": 3.950857267664082e-05,
      "loss": 2.8742,
      "step": 19220
    },
    {
      "epoch": 2.100032761821557,
      "grad_norm": 0.24181994795799255,
      "learning_rate": 3.9503112373047944e-05,
      "loss": 2.8715,
      "step": 19230
    },
    {
      "epoch": 2.1011248225401333,
      "grad_norm": 0.17476779222488403,
      "learning_rate": 3.9497652069455064e-05,
      "loss": 2.8367,
      "step": 19240
    },
    {
      "epoch": 2.102216883258709,
      "grad_norm": 0.08870408684015274,
      "learning_rate": 3.949219176586218e-05,
      "loss": 2.8658,
      "step": 19250
    },
    {
      "epoch": 2.103308943977285,
      "grad_norm": 0.22367864847183228,
      "learning_rate": 3.94867314622693e-05,
      "loss": 2.8559,
      "step": 19260
    },
    {
      "epoch": 2.104401004695861,
      "grad_norm": 0.11268527060747147,
      "learning_rate": 3.948127115867642e-05,
      "loss": 2.8893,
      "step": 19270
    },
    {
      "epoch": 2.105493065414437,
      "grad_norm": 0.20073482394218445,
      "learning_rate": 3.947581085508355e-05,
      "loss": 2.8483,
      "step": 19280
    },
    {
      "epoch": 2.106585126133013,
      "grad_norm": 0.17572945356369019,
      "learning_rate": 3.947035055149067e-05,
      "loss": 2.8395,
      "step": 19290
    },
    {
      "epoch": 2.107677186851589,
      "grad_norm": 0.13653254508972168,
      "learning_rate": 3.946489024789779e-05,
      "loss": 2.8503,
      "step": 19300
    },
    {
      "epoch": 2.108769247570165,
      "grad_norm": 0.17861758172512054,
      "learning_rate": 3.9459429944304906e-05,
      "loss": 2.8935,
      "step": 19310
    },
    {
      "epoch": 2.1098613082887407,
      "grad_norm": 0.16337907314300537,
      "learning_rate": 3.9453969640712026e-05,
      "loss": 2.8704,
      "step": 19320
    },
    {
      "epoch": 2.110953369007317,
      "grad_norm": 0.14792963862419128,
      "learning_rate": 3.9448509337119145e-05,
      "loss": 2.8435,
      "step": 19330
    },
    {
      "epoch": 2.112045429725893,
      "grad_norm": 0.20340503752231598,
      "learning_rate": 3.9443049033526264e-05,
      "loss": 2.8623,
      "step": 19340
    },
    {
      "epoch": 2.1131374904444686,
      "grad_norm": 0.13867829740047455,
      "learning_rate": 3.9437588729933384e-05,
      "loss": 2.8558,
      "step": 19350
    },
    {
      "epoch": 2.1142295511630445,
      "grad_norm": 0.087694451212883,
      "learning_rate": 3.943212842634051e-05,
      "loss": 2.8448,
      "step": 19360
    },
    {
      "epoch": 2.1153216118816207,
      "grad_norm": 0.11658741533756256,
      "learning_rate": 3.942666812274763e-05,
      "loss": 2.8707,
      "step": 19370
    },
    {
      "epoch": 2.1164136726001965,
      "grad_norm": 0.09981957823038101,
      "learning_rate": 3.942120781915475e-05,
      "loss": 2.8558,
      "step": 19380
    },
    {
      "epoch": 2.1175057333187723,
      "grad_norm": 0.2295687049627304,
      "learning_rate": 3.941574751556187e-05,
      "loss": 2.9077,
      "step": 19390
    },
    {
      "epoch": 2.1185977940373486,
      "grad_norm": 0.1408669650554657,
      "learning_rate": 3.941028721196899e-05,
      "loss": 2.8553,
      "step": 19400
    },
    {
      "epoch": 2.1196898547559244,
      "grad_norm": 0.1368502676486969,
      "learning_rate": 3.940482690837611e-05,
      "loss": 2.8498,
      "step": 19410
    },
    {
      "epoch": 2.1207819154745002,
      "grad_norm": 0.14186349511146545,
      "learning_rate": 3.9399366604783226e-05,
      "loss": 2.8629,
      "step": 19420
    },
    {
      "epoch": 2.1218739761930765,
      "grad_norm": 0.12142197042703629,
      "learning_rate": 3.9393906301190346e-05,
      "loss": 2.8384,
      "step": 19430
    },
    {
      "epoch": 2.1229660369116523,
      "grad_norm": 0.13623037934303284,
      "learning_rate": 3.9388445997597465e-05,
      "loss": 2.8442,
      "step": 19440
    },
    {
      "epoch": 2.124058097630228,
      "grad_norm": 0.20993547141551971,
      "learning_rate": 3.9382985694004585e-05,
      "loss": 2.8405,
      "step": 19450
    },
    {
      "epoch": 2.1251501583488044,
      "grad_norm": 0.22372378408908844,
      "learning_rate": 3.9377525390411704e-05,
      "loss": 2.8828,
      "step": 19460
    },
    {
      "epoch": 2.12624221906738,
      "grad_norm": 0.2553606629371643,
      "learning_rate": 3.9372065086818823e-05,
      "loss": 2.8423,
      "step": 19470
    },
    {
      "epoch": 2.127334279785956,
      "grad_norm": 0.17701756954193115,
      "learning_rate": 3.936660478322595e-05,
      "loss": 2.846,
      "step": 19480
    },
    {
      "epoch": 2.1284263405045323,
      "grad_norm": 0.4477643668651581,
      "learning_rate": 3.936114447963307e-05,
      "loss": 2.9532,
      "step": 19490
    },
    {
      "epoch": 2.129518401223108,
      "grad_norm": 0.24496661126613617,
      "learning_rate": 3.935568417604019e-05,
      "loss": 2.8439,
      "step": 19500
    },
    {
      "epoch": 2.130610461941684,
      "grad_norm": 0.21291270852088928,
      "learning_rate": 3.9350223872447315e-05,
      "loss": 2.8733,
      "step": 19510
    },
    {
      "epoch": 2.1317025226602597,
      "grad_norm": 0.16724145412445068,
      "learning_rate": 3.9344763568854434e-05,
      "loss": 2.8569,
      "step": 19520
    },
    {
      "epoch": 2.132794583378836,
      "grad_norm": 0.12859538197517395,
      "learning_rate": 3.9339303265261553e-05,
      "loss": 2.8271,
      "step": 19530
    },
    {
      "epoch": 2.133886644097412,
      "grad_norm": 0.614234447479248,
      "learning_rate": 3.933384296166867e-05,
      "loss": 2.8747,
      "step": 19540
    },
    {
      "epoch": 2.1349787048159876,
      "grad_norm": 0.11565715074539185,
      "learning_rate": 3.932838265807579e-05,
      "loss": 2.8301,
      "step": 19550
    },
    {
      "epoch": 2.136070765534564,
      "grad_norm": 0.13490688800811768,
      "learning_rate": 3.932292235448291e-05,
      "loss": 2.8326,
      "step": 19560
    },
    {
      "epoch": 2.1371628262531397,
      "grad_norm": 0.23110665380954742,
      "learning_rate": 3.931746205089003e-05,
      "loss": 2.8429,
      "step": 19570
    },
    {
      "epoch": 2.1382548869717155,
      "grad_norm": 0.16977179050445557,
      "learning_rate": 3.931200174729715e-05,
      "loss": 2.9138,
      "step": 19580
    },
    {
      "epoch": 2.1393469476902918,
      "grad_norm": 0.2980276346206665,
      "learning_rate": 3.930654144370427e-05,
      "loss": 2.8424,
      "step": 19590
    },
    {
      "epoch": 2.1404390084088676,
      "grad_norm": 0.27465692162513733,
      "learning_rate": 3.930108114011139e-05,
      "loss": 2.8883,
      "step": 19600
    },
    {
      "epoch": 2.1415310691274434,
      "grad_norm": 0.7395850419998169,
      "learning_rate": 3.929562083651851e-05,
      "loss": 2.8884,
      "step": 19610
    },
    {
      "epoch": 2.1426231298460197,
      "grad_norm": 0.11241656541824341,
      "learning_rate": 3.9290160532925635e-05,
      "loss": 2.8453,
      "step": 19620
    },
    {
      "epoch": 2.1437151905645955,
      "grad_norm": 0.15027548372745514,
      "learning_rate": 3.9284700229332754e-05,
      "loss": 2.8492,
      "step": 19630
    },
    {
      "epoch": 2.1448072512831713,
      "grad_norm": 0.565562903881073,
      "learning_rate": 3.9279239925739874e-05,
      "loss": 2.8568,
      "step": 19640
    },
    {
      "epoch": 2.145899312001747,
      "grad_norm": 0.17442813515663147,
      "learning_rate": 3.927377962214699e-05,
      "loss": 2.856,
      "step": 19650
    },
    {
      "epoch": 2.1469913727203234,
      "grad_norm": 0.10966506600379944,
      "learning_rate": 3.926831931855411e-05,
      "loss": 2.8986,
      "step": 19660
    },
    {
      "epoch": 2.148083433438899,
      "grad_norm": 0.8815374970436096,
      "learning_rate": 3.926285901496123e-05,
      "loss": 2.8746,
      "step": 19670
    },
    {
      "epoch": 2.149175494157475,
      "grad_norm": 0.27799472212791443,
      "learning_rate": 3.925739871136835e-05,
      "loss": 2.8777,
      "step": 19680
    },
    {
      "epoch": 2.1502675548760513,
      "grad_norm": 0.2142411768436432,
      "learning_rate": 3.925193840777547e-05,
      "loss": 2.8828,
      "step": 19690
    },
    {
      "epoch": 2.151359615594627,
      "grad_norm": 0.10944648832082748,
      "learning_rate": 3.924647810418259e-05,
      "loss": 2.8668,
      "step": 19700
    },
    {
      "epoch": 2.152451676313203,
      "grad_norm": 0.1206774190068245,
      "learning_rate": 3.9241017800589716e-05,
      "loss": 2.8737,
      "step": 19710
    },
    {
      "epoch": 2.153543737031779,
      "grad_norm": 0.3218349516391754,
      "learning_rate": 3.9235557496996836e-05,
      "loss": 2.8926,
      "step": 19720
    },
    {
      "epoch": 2.154635797750355,
      "grad_norm": 0.16456618905067444,
      "learning_rate": 3.9230097193403955e-05,
      "loss": 2.8661,
      "step": 19730
    },
    {
      "epoch": 2.155727858468931,
      "grad_norm": 0.14371877908706665,
      "learning_rate": 3.9224636889811075e-05,
      "loss": 2.8958,
      "step": 19740
    },
    {
      "epoch": 2.156819919187507,
      "grad_norm": 0.24171067774295807,
      "learning_rate": 3.92191765862182e-05,
      "loss": 2.8889,
      "step": 19750
    },
    {
      "epoch": 2.157911979906083,
      "grad_norm": 0.10108406096696854,
      "learning_rate": 3.921371628262532e-05,
      "loss": 2.8683,
      "step": 19760
    },
    {
      "epoch": 2.1590040406246587,
      "grad_norm": 0.24301692843437195,
      "learning_rate": 3.920825597903244e-05,
      "loss": 2.8681,
      "step": 19770
    },
    {
      "epoch": 2.1600961013432345,
      "grad_norm": 0.21860358119010925,
      "learning_rate": 3.920279567543956e-05,
      "loss": 2.8967,
      "step": 19780
    },
    {
      "epoch": 2.1611881620618107,
      "grad_norm": 0.10127697139978409,
      "learning_rate": 3.919733537184668e-05,
      "loss": 2.9162,
      "step": 19790
    },
    {
      "epoch": 2.1622802227803866,
      "grad_norm": 0.16556765139102936,
      "learning_rate": 3.91918750682538e-05,
      "loss": 2.8948,
      "step": 19800
    },
    {
      "epoch": 2.1633722834989624,
      "grad_norm": 0.21081596612930298,
      "learning_rate": 3.918641476466092e-05,
      "loss": 2.8539,
      "step": 19810
    },
    {
      "epoch": 2.1644643442175386,
      "grad_norm": 0.23843632638454437,
      "learning_rate": 3.918095446106804e-05,
      "loss": 2.8643,
      "step": 19820
    },
    {
      "epoch": 2.1655564049361145,
      "grad_norm": 0.18950414657592773,
      "learning_rate": 3.9175494157475156e-05,
      "loss": 2.8546,
      "step": 19830
    },
    {
      "epoch": 2.1666484656546903,
      "grad_norm": 0.570133626461029,
      "learning_rate": 3.9170033853882276e-05,
      "loss": 2.8704,
      "step": 19840
    },
    {
      "epoch": 2.1677405263732665,
      "grad_norm": 0.7423564791679382,
      "learning_rate": 3.9164573550289395e-05,
      "loss": 2.8675,
      "step": 19850
    },
    {
      "epoch": 2.1688325870918423,
      "grad_norm": 0.1990952491760254,
      "learning_rate": 3.9159113246696514e-05,
      "loss": 2.8331,
      "step": 19860
    },
    {
      "epoch": 2.169924647810418,
      "grad_norm": 0.2624337375164032,
      "learning_rate": 3.9153652943103634e-05,
      "loss": 2.8465,
      "step": 19870
    },
    {
      "epoch": 2.1710167085289944,
      "grad_norm": 0.1391598880290985,
      "learning_rate": 3.914819263951076e-05,
      "loss": 2.8418,
      "step": 19880
    },
    {
      "epoch": 2.1721087692475702,
      "grad_norm": 0.6815445423126221,
      "learning_rate": 3.914273233591788e-05,
      "loss": 2.8861,
      "step": 19890
    },
    {
      "epoch": 2.173200829966146,
      "grad_norm": 0.1957910805940628,
      "learning_rate": 3.9137272032325e-05,
      "loss": 2.8516,
      "step": 19900
    },
    {
      "epoch": 2.174292890684722,
      "grad_norm": 0.2148108333349228,
      "learning_rate": 3.913181172873212e-05,
      "loss": 2.8981,
      "step": 19910
    },
    {
      "epoch": 2.175384951403298,
      "grad_norm": 0.18843448162078857,
      "learning_rate": 3.912635142513924e-05,
      "loss": 2.9123,
      "step": 19920
    },
    {
      "epoch": 2.176477012121874,
      "grad_norm": 0.20914770662784576,
      "learning_rate": 3.912089112154636e-05,
      "loss": 2.8593,
      "step": 19930
    },
    {
      "epoch": 2.1775690728404498,
      "grad_norm": 0.20125706493854523,
      "learning_rate": 3.911543081795348e-05,
      "loss": 2.8309,
      "step": 19940
    },
    {
      "epoch": 2.178661133559026,
      "grad_norm": 0.1534336805343628,
      "learning_rate": 3.91099705143606e-05,
      "loss": 2.8831,
      "step": 19950
    },
    {
      "epoch": 2.179753194277602,
      "grad_norm": 0.15009114146232605,
      "learning_rate": 3.910451021076772e-05,
      "loss": 2.8506,
      "step": 19960
    },
    {
      "epoch": 2.1808452549961777,
      "grad_norm": 0.228633850812912,
      "learning_rate": 3.909904990717484e-05,
      "loss": 2.8463,
      "step": 19970
    },
    {
      "epoch": 2.181937315714754,
      "grad_norm": 0.11626502871513367,
      "learning_rate": 3.909358960358196e-05,
      "loss": 2.874,
      "step": 19980
    },
    {
      "epoch": 2.1830293764333297,
      "grad_norm": 0.2533266246318817,
      "learning_rate": 3.908812929998908e-05,
      "loss": 2.8652,
      "step": 19990
    },
    {
      "epoch": 2.1841214371519055,
      "grad_norm": 0.12143604457378387,
      "learning_rate": 3.90826689963962e-05,
      "loss": 2.8471,
      "step": 20000
    },
    {
      "epoch": 2.185213497870482,
      "grad_norm": 0.36196640133857727,
      "learning_rate": 3.9077208692803326e-05,
      "loss": 2.8585,
      "step": 20010
    },
    {
      "epoch": 2.1863055585890576,
      "grad_norm": 0.10390021651983261,
      "learning_rate": 3.9071748389210445e-05,
      "loss": 2.8606,
      "step": 20020
    },
    {
      "epoch": 2.1873976193076334,
      "grad_norm": 1.2414703369140625,
      "learning_rate": 3.9066288085617565e-05,
      "loss": 3.0103,
      "step": 20030
    },
    {
      "epoch": 2.1884896800262092,
      "grad_norm": 0.13456885516643524,
      "learning_rate": 3.9060827782024684e-05,
      "loss": 2.834,
      "step": 20040
    },
    {
      "epoch": 2.1895817407447855,
      "grad_norm": 0.2760607600212097,
      "learning_rate": 3.9055367478431803e-05,
      "loss": 2.8427,
      "step": 20050
    },
    {
      "epoch": 2.1906738014633613,
      "grad_norm": 0.18540024757385254,
      "learning_rate": 3.904990717483892e-05,
      "loss": 2.8947,
      "step": 20060
    },
    {
      "epoch": 2.191765862181937,
      "grad_norm": 0.2600005269050598,
      "learning_rate": 3.904444687124604e-05,
      "loss": 2.8779,
      "step": 20070
    },
    {
      "epoch": 2.1928579229005134,
      "grad_norm": 0.30428850650787354,
      "learning_rate": 3.903898656765316e-05,
      "loss": 2.8338,
      "step": 20080
    },
    {
      "epoch": 2.193949983619089,
      "grad_norm": 0.17108389735221863,
      "learning_rate": 3.903352626406028e-05,
      "loss": 2.8434,
      "step": 20090
    },
    {
      "epoch": 2.195042044337665,
      "grad_norm": 0.15971486270427704,
      "learning_rate": 3.90280659604674e-05,
      "loss": 2.8534,
      "step": 20100
    },
    {
      "epoch": 2.1961341050562413,
      "grad_norm": 0.19152802228927612,
      "learning_rate": 3.902260565687452e-05,
      "loss": 2.8788,
      "step": 20110
    },
    {
      "epoch": 2.197226165774817,
      "grad_norm": 0.23827184736728668,
      "learning_rate": 3.901714535328164e-05,
      "loss": 2.8699,
      "step": 20120
    },
    {
      "epoch": 2.198318226493393,
      "grad_norm": 0.1917930692434311,
      "learning_rate": 3.901168504968876e-05,
      "loss": 2.9103,
      "step": 20130
    },
    {
      "epoch": 2.199410287211969,
      "grad_norm": 0.09388403594493866,
      "learning_rate": 3.9006224746095885e-05,
      "loss": 2.8361,
      "step": 20140
    },
    {
      "epoch": 2.200502347930545,
      "grad_norm": 0.2699984312057495,
      "learning_rate": 3.9000764442503004e-05,
      "loss": 2.8577,
      "step": 20150
    },
    {
      "epoch": 2.201594408649121,
      "grad_norm": 0.11937203258275986,
      "learning_rate": 3.899585016926942e-05,
      "loss": 2.9275,
      "step": 20160
    },
    {
      "epoch": 2.2026864693676966,
      "grad_norm": 0.16379143297672272,
      "learning_rate": 3.899038986567654e-05,
      "loss": 2.863,
      "step": 20170
    },
    {
      "epoch": 2.203778530086273,
      "grad_norm": 0.1336854249238968,
      "learning_rate": 3.898492956208366e-05,
      "loss": 2.8767,
      "step": 20180
    },
    {
      "epoch": 2.2048705908048487,
      "grad_norm": 0.15506213903427124,
      "learning_rate": 3.8979469258490776e-05,
      "loss": 2.9075,
      "step": 20190
    },
    {
      "epoch": 2.2059626515234245,
      "grad_norm": 0.30616089701652527,
      "learning_rate": 3.8974008954897896e-05,
      "loss": 2.8275,
      "step": 20200
    },
    {
      "epoch": 2.207054712242001,
      "grad_norm": 0.19347873330116272,
      "learning_rate": 3.8968548651305015e-05,
      "loss": 2.8613,
      "step": 20210
    },
    {
      "epoch": 2.2081467729605766,
      "grad_norm": 0.1836383044719696,
      "learning_rate": 3.8963088347712134e-05,
      "loss": 2.868,
      "step": 20220
    },
    {
      "epoch": 2.2092388336791524,
      "grad_norm": 0.20449614524841309,
      "learning_rate": 3.8957628044119254e-05,
      "loss": 2.906,
      "step": 20230
    },
    {
      "epoch": 2.2103308943977287,
      "grad_norm": 0.258729487657547,
      "learning_rate": 3.895216774052637e-05,
      "loss": 2.8721,
      "step": 20240
    },
    {
      "epoch": 2.2114229551163045,
      "grad_norm": 0.16332122683525085,
      "learning_rate": 3.894670743693349e-05,
      "loss": 2.8307,
      "step": 20250
    },
    {
      "epoch": 2.2125150158348803,
      "grad_norm": 0.09269484877586365,
      "learning_rate": 3.894124713334061e-05,
      "loss": 2.8505,
      "step": 20260
    },
    {
      "epoch": 2.2136070765534566,
      "grad_norm": 0.24502669274806976,
      "learning_rate": 3.893578682974773e-05,
      "loss": 2.8281,
      "step": 20270
    },
    {
      "epoch": 2.2146991372720324,
      "grad_norm": 0.11311028897762299,
      "learning_rate": 3.893032652615485e-05,
      "loss": 2.8662,
      "step": 20280
    },
    {
      "epoch": 2.215791197990608,
      "grad_norm": 0.11837364733219147,
      "learning_rate": 3.892486622256198e-05,
      "loss": 2.8385,
      "step": 20290
    },
    {
      "epoch": 2.216883258709184,
      "grad_norm": 0.13001203536987305,
      "learning_rate": 3.8919405918969096e-05,
      "loss": 2.8273,
      "step": 20300
    },
    {
      "epoch": 2.2179753194277603,
      "grad_norm": 0.12985755503177643,
      "learning_rate": 3.8913945615376216e-05,
      "loss": 2.8386,
      "step": 20310
    },
    {
      "epoch": 2.219067380146336,
      "grad_norm": 0.20863379538059235,
      "learning_rate": 3.8908485311783335e-05,
      "loss": 2.8493,
      "step": 20320
    },
    {
      "epoch": 2.220159440864912,
      "grad_norm": 0.22472785413265228,
      "learning_rate": 3.8903025008190455e-05,
      "loss": 2.8467,
      "step": 20330
    },
    {
      "epoch": 2.221251501583488,
      "grad_norm": 0.227775439620018,
      "learning_rate": 3.889756470459758e-05,
      "loss": 2.856,
      "step": 20340
    },
    {
      "epoch": 2.222343562302064,
      "grad_norm": 0.35365840792655945,
      "learning_rate": 3.88921044010047e-05,
      "loss": 2.8656,
      "step": 20350
    },
    {
      "epoch": 2.22343562302064,
      "grad_norm": 0.1935771107673645,
      "learning_rate": 3.888664409741182e-05,
      "loss": 2.9105,
      "step": 20360
    },
    {
      "epoch": 2.224527683739216,
      "grad_norm": 0.29982608556747437,
      "learning_rate": 3.888118379381894e-05,
      "loss": 2.8959,
      "step": 20370
    },
    {
      "epoch": 2.225619744457792,
      "grad_norm": 0.24029922485351562,
      "learning_rate": 3.887572349022606e-05,
      "loss": 2.924,
      "step": 20380
    },
    {
      "epoch": 2.2267118051763677,
      "grad_norm": 0.8191167116165161,
      "learning_rate": 3.887026318663318e-05,
      "loss": 2.9286,
      "step": 20390
    },
    {
      "epoch": 2.227803865894944,
      "grad_norm": 0.16419334709644318,
      "learning_rate": 3.88648028830403e-05,
      "loss": 2.8306,
      "step": 20400
    },
    {
      "epoch": 2.2288959266135198,
      "grad_norm": 0.09553220868110657,
      "learning_rate": 3.885934257944742e-05,
      "loss": 2.8556,
      "step": 20410
    },
    {
      "epoch": 2.2299879873320956,
      "grad_norm": 0.11839299649000168,
      "learning_rate": 3.885388227585454e-05,
      "loss": 2.8551,
      "step": 20420
    },
    {
      "epoch": 2.2310800480506714,
      "grad_norm": 0.17367134988307953,
      "learning_rate": 3.884842197226166e-05,
      "loss": 2.8606,
      "step": 20430
    },
    {
      "epoch": 2.2321721087692477,
      "grad_norm": 0.10751771181821823,
      "learning_rate": 3.884296166866878e-05,
      "loss": 2.8402,
      "step": 20440
    },
    {
      "epoch": 2.2332641694878235,
      "grad_norm": 0.2480771690607071,
      "learning_rate": 3.88375013650759e-05,
      "loss": 2.8714,
      "step": 20450
    },
    {
      "epoch": 2.2343562302063993,
      "grad_norm": 0.5417212247848511,
      "learning_rate": 3.883204106148302e-05,
      "loss": 2.8584,
      "step": 20460
    },
    {
      "epoch": 2.2354482909249755,
      "grad_norm": 0.2651427090167999,
      "learning_rate": 3.882658075789014e-05,
      "loss": 2.8402,
      "step": 20470
    },
    {
      "epoch": 2.2365403516435514,
      "grad_norm": 0.2041928619146347,
      "learning_rate": 3.882112045429726e-05,
      "loss": 2.889,
      "step": 20480
    },
    {
      "epoch": 2.237632412362127,
      "grad_norm": 0.1197926253080368,
      "learning_rate": 3.881566015070438e-05,
      "loss": 2.8795,
      "step": 20490
    },
    {
      "epoch": 2.2387244730807034,
      "grad_norm": 0.11134318262338638,
      "learning_rate": 3.88101998471115e-05,
      "loss": 2.8411,
      "step": 20500
    },
    {
      "epoch": 2.2398165337992793,
      "grad_norm": 0.2684532701969147,
      "learning_rate": 3.880473954351862e-05,
      "loss": 2.8652,
      "step": 20510
    },
    {
      "epoch": 2.240908594517855,
      "grad_norm": 0.16273680329322815,
      "learning_rate": 3.879927923992574e-05,
      "loss": 2.8503,
      "step": 20520
    },
    {
      "epoch": 2.2420006552364313,
      "grad_norm": 0.10532015562057495,
      "learning_rate": 3.8793818936332856e-05,
      "loss": 2.8545,
      "step": 20530
    },
    {
      "epoch": 2.243092715955007,
      "grad_norm": 0.12274997681379318,
      "learning_rate": 3.878835863273998e-05,
      "loss": 2.8636,
      "step": 20540
    },
    {
      "epoch": 2.244184776673583,
      "grad_norm": 0.17698954045772552,
      "learning_rate": 3.87828983291471e-05,
      "loss": 2.8531,
      "step": 20550
    },
    {
      "epoch": 2.2452768373921588,
      "grad_norm": 0.3239228427410126,
      "learning_rate": 3.877743802555422e-05,
      "loss": 2.8823,
      "step": 20560
    },
    {
      "epoch": 2.246368898110735,
      "grad_norm": 0.2391275316476822,
      "learning_rate": 3.877197772196135e-05,
      "loss": 2.8735,
      "step": 20570
    },
    {
      "epoch": 2.247460958829311,
      "grad_norm": 0.12723515927791595,
      "learning_rate": 3.876651741836847e-05,
      "loss": 2.8622,
      "step": 20580
    },
    {
      "epoch": 2.2485530195478867,
      "grad_norm": 0.21431668102741241,
      "learning_rate": 3.8761057114775586e-05,
      "loss": 2.8751,
      "step": 20590
    },
    {
      "epoch": 2.249645080266463,
      "grad_norm": 0.16222190856933594,
      "learning_rate": 3.8755596811182706e-05,
      "loss": 2.8205,
      "step": 20600
    },
    {
      "epoch": 2.2507371409850387,
      "grad_norm": 0.10369952768087387,
      "learning_rate": 3.8750136507589825e-05,
      "loss": 2.8556,
      "step": 20610
    },
    {
      "epoch": 2.2518292017036146,
      "grad_norm": 0.3885027766227722,
      "learning_rate": 3.8744676203996945e-05,
      "loss": 2.8433,
      "step": 20620
    },
    {
      "epoch": 2.252921262422191,
      "grad_norm": 0.41994529962539673,
      "learning_rate": 3.8739215900404064e-05,
      "loss": 2.9081,
      "step": 20630
    },
    {
      "epoch": 2.2540133231407666,
      "grad_norm": 0.2864692807197571,
      "learning_rate": 3.8733755596811183e-05,
      "loss": 2.8744,
      "step": 20640
    },
    {
      "epoch": 2.2551053838593424,
      "grad_norm": 0.12413552403450012,
      "learning_rate": 3.87282952932183e-05,
      "loss": 2.8372,
      "step": 20650
    },
    {
      "epoch": 2.2561974445779187,
      "grad_norm": 0.15732720494270325,
      "learning_rate": 3.872283498962542e-05,
      "loss": 2.8651,
      "step": 20660
    },
    {
      "epoch": 2.2572895052964945,
      "grad_norm": 0.7269350290298462,
      "learning_rate": 3.871737468603254e-05,
      "loss": 2.887,
      "step": 20670
    },
    {
      "epoch": 2.2583815660150703,
      "grad_norm": 0.25474128127098083,
      "learning_rate": 3.871191438243967e-05,
      "loss": 2.866,
      "step": 20680
    },
    {
      "epoch": 2.259473626733646,
      "grad_norm": 0.1301918625831604,
      "learning_rate": 3.870645407884679e-05,
      "loss": 2.8993,
      "step": 20690
    },
    {
      "epoch": 2.2605656874522224,
      "grad_norm": 0.2388601154088974,
      "learning_rate": 3.870099377525391e-05,
      "loss": 2.8622,
      "step": 20700
    },
    {
      "epoch": 2.2616577481707982,
      "grad_norm": 0.2183980792760849,
      "learning_rate": 3.8695533471661026e-05,
      "loss": 2.8917,
      "step": 20710
    },
    {
      "epoch": 2.2627498088893745,
      "grad_norm": 0.299401193857193,
      "learning_rate": 3.8690073168068146e-05,
      "loss": 2.859,
      "step": 20720
    },
    {
      "epoch": 2.2638418696079503,
      "grad_norm": 0.09348591417074203,
      "learning_rate": 3.8684612864475265e-05,
      "loss": 2.8569,
      "step": 20730
    },
    {
      "epoch": 2.264933930326526,
      "grad_norm": 0.1434384435415268,
      "learning_rate": 3.8679152560882384e-05,
      "loss": 2.934,
      "step": 20740
    },
    {
      "epoch": 2.266025991045102,
      "grad_norm": 0.16163432598114014,
      "learning_rate": 3.8673692257289504e-05,
      "loss": 2.8657,
      "step": 20750
    },
    {
      "epoch": 2.267118051763678,
      "grad_norm": 0.14156602323055267,
      "learning_rate": 3.866823195369662e-05,
      "loss": 2.8374,
      "step": 20760
    },
    {
      "epoch": 2.268210112482254,
      "grad_norm": 0.17415742576122284,
      "learning_rate": 3.866277165010375e-05,
      "loss": 2.866,
      "step": 20770
    },
    {
      "epoch": 2.26930217320083,
      "grad_norm": 0.15174639225006104,
      "learning_rate": 3.865731134651087e-05,
      "loss": 2.8848,
      "step": 20780
    },
    {
      "epoch": 2.270394233919406,
      "grad_norm": 0.13833662867546082,
      "learning_rate": 3.865185104291799e-05,
      "loss": 2.8557,
      "step": 20790
    },
    {
      "epoch": 2.271486294637982,
      "grad_norm": 0.20013803243637085,
      "learning_rate": 3.864639073932511e-05,
      "loss": 2.8673,
      "step": 20800
    },
    {
      "epoch": 2.2725783553565577,
      "grad_norm": 0.2082705944776535,
      "learning_rate": 3.8640930435732234e-05,
      "loss": 2.8733,
      "step": 20810
    },
    {
      "epoch": 2.2736704160751335,
      "grad_norm": 0.32673871517181396,
      "learning_rate": 3.863547013213935e-05,
      "loss": 2.8544,
      "step": 20820
    },
    {
      "epoch": 2.27476247679371,
      "grad_norm": 0.23171377182006836,
      "learning_rate": 3.863000982854647e-05,
      "loss": 2.863,
      "step": 20830
    },
    {
      "epoch": 2.2758545375122856,
      "grad_norm": 0.15493465960025787,
      "learning_rate": 3.862454952495359e-05,
      "loss": 2.818,
      "step": 20840
    },
    {
      "epoch": 2.276946598230862,
      "grad_norm": 0.23385386168956757,
      "learning_rate": 3.861908922136071e-05,
      "loss": 2.92,
      "step": 20850
    },
    {
      "epoch": 2.2780386589494377,
      "grad_norm": 0.28963780403137207,
      "learning_rate": 3.861362891776783e-05,
      "loss": 2.8521,
      "step": 20860
    },
    {
      "epoch": 2.2791307196680135,
      "grad_norm": 0.0970287173986435,
      "learning_rate": 3.860816861417495e-05,
      "loss": 2.8811,
      "step": 20870
    },
    {
      "epoch": 2.2802227803865893,
      "grad_norm": 0.2045394778251648,
      "learning_rate": 3.860270831058207e-05,
      "loss": 2.9157,
      "step": 20880
    },
    {
      "epoch": 2.2813148411051656,
      "grad_norm": 0.08859391510486603,
      "learning_rate": 3.859724800698919e-05,
      "loss": 2.8337,
      "step": 20890
    },
    {
      "epoch": 2.2824069018237414,
      "grad_norm": 0.13293302059173584,
      "learning_rate": 3.859178770339631e-05,
      "loss": 2.868,
      "step": 20900
    },
    {
      "epoch": 2.283498962542317,
      "grad_norm": 0.2045769840478897,
      "learning_rate": 3.858632739980343e-05,
      "loss": 2.8587,
      "step": 20910
    },
    {
      "epoch": 2.2845910232608935,
      "grad_norm": 0.2698449194431305,
      "learning_rate": 3.858086709621055e-05,
      "loss": 2.8914,
      "step": 20920
    },
    {
      "epoch": 2.2856830839794693,
      "grad_norm": 0.1279963254928589,
      "learning_rate": 3.857540679261767e-05,
      "loss": 2.8687,
      "step": 20930
    },
    {
      "epoch": 2.286775144698045,
      "grad_norm": 0.6213738322257996,
      "learning_rate": 3.856994648902479e-05,
      "loss": 2.8554,
      "step": 20940
    },
    {
      "epoch": 2.287867205416621,
      "grad_norm": 0.13829481601715088,
      "learning_rate": 3.856448618543191e-05,
      "loss": 2.8275,
      "step": 20950
    },
    {
      "epoch": 2.288959266135197,
      "grad_norm": 0.5093804597854614,
      "learning_rate": 3.855902588183903e-05,
      "loss": 2.8564,
      "step": 20960
    },
    {
      "epoch": 2.290051326853773,
      "grad_norm": 0.11114208400249481,
      "learning_rate": 3.855356557824615e-05,
      "loss": 2.8341,
      "step": 20970
    },
    {
      "epoch": 2.2911433875723493,
      "grad_norm": 0.34430941939353943,
      "learning_rate": 3.854810527465327e-05,
      "loss": 2.8419,
      "step": 20980
    },
    {
      "epoch": 2.292235448290925,
      "grad_norm": 0.20971716940402985,
      "learning_rate": 3.854264497106039e-05,
      "loss": 2.8619,
      "step": 20990
    },
    {
      "epoch": 2.293327509009501,
      "grad_norm": 0.11075947433710098,
      "learning_rate": 3.8537184667467516e-05,
      "loss": 2.8357,
      "step": 21000
    },
    {
      "epoch": 2.2944195697280767,
      "grad_norm": 0.09103544801473618,
      "learning_rate": 3.8531724363874635e-05,
      "loss": 2.853,
      "step": 21010
    },
    {
      "epoch": 2.295511630446653,
      "grad_norm": 0.17089469730854034,
      "learning_rate": 3.8526264060281755e-05,
      "loss": 2.8759,
      "step": 21020
    },
    {
      "epoch": 2.2966036911652288,
      "grad_norm": 0.19014257192611694,
      "learning_rate": 3.8520803756688874e-05,
      "loss": 2.9885,
      "step": 21030
    },
    {
      "epoch": 2.2976957518838046,
      "grad_norm": 0.15605095028877258,
      "learning_rate": 3.8515343453095994e-05,
      "loss": 2.8496,
      "step": 21040
    },
    {
      "epoch": 2.298787812602381,
      "grad_norm": 0.1401294618844986,
      "learning_rate": 3.850988314950311e-05,
      "loss": 2.8807,
      "step": 21050
    },
    {
      "epoch": 2.2998798733209567,
      "grad_norm": 0.13467207551002502,
      "learning_rate": 3.850442284591023e-05,
      "loss": 2.9479,
      "step": 21060
    },
    {
      "epoch": 2.3009719340395325,
      "grad_norm": 0.26114708185195923,
      "learning_rate": 3.849896254231736e-05,
      "loss": 2.8475,
      "step": 21070
    },
    {
      "epoch": 2.3020639947581087,
      "grad_norm": 0.34302783012390137,
      "learning_rate": 3.849350223872448e-05,
      "loss": 2.9206,
      "step": 21080
    },
    {
      "epoch": 2.3031560554766846,
      "grad_norm": 0.15473560988903046,
      "learning_rate": 3.84880419351316e-05,
      "loss": 2.8504,
      "step": 21090
    },
    {
      "epoch": 2.3042481161952604,
      "grad_norm": 0.4372457265853882,
      "learning_rate": 3.848258163153872e-05,
      "loss": 2.8367,
      "step": 21100
    },
    {
      "epoch": 2.3053401769138366,
      "grad_norm": 0.11321347951889038,
      "learning_rate": 3.8477121327945836e-05,
      "loss": 2.8706,
      "step": 21110
    },
    {
      "epoch": 2.3064322376324125,
      "grad_norm": 0.3258703351020813,
      "learning_rate": 3.8471661024352956e-05,
      "loss": 2.8762,
      "step": 21120
    },
    {
      "epoch": 2.3075242983509883,
      "grad_norm": 0.15864428877830505,
      "learning_rate": 3.8466200720760075e-05,
      "loss": 2.8428,
      "step": 21130
    },
    {
      "epoch": 2.308616359069564,
      "grad_norm": 0.1154695376753807,
      "learning_rate": 3.8460740417167195e-05,
      "loss": 2.8638,
      "step": 21140
    },
    {
      "epoch": 2.3097084197881403,
      "grad_norm": 0.25560444593429565,
      "learning_rate": 3.8455280113574314e-05,
      "loss": 2.8796,
      "step": 21150
    },
    {
      "epoch": 2.310800480506716,
      "grad_norm": 0.6973269581794739,
      "learning_rate": 3.8449819809981433e-05,
      "loss": 2.8694,
      "step": 21160
    },
    {
      "epoch": 2.311892541225292,
      "grad_norm": 0.12109608948230743,
      "learning_rate": 3.844435950638855e-05,
      "loss": 2.8699,
      "step": 21170
    },
    {
      "epoch": 2.3129846019438682,
      "grad_norm": 0.16638094186782837,
      "learning_rate": 3.843889920279567e-05,
      "loss": 2.8665,
      "step": 21180
    },
    {
      "epoch": 2.314076662662444,
      "grad_norm": 0.09926625341176987,
      "learning_rate": 3.843343889920279e-05,
      "loss": 2.8383,
      "step": 21190
    },
    {
      "epoch": 2.31516872338102,
      "grad_norm": 0.12974511086940765,
      "learning_rate": 3.842797859560992e-05,
      "loss": 2.8966,
      "step": 21200
    },
    {
      "epoch": 2.316260784099596,
      "grad_norm": 0.21432536840438843,
      "learning_rate": 3.842251829201704e-05,
      "loss": 2.8748,
      "step": 21210
    },
    {
      "epoch": 2.317352844818172,
      "grad_norm": 0.12046769261360168,
      "learning_rate": 3.841705798842416e-05,
      "loss": 2.8619,
      "step": 21220
    },
    {
      "epoch": 2.3184449055367478,
      "grad_norm": 0.2064286470413208,
      "learning_rate": 3.841159768483128e-05,
      "loss": 2.8848,
      "step": 21230
    },
    {
      "epoch": 2.319536966255324,
      "grad_norm": 0.13375912606716156,
      "learning_rate": 3.84061373812384e-05,
      "loss": 2.892,
      "step": 21240
    },
    {
      "epoch": 2.3206290269739,
      "grad_norm": 0.2172042280435562,
      "learning_rate": 3.840067707764552e-05,
      "loss": 2.8435,
      "step": 21250
    },
    {
      "epoch": 2.3217210876924756,
      "grad_norm": 0.13474184274673462,
      "learning_rate": 3.839521677405264e-05,
      "loss": 2.8405,
      "step": 21260
    },
    {
      "epoch": 2.3228131484110515,
      "grad_norm": 0.1039135754108429,
      "learning_rate": 3.838975647045976e-05,
      "loss": 2.8538,
      "step": 21270
    },
    {
      "epoch": 2.3239052091296277,
      "grad_norm": 0.30394247174263,
      "learning_rate": 3.838429616686688e-05,
      "loss": 2.888,
      "step": 21280
    },
    {
      "epoch": 2.3249972698482035,
      "grad_norm": 0.21759870648384094,
      "learning_rate": 3.8378835863274e-05,
      "loss": 2.8685,
      "step": 21290
    },
    {
      "epoch": 2.3260893305667794,
      "grad_norm": 0.1687837541103363,
      "learning_rate": 3.837337555968112e-05,
      "loss": 2.8414,
      "step": 21300
    },
    {
      "epoch": 2.3271813912853556,
      "grad_norm": 0.12006974220275879,
      "learning_rate": 3.836791525608824e-05,
      "loss": 2.8508,
      "step": 21310
    },
    {
      "epoch": 2.3282734520039314,
      "grad_norm": 0.2712903320789337,
      "learning_rate": 3.836245495249536e-05,
      "loss": 2.8462,
      "step": 21320
    },
    {
      "epoch": 2.3293655127225072,
      "grad_norm": 0.1680206060409546,
      "learning_rate": 3.8356994648902484e-05,
      "loss": 2.8554,
      "step": 21330
    },
    {
      "epoch": 2.3304575734410835,
      "grad_norm": 0.42581263184547424,
      "learning_rate": 3.83515343453096e-05,
      "loss": 2.8582,
      "step": 21340
    },
    {
      "epoch": 2.3315496341596593,
      "grad_norm": 0.22202283143997192,
      "learning_rate": 3.834607404171672e-05,
      "loss": 2.8414,
      "step": 21350
    },
    {
      "epoch": 2.332641694878235,
      "grad_norm": 0.20556792616844177,
      "learning_rate": 3.834061373812384e-05,
      "loss": 2.8586,
      "step": 21360
    },
    {
      "epoch": 2.3337337555968114,
      "grad_norm": 0.13339312374591827,
      "learning_rate": 3.833515343453096e-05,
      "loss": 2.8704,
      "step": 21370
    },
    {
      "epoch": 2.334825816315387,
      "grad_norm": 0.11645250767469406,
      "learning_rate": 3.832969313093808e-05,
      "loss": 2.8736,
      "step": 21380
    },
    {
      "epoch": 2.335917877033963,
      "grad_norm": 0.27287113666534424,
      "learning_rate": 3.83242328273452e-05,
      "loss": 2.8867,
      "step": 21390
    },
    {
      "epoch": 2.337009937752539,
      "grad_norm": 0.3367709219455719,
      "learning_rate": 3.831877252375232e-05,
      "loss": 2.8523,
      "step": 21400
    },
    {
      "epoch": 2.338101998471115,
      "grad_norm": 0.1676807701587677,
      "learning_rate": 3.831331222015944e-05,
      "loss": 2.8737,
      "step": 21410
    },
    {
      "epoch": 2.339194059189691,
      "grad_norm": 0.1986633837223053,
      "learning_rate": 3.830785191656656e-05,
      "loss": 2.9074,
      "step": 21420
    },
    {
      "epoch": 2.3402861199082667,
      "grad_norm": 0.14616121351718903,
      "learning_rate": 3.8302391612973685e-05,
      "loss": 2.8673,
      "step": 21430
    },
    {
      "epoch": 2.341378180626843,
      "grad_norm": 0.40865933895111084,
      "learning_rate": 3.8296931309380804e-05,
      "loss": 2.9136,
      "step": 21440
    },
    {
      "epoch": 2.342470241345419,
      "grad_norm": 0.221041738986969,
      "learning_rate": 3.8291471005787923e-05,
      "loss": 2.9062,
      "step": 21450
    },
    {
      "epoch": 2.3435623020639946,
      "grad_norm": 0.4482000470161438,
      "learning_rate": 3.828601070219505e-05,
      "loss": 3.0068,
      "step": 21460
    },
    {
      "epoch": 2.344654362782571,
      "grad_norm": 0.25239303708076477,
      "learning_rate": 3.828055039860217e-05,
      "loss": 2.875,
      "step": 21470
    },
    {
      "epoch": 2.3457464235011467,
      "grad_norm": 0.3145805597305298,
      "learning_rate": 3.827509009500929e-05,
      "loss": 2.8545,
      "step": 21480
    },
    {
      "epoch": 2.3468384842197225,
      "grad_norm": 0.10684092342853546,
      "learning_rate": 3.826962979141641e-05,
      "loss": 2.8506,
      "step": 21490
    },
    {
      "epoch": 2.347930544938299,
      "grad_norm": 0.18994614481925964,
      "learning_rate": 3.826416948782353e-05,
      "loss": 2.895,
      "step": 21500
    },
    {
      "epoch": 2.3490226056568746,
      "grad_norm": 0.1249472051858902,
      "learning_rate": 3.825870918423065e-05,
      "loss": 2.8574,
      "step": 21510
    },
    {
      "epoch": 2.3501146663754504,
      "grad_norm": 0.8119961023330688,
      "learning_rate": 3.8253248880637766e-05,
      "loss": 2.8625,
      "step": 21520
    },
    {
      "epoch": 2.3512067270940262,
      "grad_norm": 0.15068383514881134,
      "learning_rate": 3.8247788577044885e-05,
      "loss": 2.8237,
      "step": 21530
    },
    {
      "epoch": 2.3522987878126025,
      "grad_norm": 0.11309801787137985,
      "learning_rate": 3.8242328273452005e-05,
      "loss": 2.8426,
      "step": 21540
    },
    {
      "epoch": 2.3533908485311783,
      "grad_norm": 0.14533622562885284,
      "learning_rate": 3.8236867969859124e-05,
      "loss": 2.8486,
      "step": 21550
    },
    {
      "epoch": 2.354482909249754,
      "grad_norm": 0.09156644344329834,
      "learning_rate": 3.8231407666266244e-05,
      "loss": 2.847,
      "step": 21560
    },
    {
      "epoch": 2.3555749699683304,
      "grad_norm": 0.13830514252185822,
      "learning_rate": 3.822594736267336e-05,
      "loss": 2.9,
      "step": 21570
    },
    {
      "epoch": 2.356667030686906,
      "grad_norm": 0.17697854340076447,
      "learning_rate": 3.822048705908048e-05,
      "loss": 2.8641,
      "step": 21580
    },
    {
      "epoch": 2.357759091405482,
      "grad_norm": 0.18785595893859863,
      "learning_rate": 3.821502675548761e-05,
      "loss": 2.8644,
      "step": 21590
    },
    {
      "epoch": 2.3588511521240583,
      "grad_norm": 0.48031026124954224,
      "learning_rate": 3.820956645189473e-05,
      "loss": 2.8745,
      "step": 21600
    },
    {
      "epoch": 2.359943212842634,
      "grad_norm": 0.21866221725940704,
      "learning_rate": 3.820410614830185e-05,
      "loss": 2.8623,
      "step": 21610
    },
    {
      "epoch": 2.36103527356121,
      "grad_norm": 0.16545645892620087,
      "learning_rate": 3.819864584470897e-05,
      "loss": 2.8239,
      "step": 21620
    },
    {
      "epoch": 2.362127334279786,
      "grad_norm": 0.11439312249422073,
      "learning_rate": 3.8193185541116086e-05,
      "loss": 2.8676,
      "step": 21630
    },
    {
      "epoch": 2.363219394998362,
      "grad_norm": 0.2063833624124527,
      "learning_rate": 3.8187725237523206e-05,
      "loss": 2.8354,
      "step": 21640
    },
    {
      "epoch": 2.364311455716938,
      "grad_norm": 0.2418748289346695,
      "learning_rate": 3.8182264933930325e-05,
      "loss": 2.8772,
      "step": 21650
    },
    {
      "epoch": 2.3654035164355136,
      "grad_norm": 0.1950857788324356,
      "learning_rate": 3.817680463033745e-05,
      "loss": 2.8748,
      "step": 21660
    },
    {
      "epoch": 2.36649557715409,
      "grad_norm": 0.6096048355102539,
      "learning_rate": 3.817134432674457e-05,
      "loss": 2.8611,
      "step": 21670
    },
    {
      "epoch": 2.3675876378726657,
      "grad_norm": 0.2104240208864212,
      "learning_rate": 3.816588402315169e-05,
      "loss": 2.8649,
      "step": 21680
    },
    {
      "epoch": 2.3686796985912415,
      "grad_norm": 0.11769972741603851,
      "learning_rate": 3.816042371955881e-05,
      "loss": 2.9823,
      "step": 21690
    },
    {
      "epoch": 2.3697717593098178,
      "grad_norm": 0.20795324444770813,
      "learning_rate": 3.815496341596593e-05,
      "loss": 2.9021,
      "step": 21700
    },
    {
      "epoch": 2.3708638200283936,
      "grad_norm": 0.2202311009168625,
      "learning_rate": 3.814950311237305e-05,
      "loss": 2.8823,
      "step": 21710
    },
    {
      "epoch": 2.3719558807469694,
      "grad_norm": 0.10682741552591324,
      "learning_rate": 3.8144042808780175e-05,
      "loss": 2.8779,
      "step": 21720
    },
    {
      "epoch": 2.3730479414655457,
      "grad_norm": 0.1680217832326889,
      "learning_rate": 3.8138582505187294e-05,
      "loss": 2.9131,
      "step": 21730
    },
    {
      "epoch": 2.3741400021841215,
      "grad_norm": 0.13450229167938232,
      "learning_rate": 3.8133122201594413e-05,
      "loss": 2.8702,
      "step": 21740
    },
    {
      "epoch": 2.3752320629026973,
      "grad_norm": 0.17398099601268768,
      "learning_rate": 3.812766189800153e-05,
      "loss": 2.8504,
      "step": 21750
    },
    {
      "epoch": 2.3763241236212735,
      "grad_norm": 0.1442086547613144,
      "learning_rate": 3.812220159440865e-05,
      "loss": 2.8868,
      "step": 21760
    },
    {
      "epoch": 2.3774161843398494,
      "grad_norm": 0.1333198845386505,
      "learning_rate": 3.811674129081577e-05,
      "loss": 2.8372,
      "step": 21770
    },
    {
      "epoch": 2.378508245058425,
      "grad_norm": 0.18589287996292114,
      "learning_rate": 3.811128098722289e-05,
      "loss": 2.8444,
      "step": 21780
    },
    {
      "epoch": 2.379600305777001,
      "grad_norm": 0.1425391137599945,
      "learning_rate": 3.810582068363001e-05,
      "loss": 2.8487,
      "step": 21790
    },
    {
      "epoch": 2.3806923664955773,
      "grad_norm": 0.10345183312892914,
      "learning_rate": 3.810036038003713e-05,
      "loss": 2.8515,
      "step": 21800
    },
    {
      "epoch": 2.381784427214153,
      "grad_norm": 0.20182372629642487,
      "learning_rate": 3.809490007644425e-05,
      "loss": 2.8642,
      "step": 21810
    },
    {
      "epoch": 2.382876487932729,
      "grad_norm": 0.1617046594619751,
      "learning_rate": 3.808943977285137e-05,
      "loss": 2.8557,
      "step": 21820
    },
    {
      "epoch": 2.383968548651305,
      "grad_norm": 0.16900059580802917,
      "learning_rate": 3.808397946925849e-05,
      "loss": 2.8614,
      "step": 21830
    },
    {
      "epoch": 2.385060609369881,
      "grad_norm": 0.2272443175315857,
      "learning_rate": 3.807851916566561e-05,
      "loss": 2.8501,
      "step": 21840
    },
    {
      "epoch": 2.3861526700884568,
      "grad_norm": 0.16529510915279388,
      "learning_rate": 3.8073058862072734e-05,
      "loss": 2.8857,
      "step": 21850
    },
    {
      "epoch": 2.387244730807033,
      "grad_norm": 0.1311628371477127,
      "learning_rate": 3.806759855847985e-05,
      "loss": 2.8422,
      "step": 21860
    },
    {
      "epoch": 2.388336791525609,
      "grad_norm": 0.3308079242706299,
      "learning_rate": 3.806213825488697e-05,
      "loss": 2.866,
      "step": 21870
    },
    {
      "epoch": 2.3894288522441847,
      "grad_norm": 0.19385185837745667,
      "learning_rate": 3.805667795129409e-05,
      "loss": 2.9224,
      "step": 21880
    },
    {
      "epoch": 2.390520912962761,
      "grad_norm": 0.17747581005096436,
      "learning_rate": 3.805121764770122e-05,
      "loss": 2.8639,
      "step": 21890
    },
    {
      "epoch": 2.3916129736813367,
      "grad_norm": 0.2938401699066162,
      "learning_rate": 3.804575734410834e-05,
      "loss": 2.8667,
      "step": 21900
    },
    {
      "epoch": 2.3927050343999126,
      "grad_norm": 0.3687003552913666,
      "learning_rate": 3.804029704051546e-05,
      "loss": 2.8419,
      "step": 21910
    },
    {
      "epoch": 2.3937970951184884,
      "grad_norm": 0.3271603584289551,
      "learning_rate": 3.8034836736922576e-05,
      "loss": 2.864,
      "step": 21920
    },
    {
      "epoch": 2.3948891558370646,
      "grad_norm": 0.10311858355998993,
      "learning_rate": 3.8029376433329696e-05,
      "loss": 2.8375,
      "step": 21930
    },
    {
      "epoch": 2.3959812165556404,
      "grad_norm": 0.1825525313615799,
      "learning_rate": 3.8023916129736815e-05,
      "loss": 2.9329,
      "step": 21940
    },
    {
      "epoch": 2.3970732772742163,
      "grad_norm": 0.10753358900547028,
      "learning_rate": 3.8018455826143935e-05,
      "loss": 2.8656,
      "step": 21950
    },
    {
      "epoch": 2.3981653379927925,
      "grad_norm": 0.12042021006345749,
      "learning_rate": 3.8012995522551054e-05,
      "loss": 2.825,
      "step": 21960
    },
    {
      "epoch": 2.3992573987113683,
      "grad_norm": 0.19686584174633026,
      "learning_rate": 3.800753521895817e-05,
      "loss": 2.8961,
      "step": 21970
    },
    {
      "epoch": 2.400349459429944,
      "grad_norm": 0.2215786576271057,
      "learning_rate": 3.80020749153653e-05,
      "loss": 2.885,
      "step": 21980
    },
    {
      "epoch": 2.4014415201485204,
      "grad_norm": 0.12903639674186707,
      "learning_rate": 3.799661461177242e-05,
      "loss": 2.8995,
      "step": 21990
    },
    {
      "epoch": 2.4025335808670962,
      "grad_norm": 0.14809158444404602,
      "learning_rate": 3.799115430817954e-05,
      "loss": 2.8459,
      "step": 22000
    },
    {
      "epoch": 2.403625641585672,
      "grad_norm": 0.17945197224617004,
      "learning_rate": 3.798569400458666e-05,
      "loss": 2.8366,
      "step": 22010
    },
    {
      "epoch": 2.4047177023042483,
      "grad_norm": 0.15315461158752441,
      "learning_rate": 3.798023370099378e-05,
      "loss": 2.8782,
      "step": 22020
    },
    {
      "epoch": 2.405809763022824,
      "grad_norm": 0.6053180694580078,
      "learning_rate": 3.7974773397400897e-05,
      "loss": 2.8697,
      "step": 22030
    },
    {
      "epoch": 2.4069018237414,
      "grad_norm": 0.14783194661140442,
      "learning_rate": 3.7969313093808016e-05,
      "loss": 2.9201,
      "step": 22040
    },
    {
      "epoch": 2.4079938844599758,
      "grad_norm": 0.1516779512166977,
      "learning_rate": 3.7963852790215135e-05,
      "loss": 2.8638,
      "step": 22050
    },
    {
      "epoch": 2.409085945178552,
      "grad_norm": 0.18704858422279358,
      "learning_rate": 3.7958392486622255e-05,
      "loss": 2.8517,
      "step": 22060
    },
    {
      "epoch": 2.410178005897128,
      "grad_norm": 0.11622858792543411,
      "learning_rate": 3.7952932183029374e-05,
      "loss": 2.8471,
      "step": 22070
    },
    {
      "epoch": 2.4112700666157036,
      "grad_norm": 0.1802794486284256,
      "learning_rate": 3.7947471879436494e-05,
      "loss": 2.8797,
      "step": 22080
    },
    {
      "epoch": 2.41236212733428,
      "grad_norm": 0.3786006271839142,
      "learning_rate": 3.794201157584362e-05,
      "loss": 2.849,
      "step": 22090
    },
    {
      "epoch": 2.4134541880528557,
      "grad_norm": 0.17129230499267578,
      "learning_rate": 3.793655127225074e-05,
      "loss": 2.863,
      "step": 22100
    },
    {
      "epoch": 2.4145462487714315,
      "grad_norm": 0.16991442441940308,
      "learning_rate": 3.793109096865786e-05,
      "loss": 2.8686,
      "step": 22110
    },
    {
      "epoch": 2.415638309490008,
      "grad_norm": 0.1672705113887787,
      "learning_rate": 3.7925630665064985e-05,
      "loss": 2.8586,
      "step": 22120
    },
    {
      "epoch": 2.4167303702085836,
      "grad_norm": 0.3584607243537903,
      "learning_rate": 3.7920170361472104e-05,
      "loss": 2.8489,
      "step": 22130
    },
    {
      "epoch": 2.4178224309271594,
      "grad_norm": 0.3575688600540161,
      "learning_rate": 3.7914710057879224e-05,
      "loss": 2.901,
      "step": 22140
    },
    {
      "epoch": 2.4189144916457357,
      "grad_norm": 0.13283483684062958,
      "learning_rate": 3.790924975428634e-05,
      "loss": 2.867,
      "step": 22150
    },
    {
      "epoch": 2.4200065523643115,
      "grad_norm": 0.26210901141166687,
      "learning_rate": 3.790378945069346e-05,
      "loss": 2.8644,
      "step": 22160
    },
    {
      "epoch": 2.4210986130828873,
      "grad_norm": 0.1321282833814621,
      "learning_rate": 3.789832914710058e-05,
      "loss": 2.8521,
      "step": 22170
    },
    {
      "epoch": 2.422190673801463,
      "grad_norm": 0.1573895663022995,
      "learning_rate": 3.78928688435077e-05,
      "loss": 2.8616,
      "step": 22180
    },
    {
      "epoch": 2.4232827345200394,
      "grad_norm": 0.09217941761016846,
      "learning_rate": 3.788740853991482e-05,
      "loss": 2.8489,
      "step": 22190
    },
    {
      "epoch": 2.424374795238615,
      "grad_norm": 0.1792108416557312,
      "learning_rate": 3.788194823632194e-05,
      "loss": 2.8487,
      "step": 22200
    },
    {
      "epoch": 2.425466855957191,
      "grad_norm": 0.22370655834674835,
      "learning_rate": 3.787648793272906e-05,
      "loss": 2.8316,
      "step": 22210
    },
    {
      "epoch": 2.4265589166757673,
      "grad_norm": 0.17925791442394257,
      "learning_rate": 3.7871573659495466e-05,
      "loss": 2.8943,
      "step": 22220
    },
    {
      "epoch": 2.427650977394343,
      "grad_norm": 0.23757095634937286,
      "learning_rate": 3.7866113355902586e-05,
      "loss": 2.8486,
      "step": 22230
    },
    {
      "epoch": 2.428743038112919,
      "grad_norm": 0.14933690428733826,
      "learning_rate": 3.7860653052309705e-05,
      "loss": 2.8423,
      "step": 22240
    },
    {
      "epoch": 2.429835098831495,
      "grad_norm": 0.28711727261543274,
      "learning_rate": 3.7855192748716825e-05,
      "loss": 2.8855,
      "step": 22250
    },
    {
      "epoch": 2.430927159550071,
      "grad_norm": 0.19954700767993927,
      "learning_rate": 3.784973244512395e-05,
      "loss": 2.8368,
      "step": 22260
    },
    {
      "epoch": 2.432019220268647,
      "grad_norm": 0.14851030707359314,
      "learning_rate": 3.784427214153107e-05,
      "loss": 2.8236,
      "step": 22270
    },
    {
      "epoch": 2.433111280987223,
      "grad_norm": 0.40339237451553345,
      "learning_rate": 3.783881183793819e-05,
      "loss": 2.8954,
      "step": 22280
    },
    {
      "epoch": 2.434203341705799,
      "grad_norm": 0.246980220079422,
      "learning_rate": 3.7833351534345316e-05,
      "loss": 2.8378,
      "step": 22290
    },
    {
      "epoch": 2.4352954024243747,
      "grad_norm": 0.4115878939628601,
      "learning_rate": 3.7827891230752435e-05,
      "loss": 2.8654,
      "step": 22300
    },
    {
      "epoch": 2.4363874631429505,
      "grad_norm": 0.12362823635339737,
      "learning_rate": 3.7822430927159555e-05,
      "loss": 2.9216,
      "step": 22310
    },
    {
      "epoch": 2.4374795238615268,
      "grad_norm": 0.2830134630203247,
      "learning_rate": 3.7816970623566674e-05,
      "loss": 2.866,
      "step": 22320
    },
    {
      "epoch": 2.4385715845801026,
      "grad_norm": 0.10903408378362656,
      "learning_rate": 3.7811510319973793e-05,
      "loss": 2.8396,
      "step": 22330
    },
    {
      "epoch": 2.439663645298679,
      "grad_norm": 0.19581033289432526,
      "learning_rate": 3.780605001638091e-05,
      "loss": 2.8597,
      "step": 22340
    },
    {
      "epoch": 2.4407557060172547,
      "grad_norm": 0.44914132356643677,
      "learning_rate": 3.780058971278803e-05,
      "loss": 2.8858,
      "step": 22350
    },
    {
      "epoch": 2.4418477667358305,
      "grad_norm": 0.16047321259975433,
      "learning_rate": 3.779512940919515e-05,
      "loss": 2.8358,
      "step": 22360
    },
    {
      "epoch": 2.4429398274544063,
      "grad_norm": 0.1267663836479187,
      "learning_rate": 3.778966910560227e-05,
      "loss": 2.8591,
      "step": 22370
    },
    {
      "epoch": 2.4440318881729826,
      "grad_norm": 0.42162686586380005,
      "learning_rate": 3.778420880200939e-05,
      "loss": 2.8627,
      "step": 22380
    },
    {
      "epoch": 2.4451239488915584,
      "grad_norm": 0.11404111236333847,
      "learning_rate": 3.777874849841652e-05,
      "loss": 2.8693,
      "step": 22390
    },
    {
      "epoch": 2.446216009610134,
      "grad_norm": 0.25999972224235535,
      "learning_rate": 3.7773288194823636e-05,
      "loss": 2.9333,
      "step": 22400
    },
    {
      "epoch": 2.4473080703287104,
      "grad_norm": 0.21707391738891602,
      "learning_rate": 3.7767827891230755e-05,
      "loss": 2.8966,
      "step": 22410
    },
    {
      "epoch": 2.4484001310472863,
      "grad_norm": 0.12823975086212158,
      "learning_rate": 3.7762367587637875e-05,
      "loss": 2.8487,
      "step": 22420
    },
    {
      "epoch": 2.449492191765862,
      "grad_norm": 0.2167188674211502,
      "learning_rate": 3.7756907284044994e-05,
      "loss": 2.8382,
      "step": 22430
    },
    {
      "epoch": 2.450584252484438,
      "grad_norm": 0.16972815990447998,
      "learning_rate": 3.7751446980452114e-05,
      "loss": 2.8683,
      "step": 22440
    },
    {
      "epoch": 2.451676313203014,
      "grad_norm": 0.3032892346382141,
      "learning_rate": 3.774598667685923e-05,
      "loss": 2.8811,
      "step": 22450
    },
    {
      "epoch": 2.45276837392159,
      "grad_norm": 0.25806665420532227,
      "learning_rate": 3.774052637326635e-05,
      "loss": 2.8769,
      "step": 22460
    },
    {
      "epoch": 2.4538604346401662,
      "grad_norm": 0.3346346616744995,
      "learning_rate": 3.773506606967347e-05,
      "loss": 2.863,
      "step": 22470
    },
    {
      "epoch": 2.454952495358742,
      "grad_norm": 0.6476027369499207,
      "learning_rate": 3.772960576608059e-05,
      "loss": 2.8512,
      "step": 22480
    },
    {
      "epoch": 2.456044556077318,
      "grad_norm": 0.2893872559070587,
      "learning_rate": 3.772414546248772e-05,
      "loss": 2.8806,
      "step": 22490
    },
    {
      "epoch": 2.4571366167958937,
      "grad_norm": 0.11687982082366943,
      "learning_rate": 3.771868515889484e-05,
      "loss": 2.8622,
      "step": 22500
    },
    {
      "epoch": 2.45822867751447,
      "grad_norm": 0.19156542420387268,
      "learning_rate": 3.7713224855301956e-05,
      "loss": 2.8199,
      "step": 22510
    },
    {
      "epoch": 2.4593207382330458,
      "grad_norm": 0.3425903022289276,
      "learning_rate": 3.770776455170908e-05,
      "loss": 2.8628,
      "step": 22520
    },
    {
      "epoch": 2.4604127989516216,
      "grad_norm": 0.19672791659832,
      "learning_rate": 3.77023042481162e-05,
      "loss": 2.8455,
      "step": 22530
    },
    {
      "epoch": 2.461504859670198,
      "grad_norm": 0.08329205960035324,
      "learning_rate": 3.769684394452332e-05,
      "loss": 2.8713,
      "step": 22540
    },
    {
      "epoch": 2.4625969203887736,
      "grad_norm": 0.10646489262580872,
      "learning_rate": 3.769138364093044e-05,
      "loss": 2.8265,
      "step": 22550
    },
    {
      "epoch": 2.4636889811073495,
      "grad_norm": 0.6003637909889221,
      "learning_rate": 3.768592333733756e-05,
      "loss": 2.8522,
      "step": 22560
    },
    {
      "epoch": 2.4647810418259253,
      "grad_norm": 0.1372516006231308,
      "learning_rate": 3.768046303374468e-05,
      "loss": 2.8542,
      "step": 22570
    },
    {
      "epoch": 2.4658731025445015,
      "grad_norm": 0.17595691978931427,
      "learning_rate": 3.76750027301518e-05,
      "loss": 2.8397,
      "step": 22580
    },
    {
      "epoch": 2.4669651632630774,
      "grad_norm": 0.0858013853430748,
      "learning_rate": 3.766954242655892e-05,
      "loss": 2.8533,
      "step": 22590
    },
    {
      "epoch": 2.4680572239816536,
      "grad_norm": 0.1208936795592308,
      "learning_rate": 3.766408212296604e-05,
      "loss": 2.8447,
      "step": 22600
    },
    {
      "epoch": 2.4691492847002294,
      "grad_norm": 0.21385525166988373,
      "learning_rate": 3.765862181937316e-05,
      "loss": 2.8561,
      "step": 22610
    },
    {
      "epoch": 2.4702413454188052,
      "grad_norm": 0.30614399909973145,
      "learning_rate": 3.7653161515780277e-05,
      "loss": 2.8683,
      "step": 22620
    },
    {
      "epoch": 2.471333406137381,
      "grad_norm": 0.1550077348947525,
      "learning_rate": 3.7647701212187396e-05,
      "loss": 2.8911,
      "step": 22630
    },
    {
      "epoch": 2.4724254668559573,
      "grad_norm": 0.18009543418884277,
      "learning_rate": 3.7642240908594515e-05,
      "loss": 2.8329,
      "step": 22640
    },
    {
      "epoch": 2.473517527574533,
      "grad_norm": 0.16678348183631897,
      "learning_rate": 3.763678060500164e-05,
      "loss": 2.8602,
      "step": 22650
    },
    {
      "epoch": 2.474609588293109,
      "grad_norm": 0.2204393446445465,
      "learning_rate": 3.763132030140876e-05,
      "loss": 2.8549,
      "step": 22660
    },
    {
      "epoch": 2.475701649011685,
      "grad_norm": 0.33252209424972534,
      "learning_rate": 3.762585999781588e-05,
      "loss": 2.8455,
      "step": 22670
    },
    {
      "epoch": 2.476793709730261,
      "grad_norm": 0.3624953627586365,
      "learning_rate": 3.7620399694223e-05,
      "loss": 2.8553,
      "step": 22680
    },
    {
      "epoch": 2.477885770448837,
      "grad_norm": 0.8477385640144348,
      "learning_rate": 3.761493939063012e-05,
      "loss": 2.852,
      "step": 22690
    },
    {
      "epoch": 2.4789778311674127,
      "grad_norm": 0.12606333196163177,
      "learning_rate": 3.760947908703724e-05,
      "loss": 2.906,
      "step": 22700
    },
    {
      "epoch": 2.480069891885989,
      "grad_norm": 0.14026403427124023,
      "learning_rate": 3.760401878344436e-05,
      "loss": 2.842,
      "step": 22710
    },
    {
      "epoch": 2.4811619526045647,
      "grad_norm": 0.223413348197937,
      "learning_rate": 3.7598558479851484e-05,
      "loss": 2.8665,
      "step": 22720
    },
    {
      "epoch": 2.482254013323141,
      "grad_norm": 0.11895720660686493,
      "learning_rate": 3.7593098176258604e-05,
      "loss": 2.8733,
      "step": 22730
    },
    {
      "epoch": 2.483346074041717,
      "grad_norm": 0.12075629830360413,
      "learning_rate": 3.758763787266572e-05,
      "loss": 2.8476,
      "step": 22740
    },
    {
      "epoch": 2.4844381347602926,
      "grad_norm": 0.4224436283111572,
      "learning_rate": 3.758217756907284e-05,
      "loss": 2.8512,
      "step": 22750
    },
    {
      "epoch": 2.4855301954788684,
      "grad_norm": 0.5905702114105225,
      "learning_rate": 3.757671726547996e-05,
      "loss": 2.8415,
      "step": 22760
    },
    {
      "epoch": 2.4866222561974447,
      "grad_norm": 0.1189352348446846,
      "learning_rate": 3.757125696188708e-05,
      "loss": 2.8712,
      "step": 22770
    },
    {
      "epoch": 2.4877143169160205,
      "grad_norm": 0.12626059353351593,
      "learning_rate": 3.756579665829421e-05,
      "loss": 2.8639,
      "step": 22780
    },
    {
      "epoch": 2.4888063776345963,
      "grad_norm": 0.17619779706001282,
      "learning_rate": 3.756033635470133e-05,
      "loss": 2.8333,
      "step": 22790
    },
    {
      "epoch": 2.4898984383531726,
      "grad_norm": 0.2657283842563629,
      "learning_rate": 3.7554876051108446e-05,
      "loss": 2.8798,
      "step": 22800
    },
    {
      "epoch": 2.4909904990717484,
      "grad_norm": 0.14739935100078583,
      "learning_rate": 3.7549415747515566e-05,
      "loss": 2.827,
      "step": 22810
    },
    {
      "epoch": 2.4920825597903242,
      "grad_norm": 0.2964778542518616,
      "learning_rate": 3.7543955443922685e-05,
      "loss": 2.861,
      "step": 22820
    },
    {
      "epoch": 2.4931746205089005,
      "grad_norm": 0.19962386786937714,
      "learning_rate": 3.7538495140329805e-05,
      "loss": 2.8343,
      "step": 22830
    },
    {
      "epoch": 2.4942666812274763,
      "grad_norm": 0.21907295286655426,
      "learning_rate": 3.7533034836736924e-05,
      "loss": 2.8572,
      "step": 22840
    },
    {
      "epoch": 2.495358741946052,
      "grad_norm": 0.19064615666866302,
      "learning_rate": 3.752757453314404e-05,
      "loss": 2.8622,
      "step": 22850
    },
    {
      "epoch": 2.4964508026646284,
      "grad_norm": 0.21872812509536743,
      "learning_rate": 3.752211422955116e-05,
      "loss": 2.8365,
      "step": 22860
    },
    {
      "epoch": 2.497542863383204,
      "grad_norm": 0.2866363525390625,
      "learning_rate": 3.751665392595828e-05,
      "loss": 2.8568,
      "step": 22870
    },
    {
      "epoch": 2.49863492410178,
      "grad_norm": 0.14861522614955902,
      "learning_rate": 3.75111936223654e-05,
      "loss": 2.8663,
      "step": 22880
    },
    {
      "epoch": 2.499726984820356,
      "grad_norm": 0.20287644863128662,
      "learning_rate": 3.750573331877252e-05,
      "loss": 2.8437,
      "step": 22890
    },
    {
      "epoch": 2.500819045538932,
      "grad_norm": 0.08543781191110611,
      "learning_rate": 3.750027301517964e-05,
      "loss": 2.8437,
      "step": 22900
    },
    {
      "epoch": 2.501911106257508,
      "grad_norm": 0.17027077078819275,
      "learning_rate": 3.7494812711586767e-05,
      "loss": 2.8898,
      "step": 22910
    },
    {
      "epoch": 2.5030031669760837,
      "grad_norm": 0.19321776926517487,
      "learning_rate": 3.7489352407993886e-05,
      "loss": 2.8543,
      "step": 22920
    },
    {
      "epoch": 2.50409522769466,
      "grad_norm": 0.219878152012825,
      "learning_rate": 3.7483892104401005e-05,
      "loss": 2.8848,
      "step": 22930
    },
    {
      "epoch": 2.505187288413236,
      "grad_norm": 0.10109686851501465,
      "learning_rate": 3.7478431800808125e-05,
      "loss": 2.8374,
      "step": 22940
    },
    {
      "epoch": 2.5062793491318116,
      "grad_norm": 0.10451056063175201,
      "learning_rate": 3.747297149721525e-05,
      "loss": 2.8311,
      "step": 22950
    },
    {
      "epoch": 2.5073714098503874,
      "grad_norm": 0.23859819769859314,
      "learning_rate": 3.746751119362237e-05,
      "loss": 2.8816,
      "step": 22960
    },
    {
      "epoch": 2.5084634705689637,
      "grad_norm": 0.20528188347816467,
      "learning_rate": 3.746205089002949e-05,
      "loss": 2.8382,
      "step": 22970
    },
    {
      "epoch": 2.5095555312875395,
      "grad_norm": 0.37676775455474854,
      "learning_rate": 3.745659058643661e-05,
      "loss": 2.9098,
      "step": 22980
    },
    {
      "epoch": 2.5106475920061158,
      "grad_norm": 0.13029663264751434,
      "learning_rate": 3.745113028284373e-05,
      "loss": 2.832,
      "step": 22990
    },
    {
      "epoch": 2.5117396527246916,
      "grad_norm": 0.15636302530765533,
      "learning_rate": 3.744566997925085e-05,
      "loss": 2.8751,
      "step": 23000
    },
    {
      "epoch": 2.5128317134432674,
      "grad_norm": 0.11414133012294769,
      "learning_rate": 3.744020967565797e-05,
      "loss": 2.8615,
      "step": 23010
    },
    {
      "epoch": 2.513923774161843,
      "grad_norm": 0.10634645074605942,
      "learning_rate": 3.743474937206509e-05,
      "loss": 2.8283,
      "step": 23020
    },
    {
      "epoch": 2.5150158348804195,
      "grad_norm": 0.2630913257598877,
      "learning_rate": 3.7429289068472206e-05,
      "loss": 2.8668,
      "step": 23030
    },
    {
      "epoch": 2.5161078955989953,
      "grad_norm": 0.2566410303115845,
      "learning_rate": 3.742382876487933e-05,
      "loss": 2.8497,
      "step": 23040
    },
    {
      "epoch": 2.517199956317571,
      "grad_norm": 0.12176626920700073,
      "learning_rate": 3.741836846128645e-05,
      "loss": 2.8717,
      "step": 23050
    },
    {
      "epoch": 2.5182920170361474,
      "grad_norm": 0.16431403160095215,
      "learning_rate": 3.741290815769357e-05,
      "loss": 2.8478,
      "step": 23060
    },
    {
      "epoch": 2.519384077754723,
      "grad_norm": 0.926967203617096,
      "learning_rate": 3.740744785410069e-05,
      "loss": 2.8971,
      "step": 23070
    },
    {
      "epoch": 2.520476138473299,
      "grad_norm": 0.15080779790878296,
      "learning_rate": 3.740198755050781e-05,
      "loss": 2.8942,
      "step": 23080
    },
    {
      "epoch": 2.521568199191875,
      "grad_norm": 0.28501543402671814,
      "learning_rate": 3.739652724691493e-05,
      "loss": 2.8998,
      "step": 23090
    },
    {
      "epoch": 2.522660259910451,
      "grad_norm": 0.16508817672729492,
      "learning_rate": 3.739106694332205e-05,
      "loss": 2.8695,
      "step": 23100
    },
    {
      "epoch": 2.523752320629027,
      "grad_norm": 0.1274852454662323,
      "learning_rate": 3.738560663972917e-05,
      "loss": 2.8613,
      "step": 23110
    },
    {
      "epoch": 2.524844381347603,
      "grad_norm": 0.16024623811244965,
      "learning_rate": 3.738014633613629e-05,
      "loss": 2.8946,
      "step": 23120
    },
    {
      "epoch": 2.525936442066179,
      "grad_norm": 0.15536284446716309,
      "learning_rate": 3.737468603254341e-05,
      "loss": 2.858,
      "step": 23130
    },
    {
      "epoch": 2.5270285027847548,
      "grad_norm": 0.21541599929332733,
      "learning_rate": 3.7369225728950527e-05,
      "loss": 2.8788,
      "step": 23140
    },
    {
      "epoch": 2.5281205635033306,
      "grad_norm": 0.31060007214546204,
      "learning_rate": 3.736376542535765e-05,
      "loss": 2.8926,
      "step": 23150
    },
    {
      "epoch": 2.529212624221907,
      "grad_norm": 0.14576318860054016,
      "learning_rate": 3.735830512176477e-05,
      "loss": 2.8459,
      "step": 23160
    },
    {
      "epoch": 2.5303046849404827,
      "grad_norm": 0.9053961634635925,
      "learning_rate": 3.735284481817189e-05,
      "loss": 2.9294,
      "step": 23170
    },
    {
      "epoch": 2.5313967456590585,
      "grad_norm": 0.4255218505859375,
      "learning_rate": 3.734738451457902e-05,
      "loss": 2.9236,
      "step": 23180
    },
    {
      "epoch": 2.5324888063776347,
      "grad_norm": 0.26012781262397766,
      "learning_rate": 3.734192421098614e-05,
      "loss": 2.8478,
      "step": 23190
    },
    {
      "epoch": 2.5335808670962106,
      "grad_norm": 0.2374165803194046,
      "learning_rate": 3.7336463907393257e-05,
      "loss": 2.9039,
      "step": 23200
    },
    {
      "epoch": 2.5346729278147864,
      "grad_norm": 0.31833890080451965,
      "learning_rate": 3.7331003603800376e-05,
      "loss": 2.8831,
      "step": 23210
    },
    {
      "epoch": 2.535764988533362,
      "grad_norm": 0.17793744802474976,
      "learning_rate": 3.7325543300207495e-05,
      "loss": 2.8605,
      "step": 23220
    },
    {
      "epoch": 2.5368570492519384,
      "grad_norm": 0.2410343885421753,
      "learning_rate": 3.7320082996614615e-05,
      "loss": 2.8798,
      "step": 23230
    },
    {
      "epoch": 2.5379491099705143,
      "grad_norm": 0.11693835258483887,
      "learning_rate": 3.7314622693021734e-05,
      "loss": 2.8365,
      "step": 23240
    },
    {
      "epoch": 2.5390411706890905,
      "grad_norm": 0.32874777913093567,
      "learning_rate": 3.7309162389428854e-05,
      "loss": 2.9072,
      "step": 23250
    },
    {
      "epoch": 2.5401332314076663,
      "grad_norm": 0.19088561832904816,
      "learning_rate": 3.730370208583597e-05,
      "loss": 2.8664,
      "step": 23260
    },
    {
      "epoch": 2.541225292126242,
      "grad_norm": 0.11188801378011703,
      "learning_rate": 3.729824178224309e-05,
      "loss": 2.8912,
      "step": 23270
    },
    {
      "epoch": 2.542317352844818,
      "grad_norm": 0.10281196236610413,
      "learning_rate": 3.729278147865021e-05,
      "loss": 2.8667,
      "step": 23280
    },
    {
      "epoch": 2.5434094135633942,
      "grad_norm": 0.18437029421329498,
      "learning_rate": 3.728732117505733e-05,
      "loss": 2.8294,
      "step": 23290
    },
    {
      "epoch": 2.54450147428197,
      "grad_norm": 0.1792355626821518,
      "learning_rate": 3.728186087146446e-05,
      "loss": 2.8459,
      "step": 23300
    },
    {
      "epoch": 2.5455935350005463,
      "grad_norm": 0.15524405241012573,
      "learning_rate": 3.727640056787158e-05,
      "loss": 2.8334,
      "step": 23310
    },
    {
      "epoch": 2.546685595719122,
      "grad_norm": 0.13917480409145355,
      "learning_rate": 3.7270940264278696e-05,
      "loss": 3.0293,
      "step": 23320
    },
    {
      "epoch": 2.547777656437698,
      "grad_norm": 0.1370607316493988,
      "learning_rate": 3.7265479960685816e-05,
      "loss": 2.8869,
      "step": 23330
    },
    {
      "epoch": 2.5488697171562738,
      "grad_norm": 0.18411007523536682,
      "learning_rate": 3.7260019657092935e-05,
      "loss": 2.8852,
      "step": 23340
    },
    {
      "epoch": 2.5499617778748496,
      "grad_norm": 0.21324273943901062,
      "learning_rate": 3.7254559353500055e-05,
      "loss": 2.8683,
      "step": 23350
    },
    {
      "epoch": 2.551053838593426,
      "grad_norm": 0.1513669788837433,
      "learning_rate": 3.7249099049907174e-05,
      "loss": 2.9195,
      "step": 23360
    },
    {
      "epoch": 2.5521458993120016,
      "grad_norm": 0.17860881984233856,
      "learning_rate": 3.724363874631429e-05,
      "loss": 2.8903,
      "step": 23370
    },
    {
      "epoch": 2.553237960030578,
      "grad_norm": 0.15012234449386597,
      "learning_rate": 3.723817844272142e-05,
      "loss": 2.8551,
      "step": 23380
    },
    {
      "epoch": 2.5543300207491537,
      "grad_norm": 0.3222147822380066,
      "learning_rate": 3.723271813912854e-05,
      "loss": 2.8546,
      "step": 23390
    },
    {
      "epoch": 2.5554220814677295,
      "grad_norm": 0.3434353768825531,
      "learning_rate": 3.722725783553566e-05,
      "loss": 2.9013,
      "step": 23400
    },
    {
      "epoch": 2.5565141421863053,
      "grad_norm": 0.37971213459968567,
      "learning_rate": 3.722179753194278e-05,
      "loss": 2.893,
      "step": 23410
    },
    {
      "epoch": 2.5576062029048816,
      "grad_norm": 0.194835364818573,
      "learning_rate": 3.72163372283499e-05,
      "loss": 2.8572,
      "step": 23420
    },
    {
      "epoch": 2.5586982636234574,
      "grad_norm": 0.2668936848640442,
      "learning_rate": 3.721087692475702e-05,
      "loss": 2.8572,
      "step": 23430
    },
    {
      "epoch": 2.5597903243420337,
      "grad_norm": 0.32296988368034363,
      "learning_rate": 3.720541662116414e-05,
      "loss": 2.8389,
      "step": 23440
    },
    {
      "epoch": 2.5608823850606095,
      "grad_norm": 0.16601647436618805,
      "learning_rate": 3.719995631757126e-05,
      "loss": 2.8619,
      "step": 23450
    },
    {
      "epoch": 2.5619744457791853,
      "grad_norm": 0.26828789710998535,
      "learning_rate": 3.719449601397838e-05,
      "loss": 2.8653,
      "step": 23460
    },
    {
      "epoch": 2.563066506497761,
      "grad_norm": 0.14486919343471527,
      "learning_rate": 3.71890357103855e-05,
      "loss": 2.8856,
      "step": 23470
    },
    {
      "epoch": 2.5641585672163374,
      "grad_norm": 0.2612631320953369,
      "learning_rate": 3.718357540679262e-05,
      "loss": 2.8878,
      "step": 23480
    },
    {
      "epoch": 2.565250627934913,
      "grad_norm": 0.13374531269073486,
      "learning_rate": 3.717811510319974e-05,
      "loss": 2.8455,
      "step": 23490
    },
    {
      "epoch": 2.566342688653489,
      "grad_norm": 0.3090677261352539,
      "learning_rate": 3.717265479960686e-05,
      "loss": 2.8605,
      "step": 23500
    },
    {
      "epoch": 2.5674347493720653,
      "grad_norm": 0.36417537927627563,
      "learning_rate": 3.716719449601398e-05,
      "loss": 2.8475,
      "step": 23510
    },
    {
      "epoch": 2.568526810090641,
      "grad_norm": 0.48602062463760376,
      "learning_rate": 3.71617341924211e-05,
      "loss": 2.8468,
      "step": 23520
    },
    {
      "epoch": 2.569618870809217,
      "grad_norm": 0.1822146773338318,
      "learning_rate": 3.715627388882822e-05,
      "loss": 2.846,
      "step": 23530
    },
    {
      "epoch": 2.5707109315277927,
      "grad_norm": 0.1744893491268158,
      "learning_rate": 3.715081358523534e-05,
      "loss": 2.8252,
      "step": 23540
    },
    {
      "epoch": 2.571802992246369,
      "grad_norm": 0.13982602953910828,
      "learning_rate": 3.7145353281642456e-05,
      "loss": 2.8609,
      "step": 23550
    },
    {
      "epoch": 2.572895052964945,
      "grad_norm": 0.1585211455821991,
      "learning_rate": 3.7139892978049576e-05,
      "loss": 2.8393,
      "step": 23560
    },
    {
      "epoch": 2.573987113683521,
      "grad_norm": 0.14403218030929565,
      "learning_rate": 3.71344326744567e-05,
      "loss": 2.8453,
      "step": 23570
    },
    {
      "epoch": 2.575079174402097,
      "grad_norm": 0.17544594407081604,
      "learning_rate": 3.712897237086382e-05,
      "loss": 2.8504,
      "step": 23580
    },
    {
      "epoch": 2.5761712351206727,
      "grad_norm": 0.1236477643251419,
      "learning_rate": 3.712351206727094e-05,
      "loss": 2.8604,
      "step": 23590
    },
    {
      "epoch": 2.5772632958392485,
      "grad_norm": 0.14220286905765533,
      "learning_rate": 3.711805176367806e-05,
      "loss": 2.8517,
      "step": 23600
    },
    {
      "epoch": 2.5783553565578248,
      "grad_norm": 0.2765171527862549,
      "learning_rate": 3.7112591460085186e-05,
      "loss": 2.8818,
      "step": 23610
    },
    {
      "epoch": 2.5794474172764006,
      "grad_norm": 0.45718914270401,
      "learning_rate": 3.7107131156492306e-05,
      "loss": 2.88,
      "step": 23620
    },
    {
      "epoch": 2.5805394779949764,
      "grad_norm": 0.23829731345176697,
      "learning_rate": 3.7101670852899425e-05,
      "loss": 2.928,
      "step": 23630
    },
    {
      "epoch": 2.5816315387135527,
      "grad_norm": 0.10987195372581482,
      "learning_rate": 3.7096210549306544e-05,
      "loss": 2.8664,
      "step": 23640
    },
    {
      "epoch": 2.5827235994321285,
      "grad_norm": 0.2789669930934906,
      "learning_rate": 3.7090750245713664e-05,
      "loss": 2.879,
      "step": 23650
    },
    {
      "epoch": 2.5838156601507043,
      "grad_norm": 0.19158190488815308,
      "learning_rate": 3.708528994212078e-05,
      "loss": 2.8733,
      "step": 23660
    },
    {
      "epoch": 2.58490772086928,
      "grad_norm": 0.28963807225227356,
      "learning_rate": 3.70798296385279e-05,
      "loss": 2.8587,
      "step": 23670
    },
    {
      "epoch": 2.5859997815878564,
      "grad_norm": 0.09961053729057312,
      "learning_rate": 3.707436933493502e-05,
      "loss": 2.8768,
      "step": 23680
    },
    {
      "epoch": 2.587091842306432,
      "grad_norm": 0.19096775352954865,
      "learning_rate": 3.706890903134214e-05,
      "loss": 2.8577,
      "step": 23690
    },
    {
      "epoch": 2.5881839030250084,
      "grad_norm": 0.08703254163265228,
      "learning_rate": 3.706344872774927e-05,
      "loss": 2.8411,
      "step": 23700
    },
    {
      "epoch": 2.5892759637435843,
      "grad_norm": 0.11671635508537292,
      "learning_rate": 3.705798842415639e-05,
      "loss": 2.8762,
      "step": 23710
    },
    {
      "epoch": 2.59036802446216,
      "grad_norm": 0.2970370054244995,
      "learning_rate": 3.7052528120563507e-05,
      "loss": 2.8633,
      "step": 23720
    },
    {
      "epoch": 2.591460085180736,
      "grad_norm": 0.14920896291732788,
      "learning_rate": 3.7047067816970626e-05,
      "loss": 2.861,
      "step": 23730
    },
    {
      "epoch": 2.592552145899312,
      "grad_norm": 0.4358484745025635,
      "learning_rate": 3.7041607513377745e-05,
      "loss": 2.8719,
      "step": 23740
    },
    {
      "epoch": 2.593644206617888,
      "grad_norm": 0.3236897885799408,
      "learning_rate": 3.7036147209784865e-05,
      "loss": 2.9058,
      "step": 23750
    },
    {
      "epoch": 2.594736267336464,
      "grad_norm": 0.1799726039171219,
      "learning_rate": 3.7030686906191984e-05,
      "loss": 2.8847,
      "step": 23760
    },
    {
      "epoch": 2.59582832805504,
      "grad_norm": 0.20940080285072327,
      "learning_rate": 3.7025226602599104e-05,
      "loss": 2.8295,
      "step": 23770
    },
    {
      "epoch": 2.596920388773616,
      "grad_norm": 0.489496111869812,
      "learning_rate": 3.701976629900622e-05,
      "loss": 2.8682,
      "step": 23780
    },
    {
      "epoch": 2.5980124494921917,
      "grad_norm": 0.3060218095779419,
      "learning_rate": 3.701430599541334e-05,
      "loss": 2.9312,
      "step": 23790
    },
    {
      "epoch": 2.5991045102107675,
      "grad_norm": 0.29217055439949036,
      "learning_rate": 3.700884569182046e-05,
      "loss": 2.8981,
      "step": 23800
    },
    {
      "epoch": 2.6001965709293438,
      "grad_norm": 0.1832924634218216,
      "learning_rate": 3.700338538822759e-05,
      "loss": 2.8526,
      "step": 23810
    },
    {
      "epoch": 2.6012886316479196,
      "grad_norm": 0.3996177017688751,
      "learning_rate": 3.699792508463471e-05,
      "loss": 2.8521,
      "step": 23820
    },
    {
      "epoch": 2.602380692366496,
      "grad_norm": 0.17477022111415863,
      "learning_rate": 3.699246478104183e-05,
      "loss": 2.8445,
      "step": 23830
    },
    {
      "epoch": 2.6034727530850716,
      "grad_norm": 0.23209668695926666,
      "learning_rate": 3.698700447744895e-05,
      "loss": 2.9341,
      "step": 23840
    },
    {
      "epoch": 2.6045648138036475,
      "grad_norm": 0.16133345663547516,
      "learning_rate": 3.698154417385607e-05,
      "loss": 2.8565,
      "step": 23850
    },
    {
      "epoch": 2.6056568745222233,
      "grad_norm": 0.8408934473991394,
      "learning_rate": 3.697608387026319e-05,
      "loss": 2.8948,
      "step": 23860
    },
    {
      "epoch": 2.6067489352407995,
      "grad_norm": 0.1373942494392395,
      "learning_rate": 3.697062356667031e-05,
      "loss": 2.8433,
      "step": 23870
    },
    {
      "epoch": 2.6078409959593754,
      "grad_norm": 0.27931472659111023,
      "learning_rate": 3.696516326307743e-05,
      "loss": 2.931,
      "step": 23880
    },
    {
      "epoch": 2.608933056677951,
      "grad_norm": 0.23359353840351105,
      "learning_rate": 3.695970295948455e-05,
      "loss": 2.8752,
      "step": 23890
    },
    {
      "epoch": 2.6100251173965274,
      "grad_norm": 0.20301353931427002,
      "learning_rate": 3.695424265589167e-05,
      "loss": 2.875,
      "step": 23900
    },
    {
      "epoch": 2.6111171781151032,
      "grad_norm": 0.11804252117872238,
      "learning_rate": 3.694878235229879e-05,
      "loss": 2.8427,
      "step": 23910
    },
    {
      "epoch": 2.612209238833679,
      "grad_norm": 0.186376690864563,
      "learning_rate": 3.694332204870591e-05,
      "loss": 2.8287,
      "step": 23920
    },
    {
      "epoch": 2.613301299552255,
      "grad_norm": 0.2362319529056549,
      "learning_rate": 3.693786174511303e-05,
      "loss": 2.8806,
      "step": 23930
    },
    {
      "epoch": 2.614393360270831,
      "grad_norm": 0.21075893938541412,
      "learning_rate": 3.693240144152015e-05,
      "loss": 2.8399,
      "step": 23940
    },
    {
      "epoch": 2.615485420989407,
      "grad_norm": 0.2943744659423828,
      "learning_rate": 3.6926941137927267e-05,
      "loss": 2.8732,
      "step": 23950
    },
    {
      "epoch": 2.616577481707983,
      "grad_norm": 0.13378308713436127,
      "learning_rate": 3.692148083433439e-05,
      "loss": 2.8484,
      "step": 23960
    },
    {
      "epoch": 2.617669542426559,
      "grad_norm": 0.21293343603610992,
      "learning_rate": 3.691602053074151e-05,
      "loss": 2.8954,
      "step": 23970
    },
    {
      "epoch": 2.618761603145135,
      "grad_norm": 0.7652512788772583,
      "learning_rate": 3.691056022714863e-05,
      "loss": 2.8718,
      "step": 23980
    },
    {
      "epoch": 2.6198536638637107,
      "grad_norm": 0.14411142468452454,
      "learning_rate": 3.690509992355575e-05,
      "loss": 2.8637,
      "step": 23990
    },
    {
      "epoch": 2.620945724582287,
      "grad_norm": 0.1316261887550354,
      "learning_rate": 3.689963961996287e-05,
      "loss": 2.8546,
      "step": 24000
    },
    {
      "epoch": 2.6220377853008627,
      "grad_norm": 0.1273469775915146,
      "learning_rate": 3.689417931636999e-05,
      "loss": 2.8403,
      "step": 24010
    },
    {
      "epoch": 2.6231298460194385,
      "grad_norm": 0.17194204032421112,
      "learning_rate": 3.688871901277711e-05,
      "loss": 2.9034,
      "step": 24020
    },
    {
      "epoch": 2.624221906738015,
      "grad_norm": 0.621938169002533,
      "learning_rate": 3.688325870918423e-05,
      "loss": 2.8723,
      "step": 24030
    },
    {
      "epoch": 2.6253139674565906,
      "grad_norm": 0.17044000327587128,
      "learning_rate": 3.6877798405591355e-05,
      "loss": 2.8508,
      "step": 24040
    },
    {
      "epoch": 2.6264060281751664,
      "grad_norm": 0.09855008870363235,
      "learning_rate": 3.6872338101998474e-05,
      "loss": 2.862,
      "step": 24050
    },
    {
      "epoch": 2.6274980888937423,
      "grad_norm": 0.14672526717185974,
      "learning_rate": 3.6866877798405594e-05,
      "loss": 2.8757,
      "step": 24060
    },
    {
      "epoch": 2.6285901496123185,
      "grad_norm": 0.29070016741752625,
      "learning_rate": 3.686141749481271e-05,
      "loss": 2.9279,
      "step": 24070
    },
    {
      "epoch": 2.6296822103308943,
      "grad_norm": 0.14449705183506012,
      "learning_rate": 3.685595719121983e-05,
      "loss": 2.8355,
      "step": 24080
    },
    {
      "epoch": 2.6307742710494706,
      "grad_norm": 0.8100995421409607,
      "learning_rate": 3.685049688762696e-05,
      "loss": 2.9414,
      "step": 24090
    },
    {
      "epoch": 2.6318663317680464,
      "grad_norm": 0.27832847833633423,
      "learning_rate": 3.684503658403408e-05,
      "loss": 2.8313,
      "step": 24100
    },
    {
      "epoch": 2.6329583924866222,
      "grad_norm": 0.17208367586135864,
      "learning_rate": 3.68395762804412e-05,
      "loss": 2.8474,
      "step": 24110
    },
    {
      "epoch": 2.634050453205198,
      "grad_norm": 0.4689936935901642,
      "learning_rate": 3.683411597684832e-05,
      "loss": 2.861,
      "step": 24120
    },
    {
      "epoch": 2.6351425139237743,
      "grad_norm": 0.183096781373024,
      "learning_rate": 3.6828655673255436e-05,
      "loss": 2.8701,
      "step": 24130
    },
    {
      "epoch": 2.63623457464235,
      "grad_norm": 0.13117855787277222,
      "learning_rate": 3.6823195369662556e-05,
      "loss": 2.9706,
      "step": 24140
    },
    {
      "epoch": 2.637326635360926,
      "grad_norm": 0.17592813074588776,
      "learning_rate": 3.6817735066069675e-05,
      "loss": 2.8486,
      "step": 24150
    },
    {
      "epoch": 2.638418696079502,
      "grad_norm": 0.21706640720367432,
      "learning_rate": 3.6812274762476794e-05,
      "loss": 2.8566,
      "step": 24160
    },
    {
      "epoch": 2.639510756798078,
      "grad_norm": 0.1650383472442627,
      "learning_rate": 3.6806814458883914e-05,
      "loss": 2.8486,
      "step": 24170
    },
    {
      "epoch": 2.640602817516654,
      "grad_norm": 0.12167593091726303,
      "learning_rate": 3.680135415529103e-05,
      "loss": 2.8324,
      "step": 24180
    },
    {
      "epoch": 2.6416948782352296,
      "grad_norm": 0.19983482360839844,
      "learning_rate": 3.679589385169815e-05,
      "loss": 2.8766,
      "step": 24190
    },
    {
      "epoch": 2.642786938953806,
      "grad_norm": 0.2949382960796356,
      "learning_rate": 3.679043354810527e-05,
      "loss": 2.8605,
      "step": 24200
    },
    {
      "epoch": 2.6438789996723817,
      "grad_norm": 0.2714433968067169,
      "learning_rate": 3.678497324451239e-05,
      "loss": 2.8816,
      "step": 24210
    },
    {
      "epoch": 2.644971060390958,
      "grad_norm": 0.48535990715026855,
      "learning_rate": 3.677951294091952e-05,
      "loss": 2.8594,
      "step": 24220
    },
    {
      "epoch": 2.646063121109534,
      "grad_norm": Infinity,
      "learning_rate": 3.677405263732664e-05,
      "loss": 2.8728,
      "step": 24230
    },
    {
      "epoch": 2.6471551818281096,
      "grad_norm": 0.43413078784942627,
      "learning_rate": 3.676913836409305e-05,
      "loss": 2.8542,
      "step": 24240
    },
    {
      "epoch": 2.6482472425466854,
      "grad_norm": 0.15091216564178467,
      "learning_rate": 3.676367806050017e-05,
      "loss": 2.8905,
      "step": 24250
    },
    {
      "epoch": 2.6493393032652617,
      "grad_norm": 0.16933108866214752,
      "learning_rate": 3.675821775690729e-05,
      "loss": 2.8326,
      "step": 24260
    },
    {
      "epoch": 2.6504313639838375,
      "grad_norm": 0.28401052951812744,
      "learning_rate": 3.675275745331441e-05,
      "loss": 2.8333,
      "step": 24270
    },
    {
      "epoch": 2.6515234247024133,
      "grad_norm": 0.3050503730773926,
      "learning_rate": 3.674729714972153e-05,
      "loss": 2.8674,
      "step": 24280
    },
    {
      "epoch": 2.6526154854209896,
      "grad_norm": 0.8418958783149719,
      "learning_rate": 3.674183684612865e-05,
      "loss": 2.8721,
      "step": 24290
    },
    {
      "epoch": 2.6537075461395654,
      "grad_norm": 0.1489219069480896,
      "learning_rate": 3.673637654253577e-05,
      "loss": 2.8603,
      "step": 24300
    },
    {
      "epoch": 2.654799606858141,
      "grad_norm": 0.1593628227710724,
      "learning_rate": 3.6730916238942887e-05,
      "loss": 2.8709,
      "step": 24310
    },
    {
      "epoch": 2.655891667576717,
      "grad_norm": 0.3019665777683258,
      "learning_rate": 3.6725455935350006e-05,
      "loss": 2.921,
      "step": 24320
    },
    {
      "epoch": 2.6569837282952933,
      "grad_norm": 0.20902566611766815,
      "learning_rate": 3.6719995631757125e-05,
      "loss": 2.8804,
      "step": 24330
    },
    {
      "epoch": 2.658075789013869,
      "grad_norm": 0.16666804254055023,
      "learning_rate": 3.6714535328164245e-05,
      "loss": 2.8827,
      "step": 24340
    },
    {
      "epoch": 2.6591678497324454,
      "grad_norm": 0.1336200088262558,
      "learning_rate": 3.6709075024571364e-05,
      "loss": 2.8599,
      "step": 24350
    },
    {
      "epoch": 2.660259910451021,
      "grad_norm": 0.10958860069513321,
      "learning_rate": 3.670361472097849e-05,
      "loss": 2.8266,
      "step": 24360
    },
    {
      "epoch": 2.661351971169597,
      "grad_norm": 0.352297842502594,
      "learning_rate": 3.669815441738561e-05,
      "loss": 2.8432,
      "step": 24370
    },
    {
      "epoch": 2.662444031888173,
      "grad_norm": 0.07977695763111115,
      "learning_rate": 3.669269411379273e-05,
      "loss": 2.8366,
      "step": 24380
    },
    {
      "epoch": 2.663536092606749,
      "grad_norm": 0.13248717784881592,
      "learning_rate": 3.668723381019985e-05,
      "loss": 2.8921,
      "step": 24390
    },
    {
      "epoch": 2.664628153325325,
      "grad_norm": 0.1307043880224228,
      "learning_rate": 3.668177350660697e-05,
      "loss": 2.8832,
      "step": 24400
    },
    {
      "epoch": 2.6657202140439007,
      "grad_norm": 0.1480303555727005,
      "learning_rate": 3.667631320301409e-05,
      "loss": 2.8483,
      "step": 24410
    },
    {
      "epoch": 2.666812274762477,
      "grad_norm": 0.1390766203403473,
      "learning_rate": 3.667085289942121e-05,
      "loss": 2.8185,
      "step": 24420
    },
    {
      "epoch": 2.6679043354810528,
      "grad_norm": 0.20837341248989105,
      "learning_rate": 3.6665392595828326e-05,
      "loss": 2.876,
      "step": 24430
    },
    {
      "epoch": 2.6689963961996286,
      "grad_norm": 0.08874677866697311,
      "learning_rate": 3.665993229223545e-05,
      "loss": 2.8343,
      "step": 24440
    },
    {
      "epoch": 2.6700884569182044,
      "grad_norm": 0.10003265738487244,
      "learning_rate": 3.665447198864257e-05,
      "loss": 2.8363,
      "step": 24450
    },
    {
      "epoch": 2.6711805176367807,
      "grad_norm": 0.0987195149064064,
      "learning_rate": 3.664901168504969e-05,
      "loss": 2.9101,
      "step": 24460
    },
    {
      "epoch": 2.6722725783553565,
      "grad_norm": 0.1782015711069107,
      "learning_rate": 3.664355138145681e-05,
      "loss": 2.8611,
      "step": 24470
    },
    {
      "epoch": 2.6733646390739327,
      "grad_norm": 0.14003707468509674,
      "learning_rate": 3.663809107786393e-05,
      "loss": 2.8525,
      "step": 24480
    },
    {
      "epoch": 2.6744566997925086,
      "grad_norm": 0.3444482088088989,
      "learning_rate": 3.6632630774271056e-05,
      "loss": 2.8446,
      "step": 24490
    },
    {
      "epoch": 2.6755487605110844,
      "grad_norm": 0.11468678712844849,
      "learning_rate": 3.6627170470678176e-05,
      "loss": 2.8989,
      "step": 24500
    },
    {
      "epoch": 2.67664082122966,
      "grad_norm": 0.1155933290719986,
      "learning_rate": 3.6621710167085295e-05,
      "loss": 2.8872,
      "step": 24510
    },
    {
      "epoch": 2.6777328819482364,
      "grad_norm": 0.3655255138874054,
      "learning_rate": 3.6616249863492414e-05,
      "loss": 2.8817,
      "step": 24520
    },
    {
      "epoch": 2.6788249426668123,
      "grad_norm": 0.1461518108844757,
      "learning_rate": 3.6610789559899534e-05,
      "loss": 2.8486,
      "step": 24530
    },
    {
      "epoch": 2.679917003385388,
      "grad_norm": 0.11386650800704956,
      "learning_rate": 3.660532925630665e-05,
      "loss": 2.8573,
      "step": 24540
    },
    {
      "epoch": 2.6810090641039643,
      "grad_norm": 0.29220178723335266,
      "learning_rate": 3.659986895271377e-05,
      "loss": 2.846,
      "step": 24550
    },
    {
      "epoch": 2.68210112482254,
      "grad_norm": 0.1371629238128662,
      "learning_rate": 3.659440864912089e-05,
      "loss": 2.8448,
      "step": 24560
    },
    {
      "epoch": 2.683193185541116,
      "grad_norm": 0.11428377777338028,
      "learning_rate": 3.658894834552801e-05,
      "loss": 2.8553,
      "step": 24570
    },
    {
      "epoch": 2.684285246259692,
      "grad_norm": 0.19409288465976715,
      "learning_rate": 3.658348804193513e-05,
      "loss": 2.8884,
      "step": 24580
    },
    {
      "epoch": 2.685377306978268,
      "grad_norm": 0.09109415113925934,
      "learning_rate": 3.657802773834225e-05,
      "loss": 2.848,
      "step": 24590
    },
    {
      "epoch": 2.686469367696844,
      "grad_norm": 0.09481129050254822,
      "learning_rate": 3.657256743474937e-05,
      "loss": 2.8065,
      "step": 24600
    },
    {
      "epoch": 2.68756142841542,
      "grad_norm": 0.2668944299221039,
      "learning_rate": 3.656710713115649e-05,
      "loss": 2.8924,
      "step": 24610
    },
    {
      "epoch": 2.688653489133996,
      "grad_norm": 0.27832338213920593,
      "learning_rate": 3.6561646827563615e-05,
      "loss": 2.8414,
      "step": 24620
    },
    {
      "epoch": 2.6897455498525717,
      "grad_norm": 0.27722543478012085,
      "learning_rate": 3.6556186523970735e-05,
      "loss": 2.8619,
      "step": 24630
    },
    {
      "epoch": 2.6908376105711476,
      "grad_norm": 0.3791448175907135,
      "learning_rate": 3.6550726220377854e-05,
      "loss": 2.9188,
      "step": 24640
    },
    {
      "epoch": 2.691929671289724,
      "grad_norm": 0.10426364094018936,
      "learning_rate": 3.6545265916784974e-05,
      "loss": 2.8587,
      "step": 24650
    },
    {
      "epoch": 2.6930217320082996,
      "grad_norm": 0.207752525806427,
      "learning_rate": 3.653980561319209e-05,
      "loss": 2.8495,
      "step": 24660
    },
    {
      "epoch": 2.6941137927268755,
      "grad_norm": 0.24588489532470703,
      "learning_rate": 3.653434530959922e-05,
      "loss": 2.8918,
      "step": 24670
    },
    {
      "epoch": 2.6952058534454517,
      "grad_norm": 0.11203193664550781,
      "learning_rate": 3.652888500600634e-05,
      "loss": 2.8587,
      "step": 24680
    },
    {
      "epoch": 2.6962979141640275,
      "grad_norm": 0.30351313948631287,
      "learning_rate": 3.652342470241346e-05,
      "loss": 2.8678,
      "step": 24690
    },
    {
      "epoch": 2.6973899748826033,
      "grad_norm": 0.1675250232219696,
      "learning_rate": 3.651796439882058e-05,
      "loss": 2.911,
      "step": 24700
    },
    {
      "epoch": 2.698482035601179,
      "grad_norm": 0.39156869053840637,
      "learning_rate": 3.65125040952277e-05,
      "loss": 2.8619,
      "step": 24710
    },
    {
      "epoch": 2.6995740963197554,
      "grad_norm": 0.24425195157527924,
      "learning_rate": 3.6507043791634816e-05,
      "loss": 2.8571,
      "step": 24720
    },
    {
      "epoch": 2.7006661570383312,
      "grad_norm": 0.10483193397521973,
      "learning_rate": 3.6501583488041936e-05,
      "loss": 2.8356,
      "step": 24730
    },
    {
      "epoch": 2.7017582177569075,
      "grad_norm": 0.11155456304550171,
      "learning_rate": 3.6496123184449055e-05,
      "loss": 2.8356,
      "step": 24740
    },
    {
      "epoch": 2.7028502784754833,
      "grad_norm": 0.2940547466278076,
      "learning_rate": 3.649066288085618e-05,
      "loss": 2.9282,
      "step": 24750
    },
    {
      "epoch": 2.703942339194059,
      "grad_norm": 0.11373075842857361,
      "learning_rate": 3.64852025772633e-05,
      "loss": 2.8507,
      "step": 24760
    },
    {
      "epoch": 2.705034399912635,
      "grad_norm": 0.3845062255859375,
      "learning_rate": 3.647974227367042e-05,
      "loss": 2.9397,
      "step": 24770
    },
    {
      "epoch": 2.706126460631211,
      "grad_norm": 0.15021705627441406,
      "learning_rate": 3.647428197007754e-05,
      "loss": 2.8951,
      "step": 24780
    },
    {
      "epoch": 2.707218521349787,
      "grad_norm": 0.10113044828176498,
      "learning_rate": 3.646882166648466e-05,
      "loss": 2.8536,
      "step": 24790
    },
    {
      "epoch": 2.708310582068363,
      "grad_norm": 0.6342121362686157,
      "learning_rate": 3.646336136289178e-05,
      "loss": 2.8455,
      "step": 24800
    },
    {
      "epoch": 2.709402642786939,
      "grad_norm": 0.26470300555229187,
      "learning_rate": 3.64579010592989e-05,
      "loss": 2.8971,
      "step": 24810
    },
    {
      "epoch": 2.710494703505515,
      "grad_norm": 0.21861936151981354,
      "learning_rate": 3.645244075570602e-05,
      "loss": 2.842,
      "step": 24820
    },
    {
      "epoch": 2.7115867642240907,
      "grad_norm": 0.258830726146698,
      "learning_rate": 3.6446980452113137e-05,
      "loss": 2.8471,
      "step": 24830
    },
    {
      "epoch": 2.7126788249426665,
      "grad_norm": 0.22778810560703278,
      "learning_rate": 3.6441520148520256e-05,
      "loss": 2.8616,
      "step": 24840
    },
    {
      "epoch": 2.713770885661243,
      "grad_norm": 0.2123669683933258,
      "learning_rate": 3.6436059844927375e-05,
      "loss": 2.889,
      "step": 24850
    },
    {
      "epoch": 2.7148629463798186,
      "grad_norm": 0.14521770179271698,
      "learning_rate": 3.6430599541334495e-05,
      "loss": 2.8381,
      "step": 24860
    },
    {
      "epoch": 2.715955007098395,
      "grad_norm": 0.16795627772808075,
      "learning_rate": 3.642513923774162e-05,
      "loss": 2.8341,
      "step": 24870
    },
    {
      "epoch": 2.7170470678169707,
      "grad_norm": 0.11691853404045105,
      "learning_rate": 3.641967893414874e-05,
      "loss": 2.8474,
      "step": 24880
    },
    {
      "epoch": 2.7181391285355465,
      "grad_norm": 0.204398050904274,
      "learning_rate": 3.641421863055586e-05,
      "loss": 2.846,
      "step": 24890
    },
    {
      "epoch": 2.7192311892541223,
      "grad_norm": 0.11662645637989044,
      "learning_rate": 3.6408758326962986e-05,
      "loss": 2.8568,
      "step": 24900
    },
    {
      "epoch": 2.7203232499726986,
      "grad_norm": 0.20437313616275787,
      "learning_rate": 3.6403298023370105e-05,
      "loss": 2.8855,
      "step": 24910
    },
    {
      "epoch": 2.7214153106912744,
      "grad_norm": 0.14108823239803314,
      "learning_rate": 3.6397837719777225e-05,
      "loss": 2.8582,
      "step": 24920
    },
    {
      "epoch": 2.7225073714098507,
      "grad_norm": 0.14062745869159698,
      "learning_rate": 3.6392377416184344e-05,
      "loss": 2.8692,
      "step": 24930
    },
    {
      "epoch": 2.7235994321284265,
      "grad_norm": 0.5529094934463501,
      "learning_rate": 3.6386917112591464e-05,
      "loss": 2.8508,
      "step": 24940
    },
    {
      "epoch": 2.7246914928470023,
      "grad_norm": 0.20623153448104858,
      "learning_rate": 3.638145680899858e-05,
      "loss": 2.8654,
      "step": 24950
    },
    {
      "epoch": 2.725783553565578,
      "grad_norm": 0.1305490881204605,
      "learning_rate": 3.63759965054057e-05,
      "loss": 2.8277,
      "step": 24960
    },
    {
      "epoch": 2.726875614284154,
      "grad_norm": 0.13464078307151794,
      "learning_rate": 3.637053620181282e-05,
      "loss": 2.8638,
      "step": 24970
    },
    {
      "epoch": 2.72796767500273,
      "grad_norm": 0.18542979657649994,
      "learning_rate": 3.636507589821994e-05,
      "loss": 2.8614,
      "step": 24980
    },
    {
      "epoch": 2.729059735721306,
      "grad_norm": 0.6789989471435547,
      "learning_rate": 3.635961559462706e-05,
      "loss": 2.8676,
      "step": 24990
    },
    {
      "epoch": 2.7301517964398823,
      "grad_norm": 0.1458219587802887,
      "learning_rate": 3.635415529103418e-05,
      "loss": 2.8428,
      "step": 25000
    },
    {
      "epoch": 2.731243857158458,
      "grad_norm": 0.10373524576425552,
      "learning_rate": 3.63486949874413e-05,
      "loss": 2.8659,
      "step": 25010
    },
    {
      "epoch": 2.732335917877034,
      "grad_norm": 0.12395849078893661,
      "learning_rate": 3.6343234683848426e-05,
      "loss": 2.9403,
      "step": 25020
    },
    {
      "epoch": 2.7334279785956097,
      "grad_norm": 0.2734552323818207,
      "learning_rate": 3.6337774380255545e-05,
      "loss": 2.8736,
      "step": 25030
    },
    {
      "epoch": 2.734520039314186,
      "grad_norm": 0.13392971456050873,
      "learning_rate": 3.6332314076662664e-05,
      "loss": 2.8938,
      "step": 25040
    },
    {
      "epoch": 2.735612100032762,
      "grad_norm": 0.1445970982313156,
      "learning_rate": 3.6326853773069784e-05,
      "loss": 2.8408,
      "step": 25050
    },
    {
      "epoch": 2.736704160751338,
      "grad_norm": 0.08718781173229218,
      "learning_rate": 3.63213934694769e-05,
      "loss": 2.8378,
      "step": 25060
    },
    {
      "epoch": 2.737796221469914,
      "grad_norm": 0.29046472907066345,
      "learning_rate": 3.631593316588402e-05,
      "loss": 2.8946,
      "step": 25070
    },
    {
      "epoch": 2.7388882821884897,
      "grad_norm": 0.11571898311376572,
      "learning_rate": 3.631047286229114e-05,
      "loss": 2.8556,
      "step": 25080
    },
    {
      "epoch": 2.7399803429070655,
      "grad_norm": 0.1090313196182251,
      "learning_rate": 3.630501255869826e-05,
      "loss": 2.9952,
      "step": 25090
    },
    {
      "epoch": 2.7410724036256418,
      "grad_norm": 0.1681099385023117,
      "learning_rate": 3.629955225510539e-05,
      "loss": 2.8621,
      "step": 25100
    },
    {
      "epoch": 2.7421644643442176,
      "grad_norm": 0.19233578443527222,
      "learning_rate": 3.629409195151251e-05,
      "loss": 2.8755,
      "step": 25110
    },
    {
      "epoch": 2.7432565250627934,
      "grad_norm": 0.14680509269237518,
      "learning_rate": 3.6288631647919626e-05,
      "loss": 2.9017,
      "step": 25120
    },
    {
      "epoch": 2.7443485857813696,
      "grad_norm": 0.33027735352516174,
      "learning_rate": 3.6283171344326746e-05,
      "loss": 2.8775,
      "step": 25130
    },
    {
      "epoch": 2.7454406464999455,
      "grad_norm": 0.13289830088615417,
      "learning_rate": 3.6277711040733865e-05,
      "loss": 2.8789,
      "step": 25140
    },
    {
      "epoch": 2.7465327072185213,
      "grad_norm": 0.11712116748094559,
      "learning_rate": 3.627225073714099e-05,
      "loss": 2.8447,
      "step": 25150
    },
    {
      "epoch": 2.747624767937097,
      "grad_norm": 0.2605254054069519,
      "learning_rate": 3.626679043354811e-05,
      "loss": 2.8593,
      "step": 25160
    },
    {
      "epoch": 2.7487168286556733,
      "grad_norm": 0.1568606197834015,
      "learning_rate": 3.626133012995523e-05,
      "loss": 2.8641,
      "step": 25170
    },
    {
      "epoch": 2.749808889374249,
      "grad_norm": 0.06970400363206863,
      "learning_rate": 3.625586982636235e-05,
      "loss": 2.8017,
      "step": 25180
    },
    {
      "epoch": 2.7509009500928254,
      "grad_norm": 0.20149338245391846,
      "learning_rate": 3.625040952276947e-05,
      "loss": 2.9004,
      "step": 25190
    },
    {
      "epoch": 2.7519930108114012,
      "grad_norm": 0.16544796526432037,
      "learning_rate": 3.624494921917659e-05,
      "loss": 2.8744,
      "step": 25200
    },
    {
      "epoch": 2.753085071529977,
      "grad_norm": 0.16994956135749817,
      "learning_rate": 3.623948891558371e-05,
      "loss": 2.9515,
      "step": 25210
    },
    {
      "epoch": 2.754177132248553,
      "grad_norm": 0.13188500702381134,
      "learning_rate": 3.623402861199083e-05,
      "loss": 2.8548,
      "step": 25220
    },
    {
      "epoch": 2.755269192967129,
      "grad_norm": 0.08675213158130646,
      "learning_rate": 3.622856830839795e-05,
      "loss": 2.8399,
      "step": 25230
    },
    {
      "epoch": 2.756361253685705,
      "grad_norm": 0.23600570857524872,
      "learning_rate": 3.6223108004805066e-05,
      "loss": 2.8353,
      "step": 25240
    },
    {
      "epoch": 2.7574533144042808,
      "grad_norm": 0.3186197578907013,
      "learning_rate": 3.6217647701212186e-05,
      "loss": 2.8848,
      "step": 25250
    },
    {
      "epoch": 2.758545375122857,
      "grad_norm": 0.13004590570926666,
      "learning_rate": 3.6212187397619305e-05,
      "loss": 2.8423,
      "step": 25260
    },
    {
      "epoch": 2.759637435841433,
      "grad_norm": 0.4779456555843353,
      "learning_rate": 3.6206727094026424e-05,
      "loss": 2.8368,
      "step": 25270
    },
    {
      "epoch": 2.7607294965600087,
      "grad_norm": 0.20696263015270233,
      "learning_rate": 3.620126679043355e-05,
      "loss": 2.8995,
      "step": 25280
    },
    {
      "epoch": 2.7618215572785845,
      "grad_norm": 0.1613566130399704,
      "learning_rate": 3.619580648684067e-05,
      "loss": 2.8468,
      "step": 25290
    },
    {
      "epoch": 2.7629136179971607,
      "grad_norm": 0.2236557900905609,
      "learning_rate": 3.619034618324779e-05,
      "loss": 2.846,
      "step": 25300
    },
    {
      "epoch": 2.7640056787157365,
      "grad_norm": 0.24782948195934296,
      "learning_rate": 3.618488587965491e-05,
      "loss": 2.9649,
      "step": 25310
    },
    {
      "epoch": 2.765097739434313,
      "grad_norm": 0.10387105494737625,
      "learning_rate": 3.617942557606203e-05,
      "loss": 2.8301,
      "step": 25320
    },
    {
      "epoch": 2.7661898001528886,
      "grad_norm": 0.2311522513628006,
      "learning_rate": 3.6173965272469154e-05,
      "loss": 2.8734,
      "step": 25330
    },
    {
      "epoch": 2.7672818608714644,
      "grad_norm": 0.19964167475700378,
      "learning_rate": 3.6168504968876274e-05,
      "loss": 2.9342,
      "step": 25340
    },
    {
      "epoch": 2.7683739215900403,
      "grad_norm": 0.22518391907215118,
      "learning_rate": 3.616304466528339e-05,
      "loss": 2.8539,
      "step": 25350
    },
    {
      "epoch": 2.7694659823086165,
      "grad_norm": 0.18014422059059143,
      "learning_rate": 3.615758436169051e-05,
      "loss": 2.8437,
      "step": 25360
    },
    {
      "epoch": 2.7705580430271923,
      "grad_norm": 0.14077594876289368,
      "learning_rate": 3.615212405809763e-05,
      "loss": 2.8597,
      "step": 25370
    },
    {
      "epoch": 2.771650103745768,
      "grad_norm": 0.25174498558044434,
      "learning_rate": 3.614666375450475e-05,
      "loss": 2.8657,
      "step": 25380
    },
    {
      "epoch": 2.7727421644643444,
      "grad_norm": 0.30163508653640747,
      "learning_rate": 3.614120345091187e-05,
      "loss": 2.8468,
      "step": 25390
    },
    {
      "epoch": 2.77383422518292,
      "grad_norm": 0.6964606642723083,
      "learning_rate": 3.613574314731899e-05,
      "loss": 2.8478,
      "step": 25400
    },
    {
      "epoch": 2.774926285901496,
      "grad_norm": 0.9348898530006409,
      "learning_rate": 3.6130282843726116e-05,
      "loss": 2.9383,
      "step": 25410
    },
    {
      "epoch": 2.776018346620072,
      "grad_norm": 0.42628300189971924,
      "learning_rate": 3.6124822540133236e-05,
      "loss": 2.9126,
      "step": 25420
    },
    {
      "epoch": 2.777110407338648,
      "grad_norm": 0.10144184529781342,
      "learning_rate": 3.6119362236540355e-05,
      "loss": 2.8863,
      "step": 25430
    },
    {
      "epoch": 2.778202468057224,
      "grad_norm": 0.18655411899089813,
      "learning_rate": 3.6113901932947475e-05,
      "loss": 2.8799,
      "step": 25440
    },
    {
      "epoch": 2.7792945287758,
      "grad_norm": 0.15496332943439484,
      "learning_rate": 3.6108441629354594e-05,
      "loss": 2.8286,
      "step": 25450
    },
    {
      "epoch": 2.780386589494376,
      "grad_norm": 0.380316823720932,
      "learning_rate": 3.6102981325761714e-05,
      "loss": 2.866,
      "step": 25460
    },
    {
      "epoch": 2.781478650212952,
      "grad_norm": 0.15757524967193604,
      "learning_rate": 3.609752102216883e-05,
      "loss": 2.8734,
      "step": 25470
    },
    {
      "epoch": 2.7825707109315276,
      "grad_norm": 0.1696079820394516,
      "learning_rate": 3.609206071857595e-05,
      "loss": 2.8662,
      "step": 25480
    },
    {
      "epoch": 2.783662771650104,
      "grad_norm": 0.11291073262691498,
      "learning_rate": 3.608660041498307e-05,
      "loss": 2.8296,
      "step": 25490
    },
    {
      "epoch": 2.7847548323686797,
      "grad_norm": 0.14585265517234802,
      "learning_rate": 3.608114011139019e-05,
      "loss": 2.8709,
      "step": 25500
    },
    {
      "epoch": 2.7858468930872555,
      "grad_norm": 0.2118222564458847,
      "learning_rate": 3.607567980779731e-05,
      "loss": 2.8617,
      "step": 25510
    },
    {
      "epoch": 2.786938953805832,
      "grad_norm": 0.11300548911094666,
      "learning_rate": 3.607021950420443e-05,
      "loss": 2.8447,
      "step": 25520
    },
    {
      "epoch": 2.7880310145244076,
      "grad_norm": 0.33584704995155334,
      "learning_rate": 3.6064759200611556e-05,
      "loss": 2.9138,
      "step": 25530
    },
    {
      "epoch": 2.7891230752429834,
      "grad_norm": 0.23772075772285461,
      "learning_rate": 3.6059298897018676e-05,
      "loss": 2.893,
      "step": 25540
    },
    {
      "epoch": 2.7902151359615592,
      "grad_norm": 0.30539825558662415,
      "learning_rate": 3.6053838593425795e-05,
      "loss": 2.8867,
      "step": 25550
    },
    {
      "epoch": 2.7913071966801355,
      "grad_norm": 0.13223852217197418,
      "learning_rate": 3.604837828983292e-05,
      "loss": 2.8678,
      "step": 25560
    },
    {
      "epoch": 2.7923992573987113,
      "grad_norm": 0.265021950006485,
      "learning_rate": 3.604291798624004e-05,
      "loss": 2.9433,
      "step": 25570
    },
    {
      "epoch": 2.7934913181172876,
      "grad_norm": 0.17915865778923035,
      "learning_rate": 3.603745768264716e-05,
      "loss": 2.8795,
      "step": 25580
    },
    {
      "epoch": 2.7945833788358634,
      "grad_norm": 0.14455322921276093,
      "learning_rate": 3.603199737905428e-05,
      "loss": 2.8566,
      "step": 25590
    },
    {
      "epoch": 2.795675439554439,
      "grad_norm": 0.314968079328537,
      "learning_rate": 3.60265370754614e-05,
      "loss": 2.8542,
      "step": 25600
    },
    {
      "epoch": 2.796767500273015,
      "grad_norm": 0.6385167241096497,
      "learning_rate": 3.602107677186852e-05,
      "loss": 2.8594,
      "step": 25610
    },
    {
      "epoch": 2.7978595609915913,
      "grad_norm": 0.2427113652229309,
      "learning_rate": 3.601561646827564e-05,
      "loss": 2.8643,
      "step": 25620
    },
    {
      "epoch": 2.798951621710167,
      "grad_norm": 0.14711566269397736,
      "learning_rate": 3.601015616468276e-05,
      "loss": 2.8651,
      "step": 25630
    },
    {
      "epoch": 2.800043682428743,
      "grad_norm": 0.21389706432819366,
      "learning_rate": 3.6004695861089876e-05,
      "loss": 2.8825,
      "step": 25640
    },
    {
      "epoch": 2.801135743147319,
      "grad_norm": 0.1425546407699585,
      "learning_rate": 3.5999235557496996e-05,
      "loss": 2.8804,
      "step": 25650
    },
    {
      "epoch": 2.802227803865895,
      "grad_norm": 0.13161778450012207,
      "learning_rate": 3.5993775253904115e-05,
      "loss": 2.8761,
      "step": 25660
    },
    {
      "epoch": 2.803319864584471,
      "grad_norm": 0.10661069303750992,
      "learning_rate": 3.598831495031124e-05,
      "loss": 2.8617,
      "step": 25670
    },
    {
      "epoch": 2.8044119253030466,
      "grad_norm": 0.09848751872777939,
      "learning_rate": 3.598285464671836e-05,
      "loss": 2.8636,
      "step": 25680
    },
    {
      "epoch": 2.805503986021623,
      "grad_norm": 0.1569129228591919,
      "learning_rate": 3.597739434312548e-05,
      "loss": 2.8343,
      "step": 25690
    },
    {
      "epoch": 2.8065960467401987,
      "grad_norm": 0.21795402467250824,
      "learning_rate": 3.59719340395326e-05,
      "loss": 2.8745,
      "step": 25700
    },
    {
      "epoch": 2.807688107458775,
      "grad_norm": 0.4497986435890198,
      "learning_rate": 3.596647373593972e-05,
      "loss": 2.8549,
      "step": 25710
    },
    {
      "epoch": 2.8087801681773508,
      "grad_norm": 0.1328086107969284,
      "learning_rate": 3.596101343234684e-05,
      "loss": 2.8759,
      "step": 25720
    },
    {
      "epoch": 2.8098722288959266,
      "grad_norm": 0.3109970986843109,
      "learning_rate": 3.595555312875396e-05,
      "loss": 2.8331,
      "step": 25730
    },
    {
      "epoch": 2.8109642896145024,
      "grad_norm": 0.13306191563606262,
      "learning_rate": 3.595009282516108e-05,
      "loss": 2.8436,
      "step": 25740
    },
    {
      "epoch": 2.8120563503330787,
      "grad_norm": 0.12862452864646912,
      "learning_rate": 3.59446325215682e-05,
      "loss": 2.8923,
      "step": 25750
    },
    {
      "epoch": 2.8131484110516545,
      "grad_norm": 0.1411949098110199,
      "learning_rate": 3.593917221797532e-05,
      "loss": 2.8622,
      "step": 25760
    },
    {
      "epoch": 2.8142404717702303,
      "grad_norm": 0.12824076414108276,
      "learning_rate": 3.593371191438244e-05,
      "loss": 2.8286,
      "step": 25770
    },
    {
      "epoch": 2.8153325324888065,
      "grad_norm": 0.2166760414838791,
      "learning_rate": 3.592825161078956e-05,
      "loss": 2.8601,
      "step": 25780
    },
    {
      "epoch": 2.8164245932073824,
      "grad_norm": 0.25419458746910095,
      "learning_rate": 3.592279130719668e-05,
      "loss": 2.8895,
      "step": 25790
    },
    {
      "epoch": 2.817516653925958,
      "grad_norm": 0.2502347528934479,
      "learning_rate": 3.591733100360381e-05,
      "loss": 2.8688,
      "step": 25800
    },
    {
      "epoch": 2.818608714644534,
      "grad_norm": 0.11688839644193649,
      "learning_rate": 3.591187070001093e-05,
      "loss": 2.8603,
      "step": 25810
    },
    {
      "epoch": 2.8197007753631103,
      "grad_norm": 0.29062509536743164,
      "learning_rate": 3.5906410396418046e-05,
      "loss": 2.8753,
      "step": 25820
    },
    {
      "epoch": 2.820792836081686,
      "grad_norm": 0.14136481285095215,
      "learning_rate": 3.5900950092825166e-05,
      "loss": 2.8621,
      "step": 25830
    },
    {
      "epoch": 2.8218848968002623,
      "grad_norm": 0.13694792985916138,
      "learning_rate": 3.5895489789232285e-05,
      "loss": 2.8326,
      "step": 25840
    },
    {
      "epoch": 2.822976957518838,
      "grad_norm": 0.3211086094379425,
      "learning_rate": 3.5890029485639404e-05,
      "loss": 2.8613,
      "step": 25850
    },
    {
      "epoch": 2.824069018237414,
      "grad_norm": 0.190449059009552,
      "learning_rate": 3.5884569182046524e-05,
      "loss": 2.8413,
      "step": 25860
    },
    {
      "epoch": 2.82516107895599,
      "grad_norm": 0.10610765963792801,
      "learning_rate": 3.587910887845364e-05,
      "loss": 2.8875,
      "step": 25870
    },
    {
      "epoch": 2.826253139674566,
      "grad_norm": 0.19260521233081818,
      "learning_rate": 3.587364857486076e-05,
      "loss": 2.8899,
      "step": 25880
    },
    {
      "epoch": 2.827345200393142,
      "grad_norm": 0.1670704334974289,
      "learning_rate": 3.586818827126788e-05,
      "loss": 2.8672,
      "step": 25890
    },
    {
      "epoch": 2.8284372611117177,
      "grad_norm": 0.1100878193974495,
      "learning_rate": 3.5862727967675e-05,
      "loss": 2.8684,
      "step": 25900
    },
    {
      "epoch": 2.829529321830294,
      "grad_norm": 0.3544756770133972,
      "learning_rate": 3.585726766408212e-05,
      "loss": 2.8458,
      "step": 25910
    },
    {
      "epoch": 2.8306213825488697,
      "grad_norm": 0.3008282780647278,
      "learning_rate": 3.585180736048924e-05,
      "loss": 2.8882,
      "step": 25920
    },
    {
      "epoch": 2.8317134432674456,
      "grad_norm": 0.12373395264148712,
      "learning_rate": 3.5846347056896366e-05,
      "loss": 2.8262,
      "step": 25930
    },
    {
      "epoch": 2.8328055039860214,
      "grad_norm": 0.17525345087051392,
      "learning_rate": 3.5840886753303486e-05,
      "loss": 2.8927,
      "step": 25940
    },
    {
      "epoch": 2.8338975647045976,
      "grad_norm": 0.28861406445503235,
      "learning_rate": 3.5835426449710605e-05,
      "loss": 2.8791,
      "step": 25950
    },
    {
      "epoch": 2.8349896254231735,
      "grad_norm": 0.3912934958934784,
      "learning_rate": 3.5829966146117725e-05,
      "loss": 2.8791,
      "step": 25960
    },
    {
      "epoch": 2.8360816861417497,
      "grad_norm": 0.17986427247524261,
      "learning_rate": 3.5824505842524844e-05,
      "loss": 2.8651,
      "step": 25970
    },
    {
      "epoch": 2.8371737468603255,
      "grad_norm": 0.1206117570400238,
      "learning_rate": 3.5819045538931963e-05,
      "loss": 2.8619,
      "step": 25980
    },
    {
      "epoch": 2.8382658075789013,
      "grad_norm": 0.28991660475730896,
      "learning_rate": 3.581358523533909e-05,
      "loss": 2.8502,
      "step": 25990
    },
    {
      "epoch": 2.839357868297477,
      "grad_norm": 0.24986611306667328,
      "learning_rate": 3.580812493174621e-05,
      "loss": 2.8908,
      "step": 26000
    },
    {
      "epoch": 2.8404499290160534,
      "grad_norm": 0.23968654870986938,
      "learning_rate": 3.580266462815333e-05,
      "loss": 2.8811,
      "step": 26010
    },
    {
      "epoch": 2.8415419897346292,
      "grad_norm": 0.2048051804304123,
      "learning_rate": 3.579720432456045e-05,
      "loss": 2.83,
      "step": 26020
    },
    {
      "epoch": 2.842634050453205,
      "grad_norm": 0.3548804819583893,
      "learning_rate": 3.579174402096757e-05,
      "loss": 2.8842,
      "step": 26030
    },
    {
      "epoch": 2.8437261111717813,
      "grad_norm": 0.14227615296840668,
      "learning_rate": 3.578628371737469e-05,
      "loss": 2.862,
      "step": 26040
    },
    {
      "epoch": 2.844818171890357,
      "grad_norm": 0.17203480005264282,
      "learning_rate": 3.5780823413781806e-05,
      "loss": 2.8697,
      "step": 26050
    },
    {
      "epoch": 2.845910232608933,
      "grad_norm": 0.10582785308361053,
      "learning_rate": 3.577536311018893e-05,
      "loss": 2.8393,
      "step": 26060
    },
    {
      "epoch": 2.8470022933275088,
      "grad_norm": 0.12372995913028717,
      "learning_rate": 3.576990280659605e-05,
      "loss": 2.8715,
      "step": 26070
    },
    {
      "epoch": 2.848094354046085,
      "grad_norm": 0.35749372839927673,
      "learning_rate": 3.576444250300317e-05,
      "loss": 2.8824,
      "step": 26080
    },
    {
      "epoch": 2.849186414764661,
      "grad_norm": 0.1480393409729004,
      "learning_rate": 3.575898219941029e-05,
      "loss": 2.8561,
      "step": 26090
    },
    {
      "epoch": 2.850278475483237,
      "grad_norm": 0.1825668215751648,
      "learning_rate": 3.575352189581741e-05,
      "loss": 2.8815,
      "step": 26100
    },
    {
      "epoch": 2.851370536201813,
      "grad_norm": 0.3012296259403229,
      "learning_rate": 3.574806159222453e-05,
      "loss": 2.8758,
      "step": 26110
    },
    {
      "epoch": 2.8524625969203887,
      "grad_norm": 0.8451364040374756,
      "learning_rate": 3.574260128863165e-05,
      "loss": 2.8834,
      "step": 26120
    },
    {
      "epoch": 2.8535546576389645,
      "grad_norm": 0.23429059982299805,
      "learning_rate": 3.573714098503877e-05,
      "loss": 2.8538,
      "step": 26130
    },
    {
      "epoch": 2.854646718357541,
      "grad_norm": 0.16336725652217865,
      "learning_rate": 3.573168068144589e-05,
      "loss": 2.8782,
      "step": 26140
    },
    {
      "epoch": 2.8557387790761166,
      "grad_norm": 0.1105460524559021,
      "learning_rate": 3.572622037785301e-05,
      "loss": 2.849,
      "step": 26150
    },
    {
      "epoch": 2.8568308397946924,
      "grad_norm": 0.3683798015117645,
      "learning_rate": 3.5720760074260126e-05,
      "loss": 2.8561,
      "step": 26160
    },
    {
      "epoch": 2.8579229005132687,
      "grad_norm": 0.17428192496299744,
      "learning_rate": 3.5715299770667246e-05,
      "loss": 2.9022,
      "step": 26170
    },
    {
      "epoch": 2.8590149612318445,
      "grad_norm": 0.09282540529966354,
      "learning_rate": 3.5709839467074365e-05,
      "loss": 2.9343,
      "step": 26180
    },
    {
      "epoch": 2.8601070219504203,
      "grad_norm": 0.11994018405675888,
      "learning_rate": 3.570437916348149e-05,
      "loss": 2.8594,
      "step": 26190
    },
    {
      "epoch": 2.861199082668996,
      "grad_norm": 0.11376151442527771,
      "learning_rate": 3.569891885988861e-05,
      "loss": 2.8638,
      "step": 26200
    },
    {
      "epoch": 2.8622911433875724,
      "grad_norm": 0.29210126399993896,
      "learning_rate": 3.569345855629573e-05,
      "loss": 2.8726,
      "step": 26210
    },
    {
      "epoch": 2.863383204106148,
      "grad_norm": 0.17380934953689575,
      "learning_rate": 3.5687998252702856e-05,
      "loss": 2.8889,
      "step": 26220
    },
    {
      "epoch": 2.8644752648247245,
      "grad_norm": 0.12375109642744064,
      "learning_rate": 3.5682537949109976e-05,
      "loss": 2.8428,
      "step": 26230
    },
    {
      "epoch": 2.8655673255433003,
      "grad_norm": 0.21048392355442047,
      "learning_rate": 3.5677077645517095e-05,
      "loss": 2.8509,
      "step": 26240
    },
    {
      "epoch": 2.866659386261876,
      "grad_norm": 0.2028060406446457,
      "learning_rate": 3.5671617341924215e-05,
      "loss": 2.8418,
      "step": 26250
    },
    {
      "epoch": 2.867751446980452,
      "grad_norm": 0.22395463287830353,
      "learning_rate": 3.5666157038331334e-05,
      "loss": 2.8877,
      "step": 26260
    },
    {
      "epoch": 2.868843507699028,
      "grad_norm": 0.10153002291917801,
      "learning_rate": 3.5660696734738453e-05,
      "loss": 2.8192,
      "step": 26270
    },
    {
      "epoch": 2.869935568417604,
      "grad_norm": 0.700344443321228,
      "learning_rate": 3.565578246150486e-05,
      "loss": 2.9087,
      "step": 26280
    },
    {
      "epoch": 2.87102762913618,
      "grad_norm": 0.21511250734329224,
      "learning_rate": 3.565032215791198e-05,
      "loss": 2.9006,
      "step": 26290
    },
    {
      "epoch": 2.872119689854756,
      "grad_norm": 0.5279988646507263,
      "learning_rate": 3.56448618543191e-05,
      "loss": 2.8786,
      "step": 26300
    },
    {
      "epoch": 2.873211750573332,
      "grad_norm": 0.12894846498966217,
      "learning_rate": 3.563940155072622e-05,
      "loss": 2.8284,
      "step": 26310
    },
    {
      "epoch": 2.8743038112919077,
      "grad_norm": 0.15513205528259277,
      "learning_rate": 3.563394124713334e-05,
      "loss": 2.8596,
      "step": 26320
    },
    {
      "epoch": 2.8753958720104835,
      "grad_norm": 0.2369074821472168,
      "learning_rate": 3.562848094354046e-05,
      "loss": 2.8572,
      "step": 26330
    },
    {
      "epoch": 2.87648793272906,
      "grad_norm": 0.1372552067041397,
      "learning_rate": 3.5623020639947584e-05,
      "loss": 2.8535,
      "step": 26340
    },
    {
      "epoch": 2.8775799934476356,
      "grad_norm": 0.23168765008449554,
      "learning_rate": 3.56175603363547e-05,
      "loss": 2.8849,
      "step": 26350
    },
    {
      "epoch": 2.878672054166212,
      "grad_norm": 0.17088457942008972,
      "learning_rate": 3.561210003276182e-05,
      "loss": 2.8423,
      "step": 26360
    },
    {
      "epoch": 2.8797641148847877,
      "grad_norm": 0.32034221291542053,
      "learning_rate": 3.560663972916894e-05,
      "loss": 2.8304,
      "step": 26370
    },
    {
      "epoch": 2.8808561756033635,
      "grad_norm": 0.10318619757890701,
      "learning_rate": 3.560117942557606e-05,
      "loss": 2.8766,
      "step": 26380
    },
    {
      "epoch": 2.8819482363219393,
      "grad_norm": 0.24027138948440552,
      "learning_rate": 3.559571912198319e-05,
      "loss": 2.873,
      "step": 26390
    },
    {
      "epoch": 2.8830402970405156,
      "grad_norm": 0.13457515835762024,
      "learning_rate": 3.559025881839031e-05,
      "loss": 2.8396,
      "step": 26400
    },
    {
      "epoch": 2.8841323577590914,
      "grad_norm": 0.16504070162773132,
      "learning_rate": 3.5584798514797426e-05,
      "loss": 2.8478,
      "step": 26410
    },
    {
      "epoch": 2.885224418477667,
      "grad_norm": 0.7344398498535156,
      "learning_rate": 3.5579338211204546e-05,
      "loss": 2.9184,
      "step": 26420
    },
    {
      "epoch": 2.8863164791962435,
      "grad_norm": 0.21948327124118805,
      "learning_rate": 3.5573877907611665e-05,
      "loss": 2.9086,
      "step": 26430
    },
    {
      "epoch": 2.8874085399148193,
      "grad_norm": 0.2803564965724945,
      "learning_rate": 3.5568417604018784e-05,
      "loss": 2.9028,
      "step": 26440
    },
    {
      "epoch": 2.888500600633395,
      "grad_norm": 0.2988487780094147,
      "learning_rate": 3.5562957300425904e-05,
      "loss": 2.99,
      "step": 26450
    },
    {
      "epoch": 2.889592661351971,
      "grad_norm": 0.11861257255077362,
      "learning_rate": 3.555749699683302e-05,
      "loss": 2.8821,
      "step": 26460
    },
    {
      "epoch": 2.890684722070547,
      "grad_norm": 0.09750944375991821,
      "learning_rate": 3.555203669324015e-05,
      "loss": 2.8276,
      "step": 26470
    },
    {
      "epoch": 2.891776782789123,
      "grad_norm": 0.1590077430009842,
      "learning_rate": 3.554657638964727e-05,
      "loss": 2.865,
      "step": 26480
    },
    {
      "epoch": 2.8928688435076992,
      "grad_norm": 0.24336178600788116,
      "learning_rate": 3.554111608605439e-05,
      "loss": 2.9519,
      "step": 26490
    },
    {
      "epoch": 2.893960904226275,
      "grad_norm": 0.18063679337501526,
      "learning_rate": 3.553565578246151e-05,
      "loss": 2.8289,
      "step": 26500
    }
  ],
  "logging_steps": 10,
  "max_steps": 91570,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.989313699113861e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
