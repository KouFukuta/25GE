{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.2973180076628354,
  "eval_steps": 500,
  "global_step": 1500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01532567049808429,
      "grad_norm": 4.993486404418945,
      "learning_rate": 4.9770290964777946e-05,
      "loss": 7.0189,
      "step": 10
    },
    {
      "epoch": 0.03065134099616858,
      "grad_norm": 16.713502883911133,
      "learning_rate": 4.951505870342011e-05,
      "loss": 6.6087,
      "step": 20
    },
    {
      "epoch": 0.04597701149425287,
      "grad_norm": 3.6081924438476562,
      "learning_rate": 4.925982644206228e-05,
      "loss": 6.1692,
      "step": 30
    },
    {
      "epoch": 0.06130268199233716,
      "grad_norm": 3.2973954677581787,
      "learning_rate": 4.900459418070444e-05,
      "loss": 5.9401,
      "step": 40
    },
    {
      "epoch": 0.07662835249042145,
      "grad_norm": 5.028487205505371,
      "learning_rate": 4.874936191934661e-05,
      "loss": 5.6672,
      "step": 50
    },
    {
      "epoch": 0.09195402298850575,
      "grad_norm": 3.31244158744812,
      "learning_rate": 4.849412965798877e-05,
      "loss": 5.5117,
      "step": 60
    },
    {
      "epoch": 0.10727969348659004,
      "grad_norm": 3.130452871322632,
      "learning_rate": 4.823889739663094e-05,
      "loss": 5.462,
      "step": 70
    },
    {
      "epoch": 0.12260536398467432,
      "grad_norm": 2.982247829437256,
      "learning_rate": 4.7983665135273097e-05,
      "loss": 5.4334,
      "step": 80
    },
    {
      "epoch": 0.13793103448275862,
      "grad_norm": 3.2630245685577393,
      "learning_rate": 4.772843287391526e-05,
      "loss": 5.538,
      "step": 90
    },
    {
      "epoch": 0.1532567049808429,
      "grad_norm": 2.6417508125305176,
      "learning_rate": 4.747320061255743e-05,
      "loss": 5.363,
      "step": 100
    },
    {
      "epoch": 0.1685823754789272,
      "grad_norm": 3.6666855812072754,
      "learning_rate": 4.721796835119959e-05,
      "loss": 5.2937,
      "step": 110
    },
    {
      "epoch": 0.1839080459770115,
      "grad_norm": 2.930586099624634,
      "learning_rate": 4.696273608984176e-05,
      "loss": 5.3473,
      "step": 120
    },
    {
      "epoch": 0.19923371647509577,
      "grad_norm": 3.03029727935791,
      "learning_rate": 4.670750382848392e-05,
      "loss": 5.2283,
      "step": 130
    },
    {
      "epoch": 0.21455938697318008,
      "grad_norm": 3.0060155391693115,
      "learning_rate": 4.645227156712609e-05,
      "loss": 5.1213,
      "step": 140
    },
    {
      "epoch": 0.22988505747126436,
      "grad_norm": 4.248528480529785,
      "learning_rate": 4.6197039305768254e-05,
      "loss": 5.2961,
      "step": 150
    },
    {
      "epoch": 0.24521072796934865,
      "grad_norm": 3.940612554550171,
      "learning_rate": 4.594180704441042e-05,
      "loss": 5.0007,
      "step": 160
    },
    {
      "epoch": 0.26053639846743293,
      "grad_norm": 3.2963805198669434,
      "learning_rate": 4.5686574783052584e-05,
      "loss": 5.0259,
      "step": 170
    },
    {
      "epoch": 0.27586206896551724,
      "grad_norm": 3.5475218296051025,
      "learning_rate": 4.543134252169474e-05,
      "loss": 4.9192,
      "step": 180
    },
    {
      "epoch": 0.29118773946360155,
      "grad_norm": 3.1546761989593506,
      "learning_rate": 4.517611026033691e-05,
      "loss": 4.8456,
      "step": 190
    },
    {
      "epoch": 0.3065134099616858,
      "grad_norm": 3.8092222213745117,
      "learning_rate": 4.492087799897907e-05,
      "loss": 5.0464,
      "step": 200
    },
    {
      "epoch": 0.3218390804597701,
      "grad_norm": 3.398725986480713,
      "learning_rate": 4.466564573762124e-05,
      "loss": 4.8655,
      "step": 210
    },
    {
      "epoch": 0.3371647509578544,
      "grad_norm": 3.3672430515289307,
      "learning_rate": 4.4410413476263404e-05,
      "loss": 4.8406,
      "step": 220
    },
    {
      "epoch": 0.3524904214559387,
      "grad_norm": 3.179626226425171,
      "learning_rate": 4.415518121490557e-05,
      "loss": 4.821,
      "step": 230
    },
    {
      "epoch": 0.367816091954023,
      "grad_norm": 3.911832809448242,
      "learning_rate": 4.3899948953547734e-05,
      "loss": 4.7299,
      "step": 240
    },
    {
      "epoch": 0.3831417624521073,
      "grad_norm": 4.0294623374938965,
      "learning_rate": 4.36447166921899e-05,
      "loss": 4.8241,
      "step": 250
    },
    {
      "epoch": 0.39846743295019155,
      "grad_norm": 2.9284470081329346,
      "learning_rate": 4.338948443083206e-05,
      "loss": 4.6468,
      "step": 260
    },
    {
      "epoch": 0.41379310344827586,
      "grad_norm": 3.0169897079467773,
      "learning_rate": 4.313425216947422e-05,
      "loss": 4.8027,
      "step": 270
    },
    {
      "epoch": 0.42911877394636017,
      "grad_norm": 4.263740062713623,
      "learning_rate": 4.287901990811639e-05,
      "loss": 4.2642,
      "step": 280
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 3.7393698692321777,
      "learning_rate": 4.2623787646758554e-05,
      "loss": 4.6736,
      "step": 290
    },
    {
      "epoch": 0.45977011494252873,
      "grad_norm": 3.0845704078674316,
      "learning_rate": 4.236855538540071e-05,
      "loss": 4.4619,
      "step": 300
    },
    {
      "epoch": 0.47509578544061304,
      "grad_norm": 4.800248622894287,
      "learning_rate": 4.211332312404288e-05,
      "loss": 4.5162,
      "step": 310
    },
    {
      "epoch": 0.4904214559386973,
      "grad_norm": 3.4237887859344482,
      "learning_rate": 4.185809086268504e-05,
      "loss": 4.5105,
      "step": 320
    },
    {
      "epoch": 0.5057471264367817,
      "grad_norm": 3.040534496307373,
      "learning_rate": 4.160285860132721e-05,
      "loss": 4.4729,
      "step": 330
    },
    {
      "epoch": 0.5210727969348659,
      "grad_norm": 3.9785590171813965,
      "learning_rate": 4.1347626339969374e-05,
      "loss": 4.5051,
      "step": 340
    },
    {
      "epoch": 0.5363984674329502,
      "grad_norm": 3.74117112159729,
      "learning_rate": 4.109239407861154e-05,
      "loss": 4.5909,
      "step": 350
    },
    {
      "epoch": 0.5517241379310345,
      "grad_norm": 4.0314154624938965,
      "learning_rate": 4.0837161817253704e-05,
      "loss": 4.5983,
      "step": 360
    },
    {
      "epoch": 0.5670498084291188,
      "grad_norm": 3.525372266769409,
      "learning_rate": 4.058192955589586e-05,
      "loss": 4.4851,
      "step": 370
    },
    {
      "epoch": 0.5823754789272031,
      "grad_norm": 3.8184759616851807,
      "learning_rate": 4.032669729453803e-05,
      "loss": 4.6818,
      "step": 380
    },
    {
      "epoch": 0.5977011494252874,
      "grad_norm": 3.7908923625946045,
      "learning_rate": 4.007146503318019e-05,
      "loss": 4.4236,
      "step": 390
    },
    {
      "epoch": 0.6130268199233716,
      "grad_norm": 3.3671014308929443,
      "learning_rate": 3.981623277182236e-05,
      "loss": 4.4641,
      "step": 400
    },
    {
      "epoch": 0.6283524904214559,
      "grad_norm": 3.463228464126587,
      "learning_rate": 3.9561000510464524e-05,
      "loss": 4.1054,
      "step": 410
    },
    {
      "epoch": 0.6436781609195402,
      "grad_norm": 4.114248752593994,
      "learning_rate": 3.930576824910669e-05,
      "loss": 4.4972,
      "step": 420
    },
    {
      "epoch": 0.6590038314176245,
      "grad_norm": 4.176448822021484,
      "learning_rate": 3.9050535987748854e-05,
      "loss": 4.4946,
      "step": 430
    },
    {
      "epoch": 0.6743295019157088,
      "grad_norm": 3.4700496196746826,
      "learning_rate": 3.879530372639102e-05,
      "loss": 4.3986,
      "step": 440
    },
    {
      "epoch": 0.6896551724137931,
      "grad_norm": 3.123586416244507,
      "learning_rate": 3.8540071465033185e-05,
      "loss": 4.3895,
      "step": 450
    },
    {
      "epoch": 0.7049808429118773,
      "grad_norm": 3.328894853591919,
      "learning_rate": 3.828483920367535e-05,
      "loss": 4.4743,
      "step": 460
    },
    {
      "epoch": 0.7203065134099617,
      "grad_norm": 3.7805025577545166,
      "learning_rate": 3.802960694231751e-05,
      "loss": 4.3332,
      "step": 470
    },
    {
      "epoch": 0.735632183908046,
      "grad_norm": 3.787456512451172,
      "learning_rate": 3.7774374680959674e-05,
      "loss": 4.4101,
      "step": 480
    },
    {
      "epoch": 0.7509578544061303,
      "grad_norm": 3.5603740215301514,
      "learning_rate": 3.751914241960184e-05,
      "loss": 4.3398,
      "step": 490
    },
    {
      "epoch": 0.7662835249042146,
      "grad_norm": 3.931816339492798,
      "learning_rate": 3.7263910158244005e-05,
      "loss": 4.3791,
      "step": 500
    },
    {
      "epoch": 0.7816091954022989,
      "grad_norm": 3.7181625366210938,
      "learning_rate": 3.700867789688617e-05,
      "loss": 4.3289,
      "step": 510
    },
    {
      "epoch": 0.7969348659003831,
      "grad_norm": 3.542117118835449,
      "learning_rate": 3.6753445635528335e-05,
      "loss": 4.1944,
      "step": 520
    },
    {
      "epoch": 0.8122605363984674,
      "grad_norm": 3.0881872177124023,
      "learning_rate": 3.64982133741705e-05,
      "loss": 4.2358,
      "step": 530
    },
    {
      "epoch": 0.8275862068965517,
      "grad_norm": 4.55628776550293,
      "learning_rate": 3.6242981112812666e-05,
      "loss": 4.3461,
      "step": 540
    },
    {
      "epoch": 0.842911877394636,
      "grad_norm": 5.088281154632568,
      "learning_rate": 3.598774885145483e-05,
      "loss": 4.1362,
      "step": 550
    },
    {
      "epoch": 0.8582375478927203,
      "grad_norm": 3.478172540664673,
      "learning_rate": 3.573251659009699e-05,
      "loss": 4.2862,
      "step": 560
    },
    {
      "epoch": 0.8735632183908046,
      "grad_norm": 2.960465431213379,
      "learning_rate": 3.5477284328739155e-05,
      "loss": 3.9736,
      "step": 570
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 4.741291522979736,
      "learning_rate": 3.522205206738132e-05,
      "loss": 4.2699,
      "step": 580
    },
    {
      "epoch": 0.9042145593869731,
      "grad_norm": 5.039165496826172,
      "learning_rate": 3.4966819806023485e-05,
      "loss": 4.2238,
      "step": 590
    },
    {
      "epoch": 0.9195402298850575,
      "grad_norm": 4.153669834136963,
      "learning_rate": 3.4711587544665644e-05,
      "loss": 4.224,
      "step": 600
    },
    {
      "epoch": 0.9348659003831418,
      "grad_norm": 4.232259750366211,
      "learning_rate": 3.445635528330781e-05,
      "loss": 4.0774,
      "step": 610
    },
    {
      "epoch": 0.9501915708812261,
      "grad_norm": 3.819584846496582,
      "learning_rate": 3.4201123021949974e-05,
      "loss": 4.2919,
      "step": 620
    },
    {
      "epoch": 0.9655172413793104,
      "grad_norm": 5.907370567321777,
      "learning_rate": 3.394589076059214e-05,
      "loss": 4.2553,
      "step": 630
    },
    {
      "epoch": 0.9808429118773946,
      "grad_norm": 3.9435033798217773,
      "learning_rate": 3.3690658499234305e-05,
      "loss": 4.122,
      "step": 640
    },
    {
      "epoch": 0.9961685823754789,
      "grad_norm": 4.14194917678833,
      "learning_rate": 3.343542623787647e-05,
      "loss": 4.2743,
      "step": 650
    },
    {
      "epoch": 1.010727969348659,
      "grad_norm": 3.855388879776001,
      "learning_rate": 3.318019397651863e-05,
      "loss": 4.1616,
      "step": 660
    },
    {
      "epoch": 1.0260536398467432,
      "grad_norm": 4.745604991912842,
      "learning_rate": 3.2924961715160794e-05,
      "loss": 4.1631,
      "step": 670
    },
    {
      "epoch": 1.0413793103448277,
      "grad_norm": 4.671091079711914,
      "learning_rate": 3.266972945380296e-05,
      "loss": 4.1159,
      "step": 680
    },
    {
      "epoch": 1.0567049808429119,
      "grad_norm": 4.111346244812012,
      "learning_rate": 3.2414497192445125e-05,
      "loss": 4.1525,
      "step": 690
    },
    {
      "epoch": 1.072030651340996,
      "grad_norm": 3.364577293395996,
      "learning_rate": 3.215926493108729e-05,
      "loss": 4.2349,
      "step": 700
    },
    {
      "epoch": 1.0873563218390805,
      "grad_norm": 3.7989721298217773,
      "learning_rate": 3.1904032669729455e-05,
      "loss": 4.1349,
      "step": 710
    },
    {
      "epoch": 1.1026819923371647,
      "grad_norm": 3.917511463165283,
      "learning_rate": 3.164880040837162e-05,
      "loss": 4.0807,
      "step": 720
    },
    {
      "epoch": 1.118007662835249,
      "grad_norm": 4.736073017120361,
      "learning_rate": 3.1393568147013786e-05,
      "loss": 4.1571,
      "step": 730
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 4.9449462890625,
      "learning_rate": 3.113833588565595e-05,
      "loss": 4.2305,
      "step": 740
    },
    {
      "epoch": 1.1486590038314177,
      "grad_norm": 3.8269503116607666,
      "learning_rate": 3.0883103624298116e-05,
      "loss": 4.2534,
      "step": 750
    },
    {
      "epoch": 1.163984674329502,
      "grad_norm": 3.838918447494507,
      "learning_rate": 3.0627871362940275e-05,
      "loss": 4.1544,
      "step": 760
    },
    {
      "epoch": 1.1793103448275861,
      "grad_norm": 4.576918601989746,
      "learning_rate": 3.037263910158244e-05,
      "loss": 4.0662,
      "step": 770
    },
    {
      "epoch": 1.1946360153256705,
      "grad_norm": 4.420555591583252,
      "learning_rate": 3.0117406840224605e-05,
      "loss": 4.2216,
      "step": 780
    },
    {
      "epoch": 1.2099616858237547,
      "grad_norm": 6.180518627166748,
      "learning_rate": 2.986217457886677e-05,
      "loss": 4.1496,
      "step": 790
    },
    {
      "epoch": 1.2252873563218392,
      "grad_norm": 4.5114054679870605,
      "learning_rate": 2.9606942317508936e-05,
      "loss": 4.1287,
      "step": 800
    },
    {
      "epoch": 1.2406130268199234,
      "grad_norm": 4.145188808441162,
      "learning_rate": 2.9351710056151098e-05,
      "loss": 4.2291,
      "step": 810
    },
    {
      "epoch": 1.2559386973180078,
      "grad_norm": 3.7918972969055176,
      "learning_rate": 2.9096477794793263e-05,
      "loss": 4.2508,
      "step": 820
    },
    {
      "epoch": 1.271264367816092,
      "grad_norm": 4.099668979644775,
      "learning_rate": 2.8841245533435428e-05,
      "loss": 4.25,
      "step": 830
    },
    {
      "epoch": 1.2865900383141762,
      "grad_norm": 4.8456830978393555,
      "learning_rate": 2.8586013272077594e-05,
      "loss": 3.9114,
      "step": 840
    },
    {
      "epoch": 1.3019157088122606,
      "grad_norm": 4.603920936584473,
      "learning_rate": 2.833078101071976e-05,
      "loss": 4.1614,
      "step": 850
    },
    {
      "epoch": 1.3172413793103448,
      "grad_norm": 4.739001750946045,
      "learning_rate": 2.8075548749361917e-05,
      "loss": 4.0648,
      "step": 860
    },
    {
      "epoch": 1.332567049808429,
      "grad_norm": 5.012948989868164,
      "learning_rate": 2.7820316488004083e-05,
      "loss": 4.0114,
      "step": 870
    },
    {
      "epoch": 1.3478927203065134,
      "grad_norm": 5.403959274291992,
      "learning_rate": 2.7565084226646248e-05,
      "loss": 3.9137,
      "step": 880
    },
    {
      "epoch": 1.3632183908045978,
      "grad_norm": 6.337793827056885,
      "learning_rate": 2.7309851965288413e-05,
      "loss": 4.0707,
      "step": 890
    },
    {
      "epoch": 1.378544061302682,
      "grad_norm": 4.969009876251221,
      "learning_rate": 2.705461970393058e-05,
      "loss": 4.1578,
      "step": 900
    },
    {
      "epoch": 1.3938697318007662,
      "grad_norm": 4.455796241760254,
      "learning_rate": 2.6799387442572744e-05,
      "loss": 4.0582,
      "step": 910
    },
    {
      "epoch": 1.4091954022988507,
      "grad_norm": 4.529086112976074,
      "learning_rate": 2.654415518121491e-05,
      "loss": 4.189,
      "step": 920
    },
    {
      "epoch": 1.4245210727969349,
      "grad_norm": 4.300670623779297,
      "learning_rate": 2.6288922919857074e-05,
      "loss": 4.0046,
      "step": 930
    },
    {
      "epoch": 1.439846743295019,
      "grad_norm": 4.814548492431641,
      "learning_rate": 2.603369065849924e-05,
      "loss": 4.1559,
      "step": 940
    },
    {
      "epoch": 1.4551724137931035,
      "grad_norm": 5.1841511726379395,
      "learning_rate": 2.5778458397141398e-05,
      "loss": 4.1637,
      "step": 950
    },
    {
      "epoch": 1.4704980842911877,
      "grad_norm": 5.212989807128906,
      "learning_rate": 2.5523226135783563e-05,
      "loss": 3.9412,
      "step": 960
    },
    {
      "epoch": 1.485823754789272,
      "grad_norm": 4.084173679351807,
      "learning_rate": 2.526799387442573e-05,
      "loss": 4.0507,
      "step": 970
    },
    {
      "epoch": 1.5011494252873563,
      "grad_norm": 3.4910125732421875,
      "learning_rate": 2.501276161306789e-05,
      "loss": 4.2257,
      "step": 980
    },
    {
      "epoch": 1.5164750957854407,
      "grad_norm": 4.5673909187316895,
      "learning_rate": 2.4757529351710056e-05,
      "loss": 4.0483,
      "step": 990
    },
    {
      "epoch": 1.531800766283525,
      "grad_norm": 4.208532333374023,
      "learning_rate": 2.450229709035222e-05,
      "loss": 4.2525,
      "step": 1000
    },
    {
      "epoch": 1.547126436781609,
      "grad_norm": 4.009597301483154,
      "learning_rate": 2.4247064828994386e-05,
      "loss": 4.0239,
      "step": 1010
    },
    {
      "epoch": 1.5624521072796935,
      "grad_norm": 4.4257731437683105,
      "learning_rate": 2.3991832567636548e-05,
      "loss": 4.1017,
      "step": 1020
    },
    {
      "epoch": 1.5777777777777777,
      "grad_norm": 4.4459075927734375,
      "learning_rate": 2.3736600306278714e-05,
      "loss": 4.0609,
      "step": 1030
    },
    {
      "epoch": 1.593103448275862,
      "grad_norm": 4.185982704162598,
      "learning_rate": 2.348136804492088e-05,
      "loss": 4.1562,
      "step": 1040
    },
    {
      "epoch": 1.6084291187739463,
      "grad_norm": 4.609777927398682,
      "learning_rate": 2.3226135783563044e-05,
      "loss": 4.2388,
      "step": 1050
    },
    {
      "epoch": 1.6237547892720308,
      "grad_norm": 4.611171245574951,
      "learning_rate": 2.297090352220521e-05,
      "loss": 4.0736,
      "step": 1060
    },
    {
      "epoch": 1.639080459770115,
      "grad_norm": 4.8161420822143555,
      "learning_rate": 2.271567126084737e-05,
      "loss": 4.0532,
      "step": 1070
    },
    {
      "epoch": 1.6544061302681992,
      "grad_norm": 4.610001564025879,
      "learning_rate": 2.2460438999489537e-05,
      "loss": 4.1279,
      "step": 1080
    },
    {
      "epoch": 1.6697318007662836,
      "grad_norm": 5.241499423980713,
      "learning_rate": 2.2205206738131702e-05,
      "loss": 4.1309,
      "step": 1090
    },
    {
      "epoch": 1.6850574712643678,
      "grad_norm": 4.523852825164795,
      "learning_rate": 2.1949974476773867e-05,
      "loss": 4.0825,
      "step": 1100
    },
    {
      "epoch": 1.700383141762452,
      "grad_norm": 4.654211044311523,
      "learning_rate": 2.169474221541603e-05,
      "loss": 4.0976,
      "step": 1110
    },
    {
      "epoch": 1.7157088122605364,
      "grad_norm": 4.301573753356934,
      "learning_rate": 2.1439509954058194e-05,
      "loss": 3.9,
      "step": 1120
    },
    {
      "epoch": 1.7310344827586208,
      "grad_norm": 5.229281425476074,
      "learning_rate": 2.1184277692700356e-05,
      "loss": 3.9684,
      "step": 1130
    },
    {
      "epoch": 1.746360153256705,
      "grad_norm": 4.989621162414551,
      "learning_rate": 2.092904543134252e-05,
      "loss": 4.1182,
      "step": 1140
    },
    {
      "epoch": 1.7616858237547892,
      "grad_norm": 4.3971357345581055,
      "learning_rate": 2.0673813169984687e-05,
      "loss": 4.1619,
      "step": 1150
    },
    {
      "epoch": 1.7770114942528736,
      "grad_norm": 5.080413818359375,
      "learning_rate": 2.0418580908626852e-05,
      "loss": 4.158,
      "step": 1160
    },
    {
      "epoch": 1.7923371647509578,
      "grad_norm": 3.9253971576690674,
      "learning_rate": 2.0163348647269014e-05,
      "loss": 4.04,
      "step": 1170
    },
    {
      "epoch": 1.807662835249042,
      "grad_norm": 5.258626937866211,
      "learning_rate": 1.990811638591118e-05,
      "loss": 3.8409,
      "step": 1180
    },
    {
      "epoch": 1.8229885057471265,
      "grad_norm": 4.135342597961426,
      "learning_rate": 1.9652884124553345e-05,
      "loss": 4.1386,
      "step": 1190
    },
    {
      "epoch": 1.8383141762452109,
      "grad_norm": 5.230627536773682,
      "learning_rate": 1.939765186319551e-05,
      "loss": 3.8389,
      "step": 1200
    },
    {
      "epoch": 1.853639846743295,
      "grad_norm": 5.1639723777771,
      "learning_rate": 1.9142419601837675e-05,
      "loss": 3.8615,
      "step": 1210
    },
    {
      "epoch": 1.8689655172413793,
      "grad_norm": 5.795480251312256,
      "learning_rate": 1.8887187340479837e-05,
      "loss": 3.9307,
      "step": 1220
    },
    {
      "epoch": 1.8842911877394637,
      "grad_norm": 4.315303802490234,
      "learning_rate": 1.8631955079122002e-05,
      "loss": 4.094,
      "step": 1230
    },
    {
      "epoch": 1.899616858237548,
      "grad_norm": 4.725042819976807,
      "learning_rate": 1.8376722817764168e-05,
      "loss": 3.9583,
      "step": 1240
    },
    {
      "epoch": 1.914942528735632,
      "grad_norm": 4.348941326141357,
      "learning_rate": 1.8121490556406333e-05,
      "loss": 3.8959,
      "step": 1250
    },
    {
      "epoch": 1.9302681992337165,
      "grad_norm": 5.920312881469727,
      "learning_rate": 1.7866258295048495e-05,
      "loss": 3.8143,
      "step": 1260
    },
    {
      "epoch": 1.945593869731801,
      "grad_norm": 5.306441783905029,
      "learning_rate": 1.761102603369066e-05,
      "loss": 3.881,
      "step": 1270
    },
    {
      "epoch": 1.960919540229885,
      "grad_norm": 4.225001811981201,
      "learning_rate": 1.7355793772332822e-05,
      "loss": 3.9545,
      "step": 1280
    },
    {
      "epoch": 1.9762452107279693,
      "grad_norm": 5.355570316314697,
      "learning_rate": 1.7100561510974987e-05,
      "loss": 4.0985,
      "step": 1290
    },
    {
      "epoch": 1.9915708812260537,
      "grad_norm": 5.340626239776611,
      "learning_rate": 1.6845329249617152e-05,
      "loss": 3.7873,
      "step": 1300
    },
    {
      "epoch": 2.0061302681992337,
      "grad_norm": 4.74177360534668,
      "learning_rate": 1.6590096988259314e-05,
      "loss": 3.8291,
      "step": 1310
    },
    {
      "epoch": 2.021455938697318,
      "grad_norm": 5.030824184417725,
      "learning_rate": 1.633486472690148e-05,
      "loss": 3.8375,
      "step": 1320
    },
    {
      "epoch": 2.036781609195402,
      "grad_norm": 6.1089959144592285,
      "learning_rate": 1.6079632465543645e-05,
      "loss": 3.8444,
      "step": 1330
    },
    {
      "epoch": 2.0521072796934865,
      "grad_norm": 5.400735378265381,
      "learning_rate": 1.582440020418581e-05,
      "loss": 3.8329,
      "step": 1340
    },
    {
      "epoch": 2.067432950191571,
      "grad_norm": 4.490074634552002,
      "learning_rate": 1.5569167942827975e-05,
      "loss": 3.9928,
      "step": 1350
    },
    {
      "epoch": 2.0827586206896553,
      "grad_norm": 4.509688377380371,
      "learning_rate": 1.5313935681470137e-05,
      "loss": 3.9015,
      "step": 1360
    },
    {
      "epoch": 2.0980842911877393,
      "grad_norm": 4.473080158233643,
      "learning_rate": 1.5058703420112303e-05,
      "loss": 4.0198,
      "step": 1370
    },
    {
      "epoch": 2.1134099616858237,
      "grad_norm": 4.225720405578613,
      "learning_rate": 1.4803471158754468e-05,
      "loss": 3.9551,
      "step": 1380
    },
    {
      "epoch": 2.128735632183908,
      "grad_norm": 4.986114025115967,
      "learning_rate": 1.4548238897396632e-05,
      "loss": 3.9721,
      "step": 1390
    },
    {
      "epoch": 2.144061302681992,
      "grad_norm": 4.2514328956604,
      "learning_rate": 1.4293006636038797e-05,
      "loss": 3.7863,
      "step": 1400
    },
    {
      "epoch": 2.1593869731800766,
      "grad_norm": 4.753525257110596,
      "learning_rate": 1.4037774374680959e-05,
      "loss": 4.0448,
      "step": 1410
    },
    {
      "epoch": 2.174712643678161,
      "grad_norm": 5.748292922973633,
      "learning_rate": 1.3782542113323124e-05,
      "loss": 3.9627,
      "step": 1420
    },
    {
      "epoch": 2.1900383141762454,
      "grad_norm": 4.495266437530518,
      "learning_rate": 1.352730985196529e-05,
      "loss": 4.0812,
      "step": 1430
    },
    {
      "epoch": 2.2053639846743294,
      "grad_norm": 5.449577808380127,
      "learning_rate": 1.3272077590607455e-05,
      "loss": 4.1173,
      "step": 1440
    },
    {
      "epoch": 2.220689655172414,
      "grad_norm": 5.159962177276611,
      "learning_rate": 1.301684532924962e-05,
      "loss": 3.9656,
      "step": 1450
    },
    {
      "epoch": 2.236015325670498,
      "grad_norm": 4.474415302276611,
      "learning_rate": 1.2761613067891782e-05,
      "loss": 3.8441,
      "step": 1460
    },
    {
      "epoch": 2.251340996168582,
      "grad_norm": 8.140460014343262,
      "learning_rate": 1.2506380806533945e-05,
      "loss": 4.1018,
      "step": 1470
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 4.635501384735107,
      "learning_rate": 1.225114854517611e-05,
      "loss": 4.0679,
      "step": 1480
    },
    {
      "epoch": 2.281992337164751,
      "grad_norm": 4.661222457885742,
      "learning_rate": 1.1995916283818274e-05,
      "loss": 3.8598,
      "step": 1490
    },
    {
      "epoch": 2.2973180076628354,
      "grad_norm": 5.873424053192139,
      "learning_rate": 1.174068402246044e-05,
      "loss": 3.9878,
      "step": 1500
    }
  ],
  "logging_steps": 10,
  "max_steps": 1959,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.116408819744768e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
